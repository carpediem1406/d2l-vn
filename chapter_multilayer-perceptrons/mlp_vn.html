<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    
    <title>4.1. Perceptron đa tầng &#8212; Đắm mình vào Học Sâu 0.14.4 documentation</title>

    <link rel="stylesheet" href="../_static/basic.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/d2l.css" />
    <link rel="stylesheet" href="../_static/material-design-lite-1.3.0/material.blue-deep_orange.min.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx_materialdesign_theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fontawesome/all.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fonts.css" type="text/css" />
    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/language_data.js"></script>
    <script src="../_static/d2l.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="shortcut icon" href="../_static/favicon.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="4.2. Lập trình Perceptron Đa tầng từ đầu" href="mlp-scratch_vn.html" />
    <link rel="prev" title="4. Perceptron Đa tầng" href="index_vn.html" /> 
  </head>
<body>
    <div class="mdl-layout mdl-js-layout mdl-layout--fixed-header mdl-layout--fixed-drawer"><header class="mdl-layout__header mdl-layout__header--waterfall ">
    <div class="mdl-layout__header-row">
        
        <nav class="mdl-navigation breadcrumb">
            <a class="mdl-navigation__link" href="index_vn.html"><span class="section-number">4. </span>Perceptron Đa tầng</a><i class="material-icons">navigate_next</i>
            <a class="mdl-navigation__link is-active"><span class="section-number">4.1. </span>Perceptron đa tầng</a>
        </nav>
        <div class="mdl-layout-spacer"></div>
        <nav class="mdl-navigation">
        
<form class="form-inline pull-sm-right" action="../search.html" method="get">
      <div class="mdl-textfield mdl-js-textfield mdl-textfield--expandable mdl-textfield--floating-label mdl-textfield--align-right">
        <label id="quick-search-icon" class="mdl-button mdl-js-button mdl-button--icon"  for="waterfall-exp">
          <i class="material-icons">search</i>
        </label>
        <div class="mdl-textfield__expandable-holder">
          <input class="mdl-textfield__input" type="text" name="q"  id="waterfall-exp" placeholder="Search" />
          <input type="hidden" name="check_keywords" value="yes" />
          <input type="hidden" name="area" value="default" />
        </div>
      </div>
      <div class="mdl-tooltip" data-mdl-for="quick-search-icon">
      Quick search
      </div>
</form>
        
<a id="button-show-source"
    class="mdl-button mdl-js-button mdl-button--icon"
    href="../_sources/chapter_multilayer-perceptrons/mlp_vn.rst.txt" rel="nofollow">
  <i class="material-icons">code</i>
</a>
<div class="mdl-tooltip" data-mdl-for="button-show-source">
Show Source
</div>
        </nav>
    </div>
    <div class="mdl-layout__header-row header-links">
      <div class="mdl-layout-spacer"></div>
      <nav class="mdl-navigation">
          
              <a  class="mdl-navigation__link" href="https://github.com/aivivn/d2l-vn">
                  <i class="fab fa-github"></i>
                  GitHub
              </a>
          
              <a  class="mdl-navigation__link" href="https://forum.machinelearningcoban.com/">
                  <i class="fab fa-discourse"></i>
                  Forum
              </a>
          
              <a  class="mdl-navigation__link" href="https://www.d2l.ai/">
                  <i class="fas fa-external-link-alt"></i>
                  English
              </a>
      </nav>
    </div>
</header><header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <img class="logo" src="../_static/logo-with-text-vi.png" alt="Đắm mình vào Học Sâu"/>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul>
<li class="toctree-l1"><a class="reference internal" href="../intro_vn.html">Giới thiệu từ nhóm dịch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface/index_vn.html">Lời nói đầu</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_install/index_vn.html">Cài đặt</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_notation/index_vn.html">Ký hiệu</a></li>
</ul>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../chapter_introduction/index_vn.html">1. Giới thiệu</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preliminaries/index_vn.html">2. Sơ bộ</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/ndarray_vn.html">2.1. Thao tác với Dữ liệu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/pandas_vn.html">2.2. Tiền xử lý dữ liệu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra_vn.html">2.3. Đại số tuyến tính</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/calculus_vn.html">2.4. Giải tích</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/autograd_vn.html">2.5. Tính vi phân Tự động</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/probability_vn.html">2.6. Xác suất</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/lookup-api_vn.html">2.7. Tài liệu</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_linear-networks/index_vn.html">3. Mạng nơ-ron Tuyến tính</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/linear-regression_vn.html">3.1. Hồi quy Tuyến tính</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/linear-regression-scratch_vn.html">3.2. Lập trình Hồi quy Tuyến tính từ đầu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/linear-regression-gluon_vn.html">3.3. Cách lập trình súc tích Hồi quy Tuyến tính</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/softmax-regression_vn.html">3.4. Hồi quy Softmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/fashion-mnist_vn.html">3.5. Bộ dữ liệu Phân loại Ảnh (Fashion-MNIST)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/softmax-regression-scratch_vn.html">3.6. Lập trình Hồi quy Sofmax từ đầu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/softmax-regression-gluon_vn.html">3.7. Cách lập trình súc tích Hồi quy Softmax</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="index_vn.html">4. Perceptron Đa tầng</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">4.1. Perceptron đa tầng</a></li>
<li class="toctree-l2"><a class="reference internal" href="mlp-scratch_vn.html">4.2. Lập trình Perceptron Đa tầng từ đầu</a></li>
<li class="toctree-l2"><a class="reference internal" href="mlp-gluon_vn.html">4.3. Cách lập trình súc tích Perceptron Đa tầng</a></li>
<li class="toctree-l2"><a class="reference internal" href="underfit-overfit_vn.html">4.4. Lựa Chọn Mô Hình, Dưới Khớp và Quá Khớp</a></li>
<li class="toctree-l2"><a class="reference internal" href="weight-decay_vn.html">4.5. Suy giảm trọng số</a></li>
<li class="toctree-l2"><a class="reference internal" href="dropout_vn.html">4.6. Dropout</a></li>
<li class="toctree-l2"><a class="reference internal" href="backprop_vn.html">4.7. Lan truyền xuôi, Lan truyền ngược và Đồ thị tính toán</a></li>
<li class="toctree-l2"><a class="reference internal" href="numerical-stability-and-init_vn.html">4.8. Ổn định Số học và Khởi tạo</a></li>
<li class="toctree-l2"><a class="reference internal" href="environment_vn.html">4.9. Cân nhắc tới Môi trường</a></li>
<li class="toctree-l2"><a class="reference internal" href="kaggle-house-price_vn.html">4.10. Dự đoán Giá Nhà trên Kaggle</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_deep-learning-computation/index_vn.html">5. Tính toán Học sâu</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_deep-learning-computation/model-construction_vn.html">5.1. Tầng và Khối</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_deep-learning-computation/parameters_vn.html">5.2. Quản lý Tham số</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_deep-learning-computation/deferred-init_vn.html">5.3. Khởi tạo trễ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_deep-learning-computation/custom-layer_vn.html">5.4. Các tầng Tuỳ chỉnh</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_deep-learning-computation/read-write_vn.html">5.5. Đọc/Ghi tệp</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_deep-learning-computation/use-gpu_vn.html">5.6. GPU</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_convolutional-neural-networks/index_vn.html">6. Mạng Nơ-ron Tích chập</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/why-conv_vn.html">6.1. Từ Tầng Kết nối Dày đặc đến phép Tích chập</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/conv-layer_vn.html">6.2. Phép Tích chập cho Ảnh</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/padding-and-strides_vn.html">6.3. Đệm và Sải Bước</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/channels_vn.html">6.4. Đa kênh Đầu vào và Đầu ra</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/pooling_vn.html">6.5. Gộp (<em>Pooling</em>)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/lenet_vn.html">6.6. Mạng Nơ-ron Tích chập (LeNet)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_convolutional-modern/index_vn.html">7. Mạng Nơ-ron Tích chập Hiện đại</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/alexnet_vn.html">7.1. Mạng Nơ-ron Tích chập Sâu (AlexNet)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/vgg_vn.html">7.2. Mạng sử dụng Khối (VGG)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/nin_vn.html">7.3. Mạng trong Mạng (<em>Network in Network - NiN</em>)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/googlenet_vn.html">7.4. Mạng nối song song (GoogLeNet)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/batch-norm_vn.html">7.5. Chuẩn hoá theo batch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/resnet_vn.html">7.6. Mạng phần dư (ResNet)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/densenet_vn.html">7.7. Mạng Tích chập Kết nối Dày đặc (DenseNet)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_recurrent-neural-networks/index_vn.html">8. Mạng Nơ-ron Hồi tiếp</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/sequence_vn.html">8.1. Mô hình chuỗi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/text-preprocessing_vn.html">8.2. Tiền Xử lý Dữ liệu Văn bản</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/language-models-and-dataset_vn.html">8.3. Mô hình Ngôn ngữ và Tập dữ liệu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/rnn_vn.html">8.4. Mạng nơ-ron Hồi tiếp</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/rnn-scratch_vn.html">8.5. Lập trình Mạng nơ-ron Hồi tiếp từ đầu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/rnn-gluon_vn.html">8.6. Lập trình súc tích Mạng nơ-ron Hồi tiếp</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/bptt_vn.html">8.7. Lan truyền Ngược qua Thời gian</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_recurrent-modern/index_vn.html">9. Mạng Nơ-ron Hồi tiếp Hiện đại</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/gru_vn.html">9.1. Nút Hồi tiếp có Cổng (GRU)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/lstm_vn.html">9.2. Bộ nhớ Ngắn hạn Dài (LSTM)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/deep-rnn_vn.html">9.3. Mạng Nơ-ron Hồi tiếp Sâu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/bi-rnn_vn.html">9.4. Mạng Nơ-ron Hồi tiếp Hai chiều</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/machine-translation-and-dataset_vn.html">9.5. Dịch Máy và Tập dữ liệu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/encoder-decoder_vn.html">9.6. Kiến trúc Mã hoá - Giải mã</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/seq2seq_vn.html">9.7. Chuỗi sang Chuỗi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/beam-search_vn.html">9.8. Tìm kiếm Chùm</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_attention-mechanisms/index_vn.html">10. Cơ chế Tập trung</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms/attention_vn.html">10.1. Cơ chế Tập trung</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms/seq2seq-attention_vn.html">10.2. Chuỗi sang Chuỗi áp dụng Cơ chế Tập trung</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms/transformer_vn.html">10.3. Kiến trúc Transformer</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_optimization/index_vn.html">11. Thuật toán Tối ưu</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/optimization-intro_vn.html">11.1. Tối ưu và Học sâu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/optimization-intro_vn.html#cac-thach-thuc-cua-toi-uu-trong-hoc-sau">11.2. Các Thách thức của Tối ưu trong Học sâu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/optimization-intro_vn.html#cac-vung-cuc-tieu">11.3. Các vùng Cực tiểu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/optimization-intro_vn.html#cac-diem-yen-ngua">11.4. Các điểm Yên ngựa</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/optimization-intro_vn.html#tieu-bien-gradient">11.5. Tiêu biến Gradient</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/convexity_vn.html">11.6. Tính lồi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/gd_vn.html">11.7. Hạ Gradient</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/sgd_vn.html">11.8. Hạ Gradient Ngẫu nhiên</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/minibatch-sgd_vn.html">11.9. Hạ Gradient Ngẫu nhiên theo Minibatch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/momentum_vn.html">11.10. Động lượng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/adagrad_vn.html">11.11. Adagrad</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/rmsprop_vn.html">11.12. RMSProp</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/adadelta_vn.html">11.13. Adadelta</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/adam_vn.html">11.14. Adam</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/lr-scheduler_vn.html">11.15. Định thời Tốc độ Học</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_computational-performance/index_vn.html">12. Hiệu năng Tính toán</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/hybridize_vn.html">12.1. Trình biên dịch và Trình thông dịch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/async-computation_vn.html">12.2. Tính toán Bất đồng bộ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/auto-parallelism_vn.html">12.3. Song song hóa Tự động</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/hardware_vn.html">12.4. Phần cứng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/multiple-gpus_vn.html">12.5. Huấn luyện đa GPU</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/multiple-gpus-concise_vn.html">12.6. Cách lập trình Súc tích đa GPU</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/parameterserver_vn.html">12.7. Máy chủ Tham số</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_computer-vision/index_vn.html">13. Thị giác Máy tính</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/image-augmentation_vn.html">13.1. Tăng cường Ảnh</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/fine-tuning_vn.html">13.2. Tinh Chỉnh</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/bounding-box_vn.html">13.3. Phát hiện Vật thể và Khoanh vùng Đối tượng (Khung chứa)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/anchor_vn.html">13.4. Khung neo</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/multiscale-object-detection_vn.html">13.5. Phát hiện Vật thể Đa tỷ lệ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/object-detection-dataset_vn.html">13.6. Tập dữ liệu Phát hiện Đối tượng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/ssd_vn.html">13.7. Phát hiện Nhiều khung Một lượt (SSD)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/rcnn_vn.html">13.8. CNN theo Vùng (R-CNN)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/semantic-segmentation-and-dataset_vn.html">13.9. Phân vùng theo Ngữ nghĩa và Tập dữ liệu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/transposed-conv_vn.html">13.10. Tích chập Chuyển vị</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/fcn_vn.html">13.11. Mạng Tích chập Đầy đủ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/neural-style_vn.html">13.12. Truyền tải Phong cách Nơ-ron</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/kaggle-cifar10_vn.html">13.13. Phân loại ảnh (CIFAR-10) trên Kaggle</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/kaggle-dog_vn.html">13.14. Nhận diện Giống Chó (ImageNet Dogs) trên Kaggle</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/index_vn.html">14. Xử lý Ngôn ngữ Tự nhiên: Tiền Huấn luyện</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/word2vec_vn.html">14.1. Embedding Từ (word2vec)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/approx-training_vn.html">14.2. Huấn luyện Gần đúng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/word-embedding-dataset_vn.html">14.3. Tập dữ liệu để Tiền Huấn luyện Embedding Từ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/word2vec-pretraining_vn.html">14.4. Tiền huấn luyện word2vec</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/glove_vn.html">14.5. Embedding từ với Vector Toàn cục (GloVe)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/subword-embedding_vn.html">14.6. Embedding từ con</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/similarity-analogy_vn.html">14.7. Tìm kiếm từ Đồng nghĩa và Loại suy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/bert_vn.html">14.8. Biểu diễn Mã hóa hai chiều từ Transformer (BERT)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/bert-dataset_vn.html">14.9. Tập dữ liệu để Tiền huấn luyện BERT</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/bert-pretraining_vn.html">14.10. Tiền Huấn luyện BERT</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_natural-language-processing-applications/index_vn.html">15. Xử lý Ngôn ngữ Tự nhiên: Ứng dụng</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/sentiment-analysis-and-dataset_vn.html">15.1. Tác vụ Phân tích Cảm xúc và Bộ Dữ liệu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/sentiment-analysis-rnn_vn.html">15.2. Phân tích Cảm xúc: Sử dụng Mạng Nơ-ron Hồi tiếp</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/sentiment-analysis-cnn_vn.html">15.3. Phân tích Cảm xúc: Sử dụng Mạng Nơ-ron Tích Chập</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/natural-language-inference-and-dataset_vn.html">15.4. Suy luận ngôn ngữ tự nhiên và Tập dữ liệu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/natural-language-inference-attention_vn.html">15.5. Suy luận Ngôn ngữ Tự nhiên: Sử dụng Cơ chế Tập trung</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/finetuning-bert_vn.html">15.6. Tinh chỉnh BERT cho các Ứng dụng Cấp Chuỗi và Cấp Token</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/natural-language-inference-bert_vn.html">15.7. Suy luận Ngôn ngữ Tự nhiên: Tinh chỉnh BERT</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_recommender-systems/index_vn.html">16. Hệ thống Đề xuất</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/recsys-intro_vn.html">16.1. Tổng quan về Hệ thống Đề xuất</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/movielens_vn.html">16.2. Tập dữ liệu MovieLens</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/mf_vn.html">16.3. Phân rã Ma trận</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/autorec_vn.html">16.4. AutoRec: Dự đoán Đánh giá với Bộ tự Mã hóa</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/ranking_vn.html">16.5. Cá nhân hóa Xếp hạng trong Hệ thống Đề xuất</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/neumf_vn.html">16.6. Lọc Cộng tác Nơ-ron cho Cá nhân hóa Xếp hạng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/seqrec_vn.html">16.7. Hệ thống Đề xuất có Nhận thức về Chuỗi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/ctr_vn.html">16.8. Hệ thống Đề xuất Giàu Đặc trưng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/fm_vn.html">16.9. Máy Phân rã ma trận</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/deepfm_vn.html">16.10. Máy Phân rã Ma trận Sâu</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_generative-adversarial-networks/index_vn.html">17. Mạng Đối sinh</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_generative-adversarial-networks/gan_vn.html">17.1. Mạng Đối sinh</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_generative-adversarial-networks/dcgan_vn.html">17.2. Mạng Đối sinh Tích chập Sâu</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/index_vn.html">18. Phụ lục: Toán học cho Học Sâu</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/geometry-linear-algebraic-ops_vn.html">18.1. Các phép toán Hình học và Đại số Tuyến tính</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/eigendecomposition_vn.html">18.2. Phân rã trị riêng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/single-variable-calculus_vn.html">18.3. Giải tích một biến</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/multivariable-calculus_vn.html">18.4. Giải tích Nhiều biến</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/integral-calculus_vn.html">18.5. Giải tích Tích phân</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/random-variables_vn.html">18.6. Biến Ngẫu nhiên</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/maximum-likelihood_vn.html">18.7. Hợp lý Cực đại</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/distributions_vn.html">18.8. Các Phân phối Xác suất</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/naive-bayes_vn.html">18.9. Bộ phân loại Naive Bayes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/statistics_vn.html">18.10. Thống kê</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/information-theory_vn.html">18.11. Lý thuyết Thông tin</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/index_vn.html">19. Phụ lục: Công cụ cho Học Sâu</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/jupyter_vn.html">19.1. Sử dụng Jupyter</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/sagemaker_vn.html">19.2. Sử dụng Amazon SageMaker</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/aws_vn.html">19.3. Sử dụng Máy ảo AWS EC2</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/colab_vn.html">19.4. Sử dụng Google Colab</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/selecting-servers-gpus_vn.html">19.5. Lựa chọn Máy chủ &amp; GPU</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/contributing_vn.html">19.6. Đóng góp cho Quyển sách</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/d2l_vn.html">19.7. Tài liệu API của <code class="docutils literal notranslate"><span class="pre">d2l</span></code></a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../chapter_references/zreferences.html">Tài liệu tham khảo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../glossary.html">Bảng thuật ngữ</a></li>
</ul>

            </nav>
        
        </div>
    
</header>
        <main class="mdl-layout__content" tabIndex="0">

	<script type="text/javascript" src="../_static/sphinx_materialdesign_theme.js "></script>
    <header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <img class="logo" src="../_static/logo-with-text-vi.png" alt="Đắm mình vào Học Sâu"/>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul>
<li class="toctree-l1"><a class="reference internal" href="../intro_vn.html">Giới thiệu từ nhóm dịch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface/index_vn.html">Lời nói đầu</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_install/index_vn.html">Cài đặt</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_notation/index_vn.html">Ký hiệu</a></li>
</ul>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../chapter_introduction/index_vn.html">1. Giới thiệu</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preliminaries/index_vn.html">2. Sơ bộ</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/ndarray_vn.html">2.1. Thao tác với Dữ liệu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/pandas_vn.html">2.2. Tiền xử lý dữ liệu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra_vn.html">2.3. Đại số tuyến tính</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/calculus_vn.html">2.4. Giải tích</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/autograd_vn.html">2.5. Tính vi phân Tự động</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/probability_vn.html">2.6. Xác suất</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/lookup-api_vn.html">2.7. Tài liệu</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_linear-networks/index_vn.html">3. Mạng nơ-ron Tuyến tính</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/linear-regression_vn.html">3.1. Hồi quy Tuyến tính</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/linear-regression-scratch_vn.html">3.2. Lập trình Hồi quy Tuyến tính từ đầu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/linear-regression-gluon_vn.html">3.3. Cách lập trình súc tích Hồi quy Tuyến tính</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/softmax-regression_vn.html">3.4. Hồi quy Softmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/fashion-mnist_vn.html">3.5. Bộ dữ liệu Phân loại Ảnh (Fashion-MNIST)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/softmax-regression-scratch_vn.html">3.6. Lập trình Hồi quy Sofmax từ đầu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/softmax-regression-gluon_vn.html">3.7. Cách lập trình súc tích Hồi quy Softmax</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="index_vn.html">4. Perceptron Đa tầng</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">4.1. Perceptron đa tầng</a></li>
<li class="toctree-l2"><a class="reference internal" href="mlp-scratch_vn.html">4.2. Lập trình Perceptron Đa tầng từ đầu</a></li>
<li class="toctree-l2"><a class="reference internal" href="mlp-gluon_vn.html">4.3. Cách lập trình súc tích Perceptron Đa tầng</a></li>
<li class="toctree-l2"><a class="reference internal" href="underfit-overfit_vn.html">4.4. Lựa Chọn Mô Hình, Dưới Khớp và Quá Khớp</a></li>
<li class="toctree-l2"><a class="reference internal" href="weight-decay_vn.html">4.5. Suy giảm trọng số</a></li>
<li class="toctree-l2"><a class="reference internal" href="dropout_vn.html">4.6. Dropout</a></li>
<li class="toctree-l2"><a class="reference internal" href="backprop_vn.html">4.7. Lan truyền xuôi, Lan truyền ngược và Đồ thị tính toán</a></li>
<li class="toctree-l2"><a class="reference internal" href="numerical-stability-and-init_vn.html">4.8. Ổn định Số học và Khởi tạo</a></li>
<li class="toctree-l2"><a class="reference internal" href="environment_vn.html">4.9. Cân nhắc tới Môi trường</a></li>
<li class="toctree-l2"><a class="reference internal" href="kaggle-house-price_vn.html">4.10. Dự đoán Giá Nhà trên Kaggle</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_deep-learning-computation/index_vn.html">5. Tính toán Học sâu</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_deep-learning-computation/model-construction_vn.html">5.1. Tầng và Khối</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_deep-learning-computation/parameters_vn.html">5.2. Quản lý Tham số</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_deep-learning-computation/deferred-init_vn.html">5.3. Khởi tạo trễ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_deep-learning-computation/custom-layer_vn.html">5.4. Các tầng Tuỳ chỉnh</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_deep-learning-computation/read-write_vn.html">5.5. Đọc/Ghi tệp</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_deep-learning-computation/use-gpu_vn.html">5.6. GPU</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_convolutional-neural-networks/index_vn.html">6. Mạng Nơ-ron Tích chập</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/why-conv_vn.html">6.1. Từ Tầng Kết nối Dày đặc đến phép Tích chập</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/conv-layer_vn.html">6.2. Phép Tích chập cho Ảnh</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/padding-and-strides_vn.html">6.3. Đệm và Sải Bước</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/channels_vn.html">6.4. Đa kênh Đầu vào và Đầu ra</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/pooling_vn.html">6.5. Gộp (<em>Pooling</em>)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/lenet_vn.html">6.6. Mạng Nơ-ron Tích chập (LeNet)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_convolutional-modern/index_vn.html">7. Mạng Nơ-ron Tích chập Hiện đại</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/alexnet_vn.html">7.1. Mạng Nơ-ron Tích chập Sâu (AlexNet)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/vgg_vn.html">7.2. Mạng sử dụng Khối (VGG)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/nin_vn.html">7.3. Mạng trong Mạng (<em>Network in Network - NiN</em>)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/googlenet_vn.html">7.4. Mạng nối song song (GoogLeNet)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/batch-norm_vn.html">7.5. Chuẩn hoá theo batch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/resnet_vn.html">7.6. Mạng phần dư (ResNet)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/densenet_vn.html">7.7. Mạng Tích chập Kết nối Dày đặc (DenseNet)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_recurrent-neural-networks/index_vn.html">8. Mạng Nơ-ron Hồi tiếp</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/sequence_vn.html">8.1. Mô hình chuỗi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/text-preprocessing_vn.html">8.2. Tiền Xử lý Dữ liệu Văn bản</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/language-models-and-dataset_vn.html">8.3. Mô hình Ngôn ngữ và Tập dữ liệu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/rnn_vn.html">8.4. Mạng nơ-ron Hồi tiếp</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/rnn-scratch_vn.html">8.5. Lập trình Mạng nơ-ron Hồi tiếp từ đầu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/rnn-gluon_vn.html">8.6. Lập trình súc tích Mạng nơ-ron Hồi tiếp</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/bptt_vn.html">8.7. Lan truyền Ngược qua Thời gian</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_recurrent-modern/index_vn.html">9. Mạng Nơ-ron Hồi tiếp Hiện đại</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/gru_vn.html">9.1. Nút Hồi tiếp có Cổng (GRU)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/lstm_vn.html">9.2. Bộ nhớ Ngắn hạn Dài (LSTM)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/deep-rnn_vn.html">9.3. Mạng Nơ-ron Hồi tiếp Sâu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/bi-rnn_vn.html">9.4. Mạng Nơ-ron Hồi tiếp Hai chiều</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/machine-translation-and-dataset_vn.html">9.5. Dịch Máy và Tập dữ liệu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/encoder-decoder_vn.html">9.6. Kiến trúc Mã hoá - Giải mã</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/seq2seq_vn.html">9.7. Chuỗi sang Chuỗi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/beam-search_vn.html">9.8. Tìm kiếm Chùm</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_attention-mechanisms/index_vn.html">10. Cơ chế Tập trung</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms/attention_vn.html">10.1. Cơ chế Tập trung</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms/seq2seq-attention_vn.html">10.2. Chuỗi sang Chuỗi áp dụng Cơ chế Tập trung</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms/transformer_vn.html">10.3. Kiến trúc Transformer</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_optimization/index_vn.html">11. Thuật toán Tối ưu</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/optimization-intro_vn.html">11.1. Tối ưu và Học sâu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/optimization-intro_vn.html#cac-thach-thuc-cua-toi-uu-trong-hoc-sau">11.2. Các Thách thức của Tối ưu trong Học sâu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/optimization-intro_vn.html#cac-vung-cuc-tieu">11.3. Các vùng Cực tiểu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/optimization-intro_vn.html#cac-diem-yen-ngua">11.4. Các điểm Yên ngựa</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/optimization-intro_vn.html#tieu-bien-gradient">11.5. Tiêu biến Gradient</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/convexity_vn.html">11.6. Tính lồi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/gd_vn.html">11.7. Hạ Gradient</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/sgd_vn.html">11.8. Hạ Gradient Ngẫu nhiên</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/minibatch-sgd_vn.html">11.9. Hạ Gradient Ngẫu nhiên theo Minibatch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/momentum_vn.html">11.10. Động lượng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/adagrad_vn.html">11.11. Adagrad</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/rmsprop_vn.html">11.12. RMSProp</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/adadelta_vn.html">11.13. Adadelta</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/adam_vn.html">11.14. Adam</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/lr-scheduler_vn.html">11.15. Định thời Tốc độ Học</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_computational-performance/index_vn.html">12. Hiệu năng Tính toán</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/hybridize_vn.html">12.1. Trình biên dịch và Trình thông dịch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/async-computation_vn.html">12.2. Tính toán Bất đồng bộ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/auto-parallelism_vn.html">12.3. Song song hóa Tự động</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/hardware_vn.html">12.4. Phần cứng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/multiple-gpus_vn.html">12.5. Huấn luyện đa GPU</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/multiple-gpus-concise_vn.html">12.6. Cách lập trình Súc tích đa GPU</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/parameterserver_vn.html">12.7. Máy chủ Tham số</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_computer-vision/index_vn.html">13. Thị giác Máy tính</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/image-augmentation_vn.html">13.1. Tăng cường Ảnh</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/fine-tuning_vn.html">13.2. Tinh Chỉnh</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/bounding-box_vn.html">13.3. Phát hiện Vật thể và Khoanh vùng Đối tượng (Khung chứa)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/anchor_vn.html">13.4. Khung neo</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/multiscale-object-detection_vn.html">13.5. Phát hiện Vật thể Đa tỷ lệ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/object-detection-dataset_vn.html">13.6. Tập dữ liệu Phát hiện Đối tượng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/ssd_vn.html">13.7. Phát hiện Nhiều khung Một lượt (SSD)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/rcnn_vn.html">13.8. CNN theo Vùng (R-CNN)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/semantic-segmentation-and-dataset_vn.html">13.9. Phân vùng theo Ngữ nghĩa và Tập dữ liệu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/transposed-conv_vn.html">13.10. Tích chập Chuyển vị</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/fcn_vn.html">13.11. Mạng Tích chập Đầy đủ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/neural-style_vn.html">13.12. Truyền tải Phong cách Nơ-ron</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/kaggle-cifar10_vn.html">13.13. Phân loại ảnh (CIFAR-10) trên Kaggle</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/kaggle-dog_vn.html">13.14. Nhận diện Giống Chó (ImageNet Dogs) trên Kaggle</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/index_vn.html">14. Xử lý Ngôn ngữ Tự nhiên: Tiền Huấn luyện</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/word2vec_vn.html">14.1. Embedding Từ (word2vec)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/approx-training_vn.html">14.2. Huấn luyện Gần đúng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/word-embedding-dataset_vn.html">14.3. Tập dữ liệu để Tiền Huấn luyện Embedding Từ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/word2vec-pretraining_vn.html">14.4. Tiền huấn luyện word2vec</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/glove_vn.html">14.5. Embedding từ với Vector Toàn cục (GloVe)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/subword-embedding_vn.html">14.6. Embedding từ con</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/similarity-analogy_vn.html">14.7. Tìm kiếm từ Đồng nghĩa và Loại suy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/bert_vn.html">14.8. Biểu diễn Mã hóa hai chiều từ Transformer (BERT)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/bert-dataset_vn.html">14.9. Tập dữ liệu để Tiền huấn luyện BERT</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/bert-pretraining_vn.html">14.10. Tiền Huấn luyện BERT</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_natural-language-processing-applications/index_vn.html">15. Xử lý Ngôn ngữ Tự nhiên: Ứng dụng</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/sentiment-analysis-and-dataset_vn.html">15.1. Tác vụ Phân tích Cảm xúc và Bộ Dữ liệu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/sentiment-analysis-rnn_vn.html">15.2. Phân tích Cảm xúc: Sử dụng Mạng Nơ-ron Hồi tiếp</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/sentiment-analysis-cnn_vn.html">15.3. Phân tích Cảm xúc: Sử dụng Mạng Nơ-ron Tích Chập</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/natural-language-inference-and-dataset_vn.html">15.4. Suy luận ngôn ngữ tự nhiên và Tập dữ liệu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/natural-language-inference-attention_vn.html">15.5. Suy luận Ngôn ngữ Tự nhiên: Sử dụng Cơ chế Tập trung</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/finetuning-bert_vn.html">15.6. Tinh chỉnh BERT cho các Ứng dụng Cấp Chuỗi và Cấp Token</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/natural-language-inference-bert_vn.html">15.7. Suy luận Ngôn ngữ Tự nhiên: Tinh chỉnh BERT</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_recommender-systems/index_vn.html">16. Hệ thống Đề xuất</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/recsys-intro_vn.html">16.1. Tổng quan về Hệ thống Đề xuất</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/movielens_vn.html">16.2. Tập dữ liệu MovieLens</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/mf_vn.html">16.3. Phân rã Ma trận</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/autorec_vn.html">16.4. AutoRec: Dự đoán Đánh giá với Bộ tự Mã hóa</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/ranking_vn.html">16.5. Cá nhân hóa Xếp hạng trong Hệ thống Đề xuất</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/neumf_vn.html">16.6. Lọc Cộng tác Nơ-ron cho Cá nhân hóa Xếp hạng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/seqrec_vn.html">16.7. Hệ thống Đề xuất có Nhận thức về Chuỗi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/ctr_vn.html">16.8. Hệ thống Đề xuất Giàu Đặc trưng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/fm_vn.html">16.9. Máy Phân rã ma trận</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/deepfm_vn.html">16.10. Máy Phân rã Ma trận Sâu</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_generative-adversarial-networks/index_vn.html">17. Mạng Đối sinh</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_generative-adversarial-networks/gan_vn.html">17.1. Mạng Đối sinh</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_generative-adversarial-networks/dcgan_vn.html">17.2. Mạng Đối sinh Tích chập Sâu</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/index_vn.html">18. Phụ lục: Toán học cho Học Sâu</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/geometry-linear-algebraic-ops_vn.html">18.1. Các phép toán Hình học và Đại số Tuyến tính</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/eigendecomposition_vn.html">18.2. Phân rã trị riêng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/single-variable-calculus_vn.html">18.3. Giải tích một biến</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/multivariable-calculus_vn.html">18.4. Giải tích Nhiều biến</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/integral-calculus_vn.html">18.5. Giải tích Tích phân</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/random-variables_vn.html">18.6. Biến Ngẫu nhiên</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/maximum-likelihood_vn.html">18.7. Hợp lý Cực đại</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/distributions_vn.html">18.8. Các Phân phối Xác suất</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/naive-bayes_vn.html">18.9. Bộ phân loại Naive Bayes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/statistics_vn.html">18.10. Thống kê</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/information-theory_vn.html">18.11. Lý thuyết Thông tin</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/index_vn.html">19. Phụ lục: Công cụ cho Học Sâu</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/jupyter_vn.html">19.1. Sử dụng Jupyter</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/sagemaker_vn.html">19.2. Sử dụng Amazon SageMaker</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/aws_vn.html">19.3. Sử dụng Máy ảo AWS EC2</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/colab_vn.html">19.4. Sử dụng Google Colab</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/selecting-servers-gpus_vn.html">19.5. Lựa chọn Máy chủ &amp; GPU</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/contributing_vn.html">19.6. Đóng góp cho Quyển sách</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/d2l_vn.html">19.7. Tài liệu API của <code class="docutils literal notranslate"><span class="pre">d2l</span></code></a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../chapter_references/zreferences.html">Tài liệu tham khảo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../glossary.html">Bảng thuật ngữ</a></li>
</ul>

            </nav>
        
        </div>
    
</header>

    <div class="document">
        <div class="page-content" role="main">
        
  <!-- ===================== Bắt đầu dịch Phần 1 ===================== --><!-- ========================================= REVISE PHẦN 1 - BẮT ĐẦU =================================== --><!--
# Multilayer Perceptrons
--><div class="section" id="perceptron-da-tang">
<span id="sec-mlp"></span><h1><span class="section-number">4.1. </span>Perceptron đa tầng<a class="headerlink" href="#perceptron-da-tang" title="Permalink to this headline">¶</a></h1>
<!--
In the previous chapter, we introduced softmax regression (:numref:`sec_softmax`),
implementing the algorithm from scratch (:numref:`sec_softmax_scratch`) and in gluon (:numref:`sec_softmax_gluon`)
and training classifiers to recognize 10 categories of clothing from low-resolution images.
Along the way, we learned how to wrangle data, coerce our outputs into a valid probability distribution (via `softmax`),
apply an appropriate loss function, and to minimize it with respect to our model's parameters.
Now that we have mastered these mechanics in the context of simple linear models,
we can launch our exploration of deep neural networks, the comparatively rich class of models with which this book is primarily concerned.
--><p>Trong chương trước, chúng tôi đã giới thiệu hồi quy softmax
(<a class="reference internal" href="../chapter_linear-networks/softmax-regression_vn.html#sec-softmax"><span class="std std-numref">Section 3.4</span></a>), cách lập trình giải thuật này từ đầu
(<a class="reference internal" href="../chapter_linear-networks/softmax-regression-scratch_vn.html#sec-softmax-scratch"><span class="std std-numref">Section 3.6</span></a>), sử dụng nó trong gluon
(<a class="reference internal" href="../chapter_linear-networks/softmax-regression-gluon_vn.html#sec-softmax-gluon"><span class="std std-numref">Section 3.7</span></a>), và huấn luyện các bộ phân loại để nhận
diện 10 lớp quần áo khác nhau từ các bức ảnh có độ phân giải thấp. Cùng
với đó, chúng ta đã học cách xử lý dữ liệu, ép buộc các giá trị đầu ra
tạo thành một phân phối xác suất hợp lệ (thông qua hàm <code class="docutils literal notranslate"><span class="pre">softmax</span></code>), áp
dụng một hàm mất mát phù hợp và cực tiểu hoá nó theo các tham số mô
hình. Bây giờ, sau khi đã thành thạo các cơ chế nêu trên trong ngữ cảnh
của những mô hình tuyến tính đơn giản, chúng ta có thể bắt đầu khám phá
trọng tâm của cuốn sách này: lớp mô hình phong phú của các mạng nơ-ron
sâu.</p>
<!--
## Hidden Layers
--><div class="section" id="cac-tang-an">
<h2><span class="section-number">4.1.1. </span>Các tầng ẩn<a class="headerlink" href="#cac-tang-an" title="Permalink to this headline">¶</a></h2>
<!--
To begin, recall the model architecture corresponding to our softmax regression example, illustrated in  :numref:`fig_singlelayer` below.
This model mapped our inputs directly to our outputs via a single linear transformation:
--><p>Để bắt đầu, hãy nhớ lại kiến trúc mô hình trong ví dụ của hồi quy
softmax, được minh hoạ trong <a class="reference internal" href="#fig-singlelayer"><span class="std std-numref">Fig. 4.1.1</span></a> bên dưới. Mô
hình này ánh xạ trực tiếp các đầu vào của chúng ta sang các giá trị đầu
ra thông qua một phép biến đổi tuyến tính duy nhất:</p>
<div class="math notranslate nohighlight" id="equation-chapter-multilayer-perceptrons-mlp-vn-0">
<span class="eqno">(4.1.1)<a class="headerlink" href="#equation-chapter-multilayer-perceptrons-mlp-vn-0" title="Permalink to this equation">¶</a></span>\[\hat{\mathbf{o}} = \mathrm{softmax}(\mathbf{W} \mathbf{x} + \mathbf{b}).\]</div>
<!--
![Single layer perceptron with 5 output units.](../img/singlelayer.svg)
--><div class="figure align-default" id="id1">
<span id="fig-singlelayer"></span><img alt="../_images/singlelayer.svg" src="../_images/singlelayer.svg" /><p class="caption"><span class="caption-number">Fig. 4.1.1 </span><span class="caption-text">Perceptron đơn tầng với 5 nút đầu ra.</span><a class="headerlink" href="#id1" title="Permalink to this image">¶</a></p>
</div>
<!--
If our labels truly were related to our input data by a linear function, then this approach would be sufficient.
But linearity is a *strong assumption*.
--><p>Nếu các nhãn của chúng ta thật sự có mối quan hệ tuyến tính với dữ liệu
đầu vào, thì cách tiếp cận này là đủ. Nhưng tính tuyến tính là một <em>giả
định chặt</em>.</p>
<!--
For example, linearity implies the *weaker* assumption of *monotonicity*:
that any increase in our feature must either always cause an increase in our model's output (if the corresponding weight is positive),
or always always cause a decrease in our model's output (if the corresponding weight is negative).
Sometimes that makes sense.
For example, if we were trying to predict whether an individual will repay a loan,
we might reasonably imagine that holding all else equal, an applicant with a higher income would always be more likely to repay than one with a lower income.
While monotonic, this relationship likely isn't linearly associated with the probability of repayment.
A increase in income from $0 to $50k likely corresponds to a bigger increase in likelihood of repayment than an increase from $1M to $1.05M.
One way to handle this might be to pre-process our data such that linearity becomes more plausible, say, by using the logarithm of income as our feature.
--><p>Ví dụ, tính tuyến tính ngụ ý về giả định <em>yếu hơn</em> của <em>tính đơn điệu</em>:
tức giá trị đặc trưng tăng luôn dẫn đến việc đầu ra mô hình tăng (nếu
trọng số tương ứng dương), hoặc đầu ra mô hình giảm (nếu trọng số tương
ứng âm). Điều này đôi khi cũng hợp lý. Ví dụ, nếu chúng ta đang dự đoán
liệu một người có trả được khoản vay hay không, chúng ta có thể suy diễn
một cách hợp lý như sau: bỏ qua mọi yếu tố khác, ứng viên nào có thu
nhập cao hơn sẽ có khả năng trả được nợ cao hơn so với những ứng viên
khác có thu nhập thấp hơn. Dù có tính đơn điệu, mối quan hệ này khả năng
cao là không liên quan tuyến tính tới xác suất trả nợ. Khả năng trả được
nợ thường sẽ có mức tăng lớn hơn khi thu nhập tăng từ $0 lên $50k so với
khi tăng từ $1M lên $1.05M. Một cách để giải quyết điều này là tiền xử
lý dữ liệu để tính tuyến tính trở nên hợp lý hơn, ví dụ như sử dụng
logarit của thu nhập để làm đặc trưng.</p>
<!-- ===================== Kết thúc dịch Phần 1 ===================== --><!-- ===================== Bắt đầu dịch Phần 2 ===================== --><!--
Note that we can easily come up with examples that violate *monotonicity*.
Say for example that we want to predict probability of death based on body temperature.
For individuals with a body temperature above 37°C (98.6°F), higher termperatures indicate greater risk.
However, for individuals with body termperatures below 37° C, higher temperatures indicate *lower* risk!
In this case too, we might resolve the problem with some clever preprocessing.
Namely, we might use the *distance* from 37°C as our feature.
--><p>Lưu ý rằng chúng ta có thể dễ dàng đưa ra các ví dụ vi phạm <em>tính đơn
điệu</em>. Ví dụ như, ta muốn dự đoán xác suất tử vong của một người dựa
trên thân nhiệt. Đối với người có thân nhiệt trên 37°C (98.6°F), nhiệt
độ càng cao gây ra nguy cơ tử vong càng cao. Tuy nhiên, với những người
có thân nhiệt thấp hơn 37°C, khi gặp nhiệt độ cao hơn thì nguy cơ tử
vong lại <em>thấp hơn</em>! Trong bài toán này, ta có thể giải quyết nó bằng
một vài bước tiền xử lý thật khéo léo. Cụ thể, ta có thể sử dụng <em>khoảng
cách</em> từ 37°C tới thân nhiệt làm đặc trưng.</p>
<!--
But what about classifying images of cats and dogs?
Should increasing the intensity of the pixel at location (13, 17) always increase (or always decrease) the likelihood that the image depicts a dog?
Reliance on a linear model corrsponds to the (implicit) assumption that the only requirement for differentiating cats vs. dogs is to assess the brightness of individual pixels.
This approach is doomed to fail in a world where inverting an image preserves the category.
--><p>Nhưng còn với bài toán phân loại hình ảnh chó mèo thì sao? Liệu việc
tăng cường độ sáng của điểm ảnh tại vị trí (13, 17) sẽ luôn tăng (hoặc
giảm) khả năng đó là hình một con chó? Sử dụng mô hình tuyến tính trong
trường hợp này tương ứng với việc ngầm giả định rằng chỉ cần đánh giá độ
sáng của từng pixel để phân biệt giữa mèo và chó . Cách tiếp cận này
chắc chắn sẽ không chính xác khi các hình ảnh bị đảo ngược màu sắc.</p>
<!--
And yet despite the apparent absurdity of linearity here, as compared to our previous examples, it's less obvious that we could address the problem with a simple preprocessing fix.
That is because the significance of any pixel depends in complex ways on its context (the values of the surrounding pixels).
While there might exist a representation of our data that would take into account the relevant interactions among our features (and on top of which a linear model would be suitable),
we simply do not know how to calculate it by hand.
With deep neural networks, we used observational data to jointly learn both a representation (via hidden layers) and a linear predictor that acts upon that representation.
--><p>Tuy nhiên, ta bỏ qua sự phi lý của tuyến tính ở đây, so với các ví dụ
trước, rõ ràng là ta không thể giải quyết bài toán này với vài bước tiền
xử lý chỉnh sửa đơn giản. Bởi vì ý nghĩa của các điểm ảnh phụ thuộc một
cách phức tạp vào bối cảnh xung quanh nó (các giá trị xung quanh của
điểm ảnh). Có thể vẫn tồn tại một cách biểu diễn dữ liệu nào đó nắm bắt
được sự tương tác giữa các đặc trưng liên quan (và quan trọng nhất là
phù hợp với mô hình tuyến tính), ta đơn giản là không biết làm thế nào
để tính toán nó một cách thủ công. Với các mạng nơ-ron sâu, ta sử dụng
dữ liệu đã quan sát được để đồng thời học cách biểu diễn (thông qua các
tầng ẩn) và học một bộ dự đoán tuyến tính hoạt động dựa trên biểu diễn
đó.</p>
<!-- ========================================= REVISE PHẦN 1 - KẾT THÚC ===================================--><!-- ========================================= REVISE PHẦN 2 - BẮT ĐẦU =================================== --><!--
### Incorporating Hidden Layers
--><div class="section" id="ket-hop-cac-tang-an">
<h3><span class="section-number">4.1.1.1. </span>Kết hợp các Tầng ẩn<a class="headerlink" href="#ket-hop-cac-tang-an" title="Permalink to this headline">¶</a></h3>
<!--
We can overcome these limitations of linear models and handle a more general class of functions by incorporating one or more hidden layers.
The easiest way to do this is to stack many fully-connected layers on top of each other.
Each layer feeds into the layer above it, until we generate an output.
We can think of the first $L-1$ layers as our representation and the final layer as our linear predictor.
This architecture is commonly called a *multilayer perceptron*, often abbreviated as *MLP*.
Below, we depict an MLP diagramtically (:numref:`fig_nlp`).
--><p>Ta có thể vượt qua những hạn chế của mô hình tuyến tính và làm việc với
một lớp hàm tổng quát hơn bằng cách thêm vào một hoặc nhiều tầng ẩn.
Cách dễ nhất để làm điều này là xếp chồng nhiều tầng kết nối đầy đủ lên
nhau. Giá trị đầu ra của mỗi tầng được đưa làm giá trị đầu vào cho tầng
bên trên, cho đến khi ta tạo được một đầu ra. Ta có thể xem <span class="math notranslate nohighlight">\(L-1\)</span>
tầng đầu tiên như các tầng học biểu diễn dữ liệu và tầng cuối cùng là bộ
dự đoán tuyến tính. Kiến trúc này thường được gọi là <em>perceptron đa
tầng</em> (<em>multilayer percention</em>), hay được viết tắt là <em>MLP</em>. Dưới đây,
ta mô tả sơ đồ MLP (<a class="reference internal" href="#fig-nlp"><span class="std std-numref">Fig. 4.1.2</span></a>).</p>
<!--
![Multilayer perceptron with hidden layers. This example contains a hidden layer with 5 hidden units in it. ](../img/mlp.svg)
--><div class="figure align-default" id="id2">
<span id="fig-nlp"></span><img alt="../_images/mlp.svg" src="../_images/mlp.svg" /><p class="caption"><span class="caption-number">Fig. 4.1.2 </span><span class="caption-text">Perceptron đa tầng với các tầng ẩn. Ví dụ này chứa một tầng ẩn với 5
nút ẩn.</span><a class="headerlink" href="#id2" title="Permalink to this image">¶</a></p>
</div>
<!--
This multilayer perceptron has 4 inputs, 3 outputs, and its hidden layer contains 5 hidden units.
Since the input layer does not involve any calculations, producing outputs with this network requires implementing the computations for each of the 2 layers (hidden and output).
Note that these layers are both fully connected.
Every input influences every neuron in the hidden layer, and each of these in turn influences every neuron in the output layer.
--><p>Perceptron đa tầng này có 4 đầu vào, 3 đầu ra và tầng ẩn của nó chứa 5
nút ẩn. Vì tầng đầu vào không cần bất kỳ tính toán nào, do đó đối với
mạng này để tạo đầu ra đòi hỏi phải lập trình các phép tính cho hai tầng
còn lại (tầng ẩn và tầng đầu ra). Lưu ý, tất cả tầng này đều kết nối đầy
đủ. Mỗi đầu vào đều ảnh hưởng đến mọi nơ-ron trong tầng ẩn và mỗi nơ-ron
này lại ảnh hưởng đến mọi nơ-ron trong tầng đầu ra.</p>
<!-- ===================== Kết thúc dịch Phần 2 ===================== --><!-- ===================== Bắt đầu dịch Phần 3 ===================== --><!--
### From Linear to Nonlinear
--></div>
<div class="section" id="tu-tuyen-tinh-den-phi-tuyen">
<h3><span class="section-number">4.1.1.2. </span>Từ Tuyến tính đến Phi tuyến<a class="headerlink" href="#tu-tuyen-tinh-den-phi-tuyen" title="Permalink to this headline">¶</a></h3>
<!--
Formally, we calculate each layer in this one-hidden-layer MLP as follows:
--><p>Về mặt hình thức, chúng ta tính toán mỗi tầng trong MLP một-tầng-ẩn này
như sau:</p>
<div class="math notranslate nohighlight" id="equation-chapter-multilayer-perceptrons-mlp-vn-1">
<span class="eqno">(4.1.2)<a class="headerlink" href="#equation-chapter-multilayer-perceptrons-mlp-vn-1" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{aligned}
    \mathbf{h} &amp; = \mathbf{W}_1 \mathbf{x} + \mathbf{b}_1, \\
    \mathbf{o} &amp; = \mathbf{W}_2 \mathbf{h} + \mathbf{b}_2, \\
    \hat{\mathbf{y}} &amp; = \mathrm{softmax}(\mathbf{o}).
\end{aligned}\end{split}\]</div>
<!--
Note that after adding this layer, our model now requires us to track and update two additional sets of parameters.
So what have we gained in exchange?
You might be surprised to find out that---in the model defined above---*we gain nothing for our troubles!*
The reason is plain.
The hidden units above are given by a linear function of the inputs, and the outputs (pre-softmax) are just a linear function of the hidden units.
A linear function of a linear function is itself a linear function.
Moreover, our linear model was already capable of representing any linear function.
--><p>Chú ý rằng sau khi thêm tầng này vào, mô hình lập tức yêu cầu chúng ta
phải theo dõi và cập nhật thêm hai tập tham số. Vậy thì đổi lại ta sẽ
nhận được gì? Bạn có thể bất ngờ khi phát hiện ra rằng—trong mô hình
định nghĩa bên trên—<em>chúng ta chẳng thu được lợi ích gì từ những rắc
rối thêm vào!</em> Lý do rất đơn giản. Các nút ẩn bên trên được định nghĩa
bởi một hàm tuyến tính của các đầu vào, và các đầu ra (tiền Softmax) chỉ
là một hàm tuyến tính của các nút ẩn. Một hàm tuyến tính của một hàm
tuyến tính bản thân nó cũng chính là một hàm tuyến tính. Hơn nữa, mô
hình tuyến tính của chúng ta vốn dĩ đã có khả năng biểu diễn bất kỳ hàm
tuyến tính nào rồi.</p>
<!--
We can view the equivalence formally by proving that for any values of the weights, we can just collapse out the hidden layer, yielding an equivalent single-layer model with paramters
--><p>Ta có thể thấy sự tương đồng về mặt hình thức bằng cách chứng minh rằng
với mọi giá trị của các trọng số, ta đều có thể loại bỏ tầng ẩn và tạo
ra một mô hình đơn-tầng với các tham số</p>
<p><span class="math notranslate nohighlight">\(\mathbf{W} = \mathbf{W}_2 \mathbf{W}_1\)</span> và
<span class="math notranslate nohighlight">\(\mathbf{b} = \mathbf{W}_2 \mathbf{b}_1 + \mathbf{b}_2\)</span>.</p>
<div class="math notranslate nohighlight" id="equation-chapter-multilayer-perceptrons-mlp-vn-2">
<span class="eqno">(4.1.3)<a class="headerlink" href="#equation-chapter-multilayer-perceptrons-mlp-vn-2" title="Permalink to this equation">¶</a></span>\[\mathbf{o} = \mathbf{W}_2 \mathbf{h} + \mathbf{b}_2 = \mathbf{W}_2 (\mathbf{W}_1 \mathbf{x} + \mathbf{b}_1) + \mathbf{b}_2 = (\mathbf{W}_2 \mathbf{W}_1) \mathbf{x} + (\mathbf{W}_2 \mathbf{b}_1 + \mathbf{b}_2) = \mathbf{W} \mathbf{x} + \mathbf{b}.\]</div>
<!--
In order to realize the potential of multilayer architectures,
we need one more key ingredient---an
elementwise *nonlinear activation function* $\sigma$
to be applied to each hidden unit
(following the linear transformation).
The most popular choice for the nonlinearity
these days is the rectified linear unit (ReLU)
$\mathrm{max}(x, 0)$.
In general, with these activation functions in place,
it is no longer possible to collapse our MLP into a linear model.
--><p>Để hiện thực được tiềm năng của các kiến trúc đa tầng, chúng ta cần một
thành phần quan trọng nữa—một <em>hàm kích hoạt phi tuyến</em> theo từng phần
tử <span class="math notranslate nohighlight">\(\sigma\)</span> để áp dụng lên từng nút ẩn (theo sau phép biến đổi
tuyến tính). Hiện nay, lựa chọn phổ biến nhất cho tính phi tuyến là đơn
vị tuyến tính chỉnh lưu (ReLU) <span class="math notranslate nohighlight">\(\mathrm{max}(x, 0)\)</span>. Nhìn chung,
với việc sử dụng các hàm kích hoạt này, chúng ta sẽ không thể biến MLP
thành một mô hình tuyến tính được nữa.</p>
<div class="math notranslate nohighlight" id="equation-chapter-multilayer-perceptrons-mlp-vn-3">
<span class="eqno">(4.1.4)<a class="headerlink" href="#equation-chapter-multilayer-perceptrons-mlp-vn-3" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{aligned}
    \mathbf{h} &amp; = \sigma(\mathbf{W}_1 \mathbf{x} + \mathbf{b}_1), \\
    \mathbf{o} &amp; = \mathbf{W}_2 \mathbf{h} + \mathbf{b}_2, \\
    \hat{\mathbf{y}} &amp; = \mathrm{softmax}(\mathbf{o}).
\end{aligned}\end{split}\]</div>
<!--
To build more general MLPs, we can continue stacking such hidden layers,
e.g., $\mathbf{h}_1 = \sigma(\mathbf{W}_1 \mathbf{x} + \mathbf{b}_1)$ and $\mathbf{h}_2 = \sigma(\mathbf{W}_2 \mathbf{h}_1 + \mathbf{b}_2)$,
one atop another, yielding ever more expressive models (assuming fixed width).
--><p>Để xây dựng các MLP tổng quan hơn, chúng ta có thể tiếp tục chồng thêm
các tầng ẩn, ví dụ,
<span class="math notranslate nohighlight">\(\mathbf{h}_1 = \sigma(\mathbf{W}_1 \mathbf{x} + \mathbf{b}_1)\)</span> và
<span class="math notranslate nohighlight">\(\mathbf{h}_2 = \sigma(\mathbf{W}_2 \mathbf{h}_1 + \mathbf{b}_2)\)</span>,
kế tiếp nhau, tạo ra các mô hình có khả năng biểu diễn càng cao (giả sử
chiều rộng cố định).</p>
<!--
MLPs can capture complex interactions among our inputs via their hidden neurons, which depend on the values of each of the inputs.
We can easily design hidden nodes to perform arbitrary computation, for instance, basic logic operations on a pair of inputs.
Moreover, for certain choices of the activation function, it is widely known that MLPs are universal approximators.
Even with a single-hidden-layer network, given enough nodes (possibly absurdly many), and the right set of weights, we can model any function.
*Actually learning that function is the hard part.* You might think of your neural network as being a bit like the C programming language.
The language, like any other modern language, is capable of expressing any computable program.
But actually coming up with a program that meets your specifications is the hard part.
--><p>Các MLP có thể biểu diễn được những tương tác phức tạp giữa các đầu vào
thông qua các nơ-ron ẩn, các nơ-ron ẩn này phụ thuộc vào giá trị của mỗi
đầu vào. Chúng ta có thể dễ dàng thiết kế các nút ẩn để thực hiện bất kỳ
tính toán nào, ví dụ, các phép tính logic cơ bản trên một cặp đầu vào.
Ngoài ra, với một số hàm kích hoạt cụ thể, các MLP được biết đến rộng
rãi như là các bộ xấp xỉ vạn năng. Thậm chí với một mạng chỉ có một tầng
ẩn, nếu có đủ số nút (có thể nhiều một cách vô lý) và một tập các trọng
số thích hợp, chúng ta có thể mô phỏng bất kỳ một hàm nào. <em>Thật ra thì
việc học được hàm đó mới là phần khó khăn.</em> Bạn có thể tưởng tượng mạng
nơ-ron của mình có nét giống với ngôn ngữ lập trình C. Ngôn ngữ này
giống như bất kỳ ngôn ngữ hiện đại nào khác, có khả năng biểu diễn bất
kỳ chương trình tính toán nào. Tuy nhiên việc tạo ra một chương trình
đáp ứng được các các chỉ tiêu kỹ thuật mới là phần việc khó khăn.</p>
<!--
Moreover, just because a single-layer network *can* learn any function does not mean that you should try to solve all of your problems with single-layer networks.
In fact, we can approximate many functions much more compactly by using deeper (vs wider) networks.
We'll touch upon more rigorous arguments in subsequent chapters, but first let's actually build an MLP in code.
In this example, we’ll implement an MLP with two hidden layers and one output layer.
--><p>Hơn nữa, chỉ vì một mạng đơn-tầng <em>có thể</em> học bất kỳ hàm nào không có
nghĩa rằng bạn nên cố gắng giải quyết tất cả các vấn đề của mình bằng
các mạng đơn-tầng. Thực tế, chúng ta có thể ước lượng các hàm một cách
gọn gàng hơn rất nhiều bằng cách sử dụng mạng sâu hơn (thay vì rộng
hơn). Chúng ta sẽ đề cập đến những lập luận chặt chẽ hơn trong các
chương tiếp theo, nhưng trước tiên hãy lập trình một MLP. Trong ví dụ
này, chúng ta lập trình một MLP với hai tầng ẩn và một tầng đầu ra.</p>
<!-- ===================== Kết thúc dịch Phần 3 ===================== --><!-- ===================== Bắt đầu dịch Phần 4 ===================== --><!-- ========================================= REVISE PHẦN 2 - KẾT THÚC ===================================--><!-- ========================================= REVISE PHẦN 3 - BẮT ĐẦU =================================== --><!--
### Vectorization and Minibatch
--></div>
<div class="section" id="vector-hoa-va-minibatch">
<h3><span class="section-number">4.1.1.3. </span>Vector hoá và Minibatch<a class="headerlink" href="#vector-hoa-va-minibatch" title="Permalink to this headline">¶</a></h3>
<!--
As before, by the matrix $\mathbf{X}$, we denote a minibatch of inputs.
The calculations to produce outputs from an MLP with two hidden layers can thus be expressed:
--><p>Giống như trước, chúng ta dùng ma trận <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> để ký hiệu một
minibatch các giá trị đầu vào. Các phép tính toán dẫn đến các giá trị
đầu ra từ một MLP với hai tầng ẩn khi đó có thể được biểu diễn như sau:</p>
<div class="math notranslate nohighlight" id="equation-chapter-multilayer-perceptrons-mlp-vn-4">
<span class="eqno">(4.1.5)<a class="headerlink" href="#equation-chapter-multilayer-perceptrons-mlp-vn-4" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{aligned}
    \mathbf{H}_1 &amp; = \sigma(\mathbf{W}_1 \mathbf{X} + \mathbf{b}_1), \\
    \mathbf{H}_2 &amp; = \sigma(\mathbf{W}_2 \mathbf{H}_1 + \mathbf{b}_2), \\
    \mathbf{O} &amp; = \mathrm{softmax}(\mathbf{W}_3 \mathbf{H}_2 + \mathbf{b}_3).
\end{aligned}\end{split}\]</div>
<!--
With some abuse of notation, we define the nonlinearity $\sigma$ to apply to its inputs in a row-wise fashion, i.e., one observation at a time.
Note that we are also using the notation for *softmax* in the same way to denote a row-wise operation.
Often, as in this section, the activation functions that we apply to hidden layers are not merely row-wise, but component wise.
That means that after computing the linear portion of the layer, we can calculate each nodes activation without looking at the values taken by the other hidden units.
This is true for most activation functions (the batch normalization operation will be introduced in :numref:`sec_batch_norm` is a notable exception to that rule).
--><p>Bằng việc lạm dụng ký hiệu một chút, chúng ta định nghĩa hàm phi tuyến
<span class="math notranslate nohighlight">\(\sigma\)</span> là một phép toán áp dụng theo từng hàng, tức lần lượt
từng điểm dữ liệu một. Cần chú ý rằng ta cũng sử dụng quy ước này cho
hàm <em>softmax</em> để ký hiệu toán tử tính theo từng hàng. Thông thường, như
trong mục này, các hàm kích hoạt không chỉ đơn thuần được áp dụng vào
tầng ẩn theo từng hàng mà còn theo từng phần tử. Điều đó có nghĩa là sau
khi tính toán xong phần tuyến tính của tầng, chúng ta có thể tính giá
trị kích hoạt của từng nút mà không cần đến giá trị của các nút còn lại.
Điều này cũng đúng đối với hầu hết các hàm kích hoạt (toán tử chuẩn hoá
theo batch được giới thiệu trong <a class="reference internal" href="../chapter_convolutional-modern/batch-norm_vn.html#sec-batch-norm"><span class="std std-numref">Section 7.5</span></a> là một
trường hợp ngoại lệ của quy tắc này).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="kn">from</span> <span class="nn">d2l</span> <span class="kn">import</span> <span class="n">mxnet</span> <span class="k">as</span> <span class="n">d2l</span>
<span class="kn">from</span> <span class="nn">mxnet</span> <span class="kn">import</span> <span class="n">autograd</span><span class="p">,</span> <span class="n">np</span><span class="p">,</span> <span class="n">npx</span>
<span class="n">npx</span><span class="o">.</span><span class="n">set_np</span><span class="p">()</span>
</pre></div>
</div>
<!--
## Activation Functions
--></div>
</div>
<div class="section" id="cac-ham-kich-hoat">
<h2><span class="section-number">4.1.2. </span>Các hàm Kích hoạt<a class="headerlink" href="#cac-ham-kich-hoat" title="Permalink to this headline">¶</a></h2>
<!--
Activation functions decide whether a neuron should be activated or not by calculating the weighted sum and further adding bias with it.
They are differentiable operators to transform input signals to outputs, while most of them add non-linearity.
Because activation functions are fundamental to deep learning, let's briefly survey some common activation functions.
--><p>Các hàm kích hoạt quyết định một nơ-ron có được kích hoạt hay không bằng
cách tính tổng có trọng số và cộng thêm hệ số điều chỉnh vào nó. Chúng
là các toán tử khả vi và hầu hết đều biến đổi các tín hiệu đầu vào thành
các tín hiệu đầu ra theo một cách phi tuyến tính. Bởi vì các hàm kích
hoạt rất quan trọng trong học sâu, hãy cùng tìm hiểu sơ lược một số hàm
kích hoạt thông dụng.</p>
<!-- ===================== Kết thúc dịch Phần 4 ===================== --><!-- ===================== Bắt đầu dịch Phần 5 ===================== --><!--
### ReLU Function
--><div class="section" id="ham-relu">
<h3><span class="section-number">4.1.2.1. </span>Hàm ReLU<a class="headerlink" href="#ham-relu" title="Permalink to this headline">¶</a></h3>
<!--
As stated above, the most popular choice, due to both simplicity of implementation its performance on a variety of predictive tasks is the rectified linear unit (ReLU).
ReLU provide a very simple nonlinear transformation.
Given the element $z$, the function is defined as the maximum of that element and 0.
--><p>Như đã đề cập trước đó, đơn vị tuyến tính chỉnh lưu (ReLU) là sự lựa
chọn phổ biến nhất do tính đơn giản khi lập trình và hiệu quả trong
nhiều tác vụ dự đoán. ReLU là một phép biến đổi phi tuyến đơn giản. Cho
trước một phần tử <span class="math notranslate nohighlight">\(z\)</span>, ta định nghĩa hàm ReLU là giá trị lớn nhất
giữa chính phần tử đó và 0.</p>
<div class="math notranslate nohighlight" id="equation-chapter-multilayer-perceptrons-mlp-vn-5">
<span class="eqno">(4.1.6)<a class="headerlink" href="#equation-chapter-multilayer-perceptrons-mlp-vn-5" title="Permalink to this equation">¶</a></span>\[\mathrm{ReLU}(z) = \max(z, 0).\]</div>
<!--
Informally, the ReLU function retains only positive elements and discards all negative elements (setting the corresponding activations to 0).
To gain some intuition, we can plot the function.
Because it is used so commonly, `ndarray` supports the `relu` function as a native operator.
As you can see, the activation function is piecewise linear.
--><p>Nói một cách dễ hiểu hơn, hàm ReLU chỉ giữ lại các phần tử có giá trị
dương và loại bỏ tất cả các phần tử có giá trị âm (đặt kích hoạt tương
ứng là 0). Để có một cái nhìn khái quát, ta có thể vẽ đồ thị hàm số. Bởi
vì ReLU được sử dụng rất phổ biến, <code class="docutils literal notranslate"><span class="pre">ndarray</span></code> đã hỗ trợ sẵn một toán tử
<code class="docutils literal notranslate"><span class="pre">relu</span></code>. Như bạn thấy trong hình, hàm kích hoạt là một hàm tuyến tính
từng đoạn.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mf">8.0</span><span class="p">,</span> <span class="mf">8.0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>
<span class="n">x</span><span class="o">.</span><span class="n">attach_grad</span><span class="p">()</span>
<span class="k">with</span> <span class="n">autograd</span><span class="o">.</span><span class="n">record</span><span class="p">():</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">npx</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">set_figsize</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">))</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;relu(x)&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="figure align-default">
<img alt="../_images/output_mlp_vn_df2062_3_0.svg" src="../_images/output_mlp_vn_df2062_3_0.svg" /></div>
<!--
When the input is negative, the derivative of ReLU function is 0 and when the input is positive, the derivative of ReLU function is 1.
Note that the ReLU function is not differentiable when the input takes value precisely equal to 0.
In these cases, we default to the left-hand-side (LHS) derivative and say that the derivative is 0 when the input is 0.
We can get away with this because the input may never actually be zero.
There is an old adage that if subtle boundary conditions matter, we are probably doing (*real*) mathematics, not engineering.
That conventional wisdom may apply here.
We plot the derivative of the ReLU function plotted below.
--><p>Khi đầu vào mang giá trị âm thì đạo hàm của hàm ReLu bằng 0 và khi đầu
vào mang giá trị dương thì đạo hàm của hàm ReLu bằng 1. Lưu ý rằng, hàm
ReLU không khả vi tại 0. Trong thường hợp này, ta mặc định lấy đạo hàm
trái (<em>left-hand-side</em> – LHS) và nói rằng đạo hàm của hàm ReLU tại 0 thì
bằng 0. Chỗ này có thể du di được vì đầu vào thông thường không có giá
trị chính xác bằng không. Có một ngạn ngữ xưa nói rằng, nếu ta quan tâm
nhiều đến điều kiện biên thì có lẽ ta chỉ đang làm toán (<em>thuần túy</em>),
chứ không phải đang làm kỹ thuật. Và trong trường hợp này, ngạn ngữ đó
đúng. Đồ thị đạo hàm của hàm ReLU như hình dưới.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">y</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="p">,</span> <span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;grad of relu&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="figure align-default">
<img alt="../_images/output_mlp_vn_df2062_5_0.svg" src="../_images/output_mlp_vn_df2062_5_0.svg" /></div>
<!--
Note that there are many variants to the ReLU function, including the parameterized ReLU (pReLU) of [He et al., 2015](https://arxiv.org/abs/1502.01852).
This variation adds a linear term to the ReLU, so some information still gets through, even when the argument is negative.
--><p>Lưu ý rằng, có nhiều biến thể của hàm ReLU, bao gồm ReLU được tham số
hóa (pReLU) của <a class="reference external" href="https://arxiv.org/abs/1502.01852">He et al., 2015</a>.
Phiên bản này thêm một thành phần tuyến tính vào ReLU, do đó một số
thông tin vẫn được giữ lại ngay cả khi đối số là âm.</p>
<div class="math notranslate nohighlight" id="equation-chapter-multilayer-perceptrons-mlp-vn-6">
<span class="eqno">(4.1.7)<a class="headerlink" href="#equation-chapter-multilayer-perceptrons-mlp-vn-6" title="Permalink to this equation">¶</a></span>\[\mathrm{pReLU}(x) = \max(0, x) + \alpha \min(0, x).\]</div>
<!--
The reason for using the ReLU is that its derivatives are particularly well behaved: either they vanish or they just let the argument through.
This makes optimization better behaved and it mitigated the well-documented problem of *vanishing gradients* that plagued previous versions of neural networks (more on this later).
--><p>Ta sử dụng hàm ReLU bởi vì đạo hàm của nó khá đơn giản: hoặc là chúng
biến mất hoặc là chúng cho đối số đi qua. Điều này làm cho việc tối ưu
trở nên tốt hơn và giảm thiểu được nhược điểm <em>tiêu biến gradient</em> đã
từng gây khó khăn trong các phiên bản trước của mạng nơ-ron (sẽ được đề
cập lại sau này).</p>
<!-- ===================== Kết thúc dịch Phần 5 ===================== --><!-- ===================== Bắt đầu dịch Phần 6 ===================== --><!-- ========================================= REVISE PHẦN 3 - KẾT THÚC ===================================--><!-- ========================================= REVISE PHẦN 4 - BẮT ĐẦU =================================== --><!--
### Sigmoid Function
--></div>
<div class="section" id="ham-sigmoid">
<h3><span class="section-number">4.1.2.2. </span>Hàm Sigmoid<a class="headerlink" href="#ham-sigmoid" title="Permalink to this headline">¶</a></h3>
<!--
The sigmoid function transforms its inputs, which values lie in the domain $\mathbb{R}$, to outputs that lie the interval $(0, 1)$.
For that reason, the sigmoid is often called a *squashing* function: it *squashes* any input in the range (-inf, inf) to some value in the range (0, 1).
--><p>Hàm sigmoid biến đổi các giá trị đầu vào có miền giá trị thuộc
<span class="math notranslate nohighlight">\(\mathbb{R}\)</span> thành các giá trị đầu ra nằm trong khoảng
<span class="math notranslate nohighlight">\((0, 1)\)</span>. Vì vậy, hàm sigmoid thường được gọi là hàm <em>ép</em>: nó <em>ép</em>
một giá trị đầu vào bất kỳ nằm trong khoảng (<span class="math notranslate nohighlight">\(-\infty\)</span>,
<span class="math notranslate nohighlight">\(\infty\)</span>) thành một giá trị đầu ra nằm trong khoảng (0, 1).</p>
<div class="math notranslate nohighlight" id="equation-chapter-multilayer-perceptrons-mlp-vn-7">
<span class="eqno">(4.1.8)<a class="headerlink" href="#equation-chapter-multilayer-perceptrons-mlp-vn-7" title="Permalink to this equation">¶</a></span>\[\mathrm{sigmoid}(x) = \frac{1}{1 + \exp(-x)}.\]</div>
<!--
In the earliest neural networks, scientists were interested in modeling biological neurons which either *fire* or *do not fire*.
Thus the pioneers of this field, going all the way back to McCulloch and Pitts, the inventors of the artificial neuron, focused on thresholding units.
A thresholding activation takes value $0$ when its input is below some threshold and value $1$ when the input exceeds the threshold.
--><p>Các nơ-ron sinh học mà có thể ở một trong hai trạng thái <em>kích hoạt</em>
hoặc <em>không kích hoạt</em>, là một chủ đề mô hình hoá rất được quan tâm từ
những nghiên cứu đầu tiên về mạng nơ-ron. Vì vậy mà những người tiên
phong trong lĩnh vực này, bao gồm
<a class="reference external" href="https://en.wikipedia.org/wiki/Warren_Sturgis_McCulloch">McCulloch</a>
và <a class="reference external" href="https://en.wikipedia.org/wiki/Walter_Pitts">Pitts</a>, những người
phát minh ra nơ-ron nhân tạo, đã tập trung nghiên cứu về các đơn vị
ngưỡng. Một kích hoạt ngưỡng có giá trị là <span class="math notranslate nohighlight">\(0\)</span> khi đầu vào của nó
ở dưới mức ngưỡng và giá trị là <span class="math notranslate nohighlight">\(1\)</span> khi đầu vào vượt mức ngưỡng
đó.</p>
<!--
When attention shifted to gradient based learning, the sigmoid function was a natural choice because it is a smooth, differentiable approximation to a thresholding unit.
Sigmoids are still widely used as activation functions on the output units,
when we want to interpret the outputs as probabilities for binary classification problems (you can think of the sigmoid as a special case of the softmax).
However, the sigmoid has mostly been replaced by the simpler and more easily trainable ReLU for most use in hidden layers.
In the "Recurrent Neural Network" chapter (:numref:`sec_plain_rnn`), we will describe architectures that leverage sigmoid units to control the flow of information across time.
--><p>Khi phương pháp học dựa trên gradient trở nên phổ biến, hàm sigmoid là
một lựa chọn tất yếu của đơn vị ngưỡng bởi tính liên tục và khả vi của
nó. Hàm sigmoid vẫn là hàm kích hoạt được sử dụng rộng rãi ở các đơn vị
đầu ra, khi ta muốn biểu diễn kết quả đầu ra như là các xác suất của bài
toán phân loại nhị phân (bạn có thể xem sigmoid như một trường hợp đặc
biệt của softmax). Tuy nhiên, trong các tầng ẩn, hàm sigmoid hầu hết bị
thay thế bằng hàm ReLU vì nó đơn giản hơn và giúp cho việc huấn luyện
trở nên dễ dàng hơn. Trong chương “Mạng nơ-ron hồi tiếp”
(<a class="reference internal" href="../chapter_recurrent-neural-networks/rnn_vn.html#sec-plain-rnn"><span class="std std-numref">Section 8.4</span></a>), chúng tôi sẽ mô tả các mô hình sử dụng đơn
vị sigmoid để kiểm soát luồng thông tin theo thời gian.</p>
<!--
Below, we plot the sigmoid function.
Note that when the input is close to 0, the sigmoid function approaches a linear transformation.
--><p>Dưới đây, ta vẽ đồ thị hàm sigmoid. Cần chú ý rằng, khi đầu vào có giá
trị gần bằng 0, hàm sigmoid tiến tới một phép biến đổi tuyến tính.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">autograd</span><span class="o">.</span><span class="n">record</span><span class="p">():</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">npx</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;sigmoid(x)&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="figure align-default">
<img alt="../_images/output_mlp_vn_df2062_7_0.svg" src="../_images/output_mlp_vn_df2062_7_0.svg" /></div>
<!--
The derivative of sigmoid function is given by the following equation:
--><p>Đạo hàm của hàm sigmoid được tính bởi phương trình sau:</p>
<div class="math notranslate nohighlight" id="equation-chapter-multilayer-perceptrons-mlp-vn-8">
<span class="eqno">(4.1.9)<a class="headerlink" href="#equation-chapter-multilayer-perceptrons-mlp-vn-8" title="Permalink to this equation">¶</a></span>\[\frac{d}{dx} \mathrm{sigmoid}(x) = \frac{\exp(-x)}{(1 + \exp(-x))^2} = \mathrm{sigmoid}(x)\left(1-\mathrm{sigmoid}(x)\right).\]</div>
<!--
The derivative of sigmoid function is plotted below.
Note that when the input is 0, the derivative of the sigmoid function reaches a maximum of 0.25.
As the input diverges from 0 in either direction, the derivative approaches 0.
--><p>Đồ thị đạo hàm của hàm sigmoid được vẽ ở dưới. Chú ý rằng khi đầu vào là
0, đạo hàm của hàm sigmoid đạt giá trị lớn nhất là 0.25. Khi đầu vào
phân kỳ từ 0 theo một trong hai hướng, đạo hàm sẽ tiến tới 0.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">y</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="p">,</span> <span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;grad of sigmoid&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="figure align-default">
<img alt="../_images/output_mlp_vn_df2062_9_0.svg" src="../_images/output_mlp_vn_df2062_9_0.svg" /></div>
<!-- ===================== Kết thúc dịch Phần 6 ===================== --><!-- ===================== Bắt đầu dịch Phần 7 ===================== --><!--
### Tanh Function
--></div>
<div class="section" id="ham-tanh">
<h3><span class="section-number">4.1.2.3. </span>Hàm “Tanh”<a class="headerlink" href="#ham-tanh" title="Permalink to this headline">¶</a></h3>
<!--
Like the sigmoid function, the tanh (Hyperbolic Tangent) function also squashes its inputs, transforms them into elements on the interval between -1 and 1:
--><p>Tương tự như hàm sigmoid, hàm tanh (Hyperbolic Tangent) cũng ép các biến
đầu vào và biến đổi chúng thành các phần tử nằm trong khoảng -1 và 1:</p>
<div class="math notranslate nohighlight" id="equation-chapter-multilayer-perceptrons-mlp-vn-9">
<span class="eqno">(4.1.10)<a class="headerlink" href="#equation-chapter-multilayer-perceptrons-mlp-vn-9" title="Permalink to this equation">¶</a></span>\[\text{tanh}(x) = \frac{1 - \exp(-2x)}{1 + \exp(-2x)}.\]</div>
<!--
We plot the tanh function blow. Note that as the input nears 0, the tanh function approaches a linear transformation.
Although the shape of the function is similar to the sigmoid function, the tanh function exhibits point symmetry about the origin of the coordinate system.
--><p>Chúng ta sẽ vẽ hàm tanh như sau. Chú ý rằng nếu đầu vào có giá trị gần
bằng 0, hàm tanh sẽ tiến đến một phép biến đổi tuyến tính. Mặc dù hình
dạng của hàm tanh trông khá giống hàm sigmoid, hàm tanh lại thể hiện
tính đối xứng tâm qua gốc của hệ trục tọa độ.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">autograd</span><span class="o">.</span><span class="n">record</span><span class="p">():</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;tanh(x)&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="figure align-default">
<img alt="../_images/output_mlp_vn_df2062_11_0.svg" src="../_images/output_mlp_vn_df2062_11_0.svg" /></div>
<!--
The derivative of the Tanh function is:
--><p>Đạo hàm của hàm Tanh là:</p>
<div class="math notranslate nohighlight" id="equation-chapter-multilayer-perceptrons-mlp-vn-10">
<span class="eqno">(4.1.11)<a class="headerlink" href="#equation-chapter-multilayer-perceptrons-mlp-vn-10" title="Permalink to this equation">¶</a></span>\[\frac{d}{dx} \mathrm{tanh}(x) = 1 - \mathrm{tanh}^2(x).\]</div>
<!--
The derivative of tanh function is plotted below.
As the input nears 0, the derivative of the tanh function approaches a maximum of 1.
And as we saw with the sigmoid function, as the input moves away from 0 in either direction, the derivative of the tanh function approaches 0.
--><p>Đạo hàm của hàm tanh được vẽ như sau. Khi đầu vào có giá trị gần bằng 0,
đạo hàm của hàm tanh tiến tới giá trị lớn nhất là 1. Tương tự như hàm
sigmoid, khi đầu vào phân kỳ từ 0 theo bất kỳ hướng nào, đạo hàm của hàm
tanh sẽ tiến đến 0.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">y</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="p">,</span> <span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;grad of tanh&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="figure align-default">
<img alt="../_images/output_mlp_vn_df2062_13_0.svg" src="../_images/output_mlp_vn_df2062_13_0.svg" /></div>
<!--
In summary, we now know how to incorporate nonlinearities to build expressive multilayer neural network architectures.
As a side note, your knowledge already puts you in command of a similar toolkit to a practitioner circa 1990.
In some ways, you have an advantage over anyone working the 1990s, because you can leverage powerful open-source deep learning frameworks to build models rapidly, using only a few lines of code.
Previously, getting these nets training required researchers to code up thousands of lines of C and Fortran.
--><p>Tóm lại, bây giờ chúng ta đã biết cách kết hợp các hàm phi tuyến để xây
dựng các kiến trúc mạng nơ-ron đa tầng mạnh mẽ. Một lưu ý bên lề đó là,
kiến thức của bạn bây giờ cung cấp cho bạn cách sử dụng một bộ công cụ
tương đương với của một người có chuyên môn về học sâu vào những năm
1990. Xét theo một khía cạnh nào đó, bạn còn có lợi thế hơn bất kỳ ai
làm việc trong những năm 1990, bởi vì bạn có thể tận dụng triệt để các
framework học sâu nguồn mở để xây dựng các mô hình một cách nhanh chóng,
chỉ với một vài dòng mã. Trước đây, việc huấn luyện các mạng nơ-ron đòi
hỏi các nhà nghiên cứu phải viết đến hàng ngàn dòng mã C và Fortran.</p>
<!-- ===================== Kết thúc dịch Phần 7 ===================== --><!-- ===================== Bắt đầu dịch Phần 8 ===================== --><!--
## Summary
--></div>
</div>
<div class="section" id="tom-tat">
<h2><span class="section-number">4.1.3. </span>Tóm tắt<a class="headerlink" href="#tom-tat" title="Permalink to this headline">¶</a></h2>
<!--
* The multilayer perceptron adds one or multiple fully-connected hidden layers between the output and input layers and transforms the output of the hidden layer via an activation function.
* Commonly-used activation functions include the ReLU function, the sigmoid function, and the tanh function.
--><ul class="simple">
<li>Perceptron đa tầng sẽ thêm một hoặc nhiều tầng ẩn được kết nối đầy đủ
giữa các tầng đầu ra và các tầng đầu vào nhằm biến đổi đầu ra của
tầng ẩn thông qua hàm kích hoạt.</li>
<li>Các hàm kích hoạt thường được sử dụng bao gồm hàm ReLU, hàm sigmoid,
và hàm tanh.</li>
</ul>
<!--
## Exercises
--></div>
<div class="section" id="bai-tap">
<h2><span class="section-number">4.1.4. </span>Bài tập<a class="headerlink" href="#bai-tap" title="Permalink to this headline">¶</a></h2>
<!--
1. Compute the derivative of the tanh and the pReLU activation function.
2. Show that a multilayer perceptron using only ReLU (or pReLU) constructs a continuous piecewise linear function.
3. Show that $\mathrm{tanh}(x) + 1 = 2 \mathrm{sigmoid}(2x)$.
4. Assume we have a multilayer perceptron *without* nonlinearities between the layers.
In particular, assume that we have $d$ input dimensions, $d$ output dimensions and that one of the layers had only $d/2$ dimensions.
Show that this network is less expressive (powerful) than a single layer perceptron.
5. Assume that we have a nonlinearity that applies to one minibatch at a time. What kinds of problems do you expect this to cause?
--><ol class="arabic simple">
<li>Tính đạo hàm của hàm kích hoạt tanh và pReLU.</li>
<li>Chứng minh rằng một perceptron đa tầng chỉ sử dụng ReLU (hoặc pReLU)
sẽ tạo thành một hàm tuyến tính từng đoạn liên tục.</li>
<li>Chứng minh rằng
<span class="math notranslate nohighlight">\(\mathrm{tanh}(x) + 1 = 2 \mathrm{sigmoid}(2x)\)</span>.</li>
<li>Giả sử ta có một perceptron đa tầng mà <em>không có</em> tính phi tuyến giữa
các tầng. Cụ thể là, giả sử ta có chiều của đầu vào <span class="math notranslate nohighlight">\(d\)</span>, chiều
đầu ra <span class="math notranslate nohighlight">\(d\)</span> và tầng ẩn có chiều <span class="math notranslate nohighlight">\(d/2\)</span>. Chứng minh rằng
mạng này có ít khả năng biểu diễn hơn một perceptron đơn tầng.</li>
<li>Giả sử ta có một hàm phi tuyến tính áp dụng cho từng minibatch mỗi
lúc. Việc này sẽ dẫn đến vấn đề gì?</li>
</ol>
<!-- ===================== Kết thúc dịch Phần 8 ===================== --><!-- ========================================= REVISE PHẦN 4 - KẾT THÚC ===================================--><!--
## [Discussions](https://discuss.mxnet.io/t/2338)
--></div>
<div class="section" id="thao-luan">
<h2><span class="section-number">4.1.5. </span>Thảo luận<a class="headerlink" href="#thao-luan" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><a class="reference external" href="https://discuss.mxnet.io/t/2338">Tiếng Anh</a></li>
<li><a class="reference external" href="https://forum.machinelearningcoban.com/c/d2l">Tiếng Việt</a></li>
</ul>
</div>
<div class="section" id="nhung-nguoi-thuc-hien">
<h2><span class="section-number">4.1.6. </span>Những người thực hiện<a class="headerlink" href="#nhung-nguoi-thuc-hien" title="Permalink to this headline">¶</a></h2>
<p>Bản dịch trong trang này được thực hiện bởi:</p>
<ul class="simple">
<li>Đoàn Võ Duy Thanh</li>
<li>Đinh Minh Tân</li>
<li>Phạm Minh Đức</li>
<li>Vũ Hữu Tiệp</li>
<li>Nguyễn Lê Quang Nhật</li>
<li>Nguyễn Minh Thư</li>
<li>Nguyễn Duy Du</li>
<li>Phạm Hồng Vinh</li>
<li>Lê Cao Thăng</li>
<li>Lý Phi Long</li>
<li>Lê Khắc Hồng Phúc</li>
<li>Lâm Ngọc Tâm</li>
<li>Bùi Nhật Quân</li>
</ul>
</div>
</div>


        </div>
        <div class="side-doc-outline">
            <div class="side-doc-outline--content"> 
<div class="localtoc">
    <p class="caption">
      <span class="caption-text">Table Of Contents</span>
    </p>
    <ul>
<li><a class="reference internal" href="#">4.1. Perceptron đa tầng</a><ul>
<li><a class="reference internal" href="#cac-tang-an">4.1.1. Các tầng ẩn</a><ul>
<li><a class="reference internal" href="#ket-hop-cac-tang-an">4.1.1.1. Kết hợp các Tầng ẩn</a></li>
<li><a class="reference internal" href="#tu-tuyen-tinh-den-phi-tuyen">4.1.1.2. Từ Tuyến tính đến Phi tuyến</a></li>
<li><a class="reference internal" href="#vector-hoa-va-minibatch">4.1.1.3. Vector hoá và Minibatch</a></li>
</ul>
</li>
<li><a class="reference internal" href="#cac-ham-kich-hoat">4.1.2. Các hàm Kích hoạt</a><ul>
<li><a class="reference internal" href="#ham-relu">4.1.2.1. Hàm ReLU</a></li>
<li><a class="reference internal" href="#ham-sigmoid">4.1.2.2. Hàm Sigmoid</a></li>
<li><a class="reference internal" href="#ham-tanh">4.1.2.3. Hàm “Tanh”</a></li>
</ul>
</li>
<li><a class="reference internal" href="#tom-tat">4.1.3. Tóm tắt</a></li>
<li><a class="reference internal" href="#bai-tap">4.1.4. Bài tập</a></li>
<li><a class="reference internal" href="#thao-luan">4.1.5. Thảo luận</a></li>
<li><a class="reference internal" href="#nhung-nguoi-thuc-hien">4.1.6. Những người thực hiện</a></li>
</ul>
</li>
</ul>

</div>
            </div>
        </div>

      <div class="clearer"></div>
    </div><div class="pagenation">
     <a id="button-prev" href="index_vn.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="P">
         <i class="pagenation-arrow-L fas fa-arrow-left fa-lg"></i>
         <div class="pagenation-text">
            <span class="pagenation-direction">Previous</span>
            <div>4. Perceptron Đa tầng</div>
         </div>
     </a>
     <a id="button-next" href="mlp-scratch_vn.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="N">
         <i class="pagenation-arrow-R fas fa-arrow-right fa-lg"></i>
        <div class="pagenation-text">
            <span class="pagenation-direction">Next</span>
            <div>4.2. Lập trình Perceptron Đa tầng từ đầu</div>
        </div>
     </a>
  </div>
        
        </main>
    </div>
  </body>
</html>