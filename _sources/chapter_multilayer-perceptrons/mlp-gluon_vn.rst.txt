
.. raw:: html

   <!-- ===================== Bắt đầu phần dịch===================== -->

.. raw:: html

   <!-- ========================================= REVISE - BẮT ĐẦU =================================== -->

.. raw:: html

   <!--
   # Concise Implementation of Multilayer Perceptron
   -->

.. _sec_mlp_gluon:

Cách lập trình súc tích Perceptron Đa tầng
==========================================


.. raw:: html

   <!--
   As you might expect, by relying on the Gluon library, we can implement MLPs even more concisely.
   -->

Như bạn đã có thể đoán trước, ta có thể dựa vào thư viện Gluon để lập
trình MLP một cách súc tích hơn.

.. code:: python

    from d2l import mxnet as d2l
    from mxnet import gluon, init, npx
    from mxnet.gluon import nn
    npx.set_np()

.. raw:: html

   <!--
   ## The Model
   -->

Mô hình
-------

.. raw:: html

   <!--
   As compared to our gluon implementation of softmax regression implementation (:numref:`sec_softmax_gluon`), 
   the only difference is that we add *two* `Dense` (fully-connected) layers (previously, we added *one*).
   The first is our hidden layer, which contains *256* hidden units and applies the ReLU activation function.
   The second, is our output layer.
   -->

So với việc dùng gluon để lập trình hồi quy softmax
(:numref:`sec_softmax_gluon`), khác biệt duy nhất ở đây là ta thêm
*hai* tầng ``Dense`` (kết nối đầy đủ), trong khi trước đây ta chỉ có
*một*. Tầng đầu tiên là tầng ẩn, chứa *256* nút ẩn và áp dụng hàm kích
hoạt ReLU. Còn tầng thứ hai là tầng đầu ra.

.. code:: python

    net = nn.Sequential()
    net.add(nn.Dense(256, activation='relu'),
            nn.Dense(10))
    net.initialize(init.Normal(sigma=0.01))

.. raw:: html

   <!--
   Note that Gluon, as usual, automatically infers the missing input dimensions to each layer.
   -->

Lưu ý rằng như thường lệ, Gluon sẽ tự động suy ra chiều đầu vào còn
thiếu cho mỗi tầng.

.. raw:: html

   <!--
   The training loop is *exactly* the same as when we implemented softmax regression.
   This modularity enables us to separate matterns concerning the model architecture from orthogonal considerations.
   -->

Vòng lặp huấn luyện ở đây giống *hệt* như lúc ta lập trình hồi quy
softmax. Lập trình hướng mô-đun như vậy cho phép ta tách các chi tiết
liên quan đến kiến trúc của mô hình ra khỏi các mối bận tâm khác.

.. code:: python

    batch_size, num_epochs = 256, 10
    train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
    loss = gluon.loss.SoftmaxCrossEntropyLoss()
    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.5})
    d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)



.. figure:: output_mlp-gluon_vn_b38e32_5_0.svg


.. raw:: html

   <!--
   ## Exercises
   -->

Bài tập
-------

.. raw:: html

   <!--
   1. Try adding different numbers of hidden layers. What setting (keeping other parameters and hyperparameters constant) works best?
   2. Try out different activation functions. Which ones work best?
   3. Try different schemes for initializing the weights. What method works best?
   -->

1. Bằng việc thử thêm số lượng các tầng ẩn khác nhau, bạn hãy xem thiết
   lập nào cho kết quả tốt nhất (giữ nguyên giá trị các tham số và siêu
   tham số khác)?
2. Bằng việc thử thay đổi các hàm kích hoạt khác nhau, bạn hãy chỉ ra
   hàm nào mang lại kết quả tốt nhất?
3. Bạn hãy thử các cách khác nhau để khởi tạo trọng số. Phương pháp nào
   là tốt nhất?

.. raw:: html

   <!-- ===================== Kết thúc phần dịch ===================== -->

.. raw:: html

   <!-- ========================================= REVISE - KẾT THÚC ===================================-->

.. raw:: html

   <!--
   ## [Discussions](https://discuss.mxnet.io/t/2340)
   -->

Thảo luận
---------

-  `Tiếng Anh <https://discuss.mxnet.io/t/2340>`__
-  `Tiếng Việt <https://forum.machinelearningcoban.com/c/d2l>`__

Những người thực hiện
---------------------

Bản dịch trong trang này được thực hiện bởi:

-  Đoàn Võ Duy Thanh
-  Lý Phi Long
-  Vũ Hữu Tiệp
-  Phạm Hồng Vinh
-  Lê Khắc Hồng Phúc
-  Phạm Minh Đức
