<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    
    <title>8.1. Mô hình chuỗi &#8212; Đắm mình vào Học Sâu 0.14.4 documentation</title>

    <link rel="stylesheet" href="../_static/basic.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/d2l.css" />
    <link rel="stylesheet" href="../_static/material-design-lite-1.3.0/material.blue-deep_orange.min.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx_materialdesign_theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fontawesome/all.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fonts.css" type="text/css" />
    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/language_data.js"></script>
    <script src="../_static/d2l.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="shortcut icon" href="../_static/favicon.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="8.2. Tiền Xử lý Dữ liệu Văn bản" href="text-preprocessing_vn.html" />
    <link rel="prev" title="8. Mạng Nơ-ron Hồi tiếp" href="index_vn.html" /> 
  </head>
<body>
    <div class="mdl-layout mdl-js-layout mdl-layout--fixed-header mdl-layout--fixed-drawer"><header class="mdl-layout__header mdl-layout__header--waterfall ">
    <div class="mdl-layout__header-row">
        
        <nav class="mdl-navigation breadcrumb">
            <a class="mdl-navigation__link" href="index_vn.html"><span class="section-number">8. </span>Mạng Nơ-ron Hồi tiếp</a><i class="material-icons">navigate_next</i>
            <a class="mdl-navigation__link is-active"><span class="section-number">8.1. </span>Mô hình chuỗi</a>
        </nav>
        <div class="mdl-layout-spacer"></div>
        <nav class="mdl-navigation">
        
<form class="form-inline pull-sm-right" action="../search.html" method="get">
      <div class="mdl-textfield mdl-js-textfield mdl-textfield--expandable mdl-textfield--floating-label mdl-textfield--align-right">
        <label id="quick-search-icon" class="mdl-button mdl-js-button mdl-button--icon"  for="waterfall-exp">
          <i class="material-icons">search</i>
        </label>
        <div class="mdl-textfield__expandable-holder">
          <input class="mdl-textfield__input" type="text" name="q"  id="waterfall-exp" placeholder="Search" />
          <input type="hidden" name="check_keywords" value="yes" />
          <input type="hidden" name="area" value="default" />
        </div>
      </div>
      <div class="mdl-tooltip" data-mdl-for="quick-search-icon">
      Quick search
      </div>
</form>
        
<a id="button-show-source"
    class="mdl-button mdl-js-button mdl-button--icon"
    href="../_sources/chapter_recurrent-neural-networks/sequence_vn.rst.txt" rel="nofollow">
  <i class="material-icons">code</i>
</a>
<div class="mdl-tooltip" data-mdl-for="button-show-source">
Show Source
</div>
        </nav>
    </div>
    <div class="mdl-layout__header-row header-links">
      <div class="mdl-layout-spacer"></div>
      <nav class="mdl-navigation">
          
              <a  class="mdl-navigation__link" href="https://github.com/aivivn/d2l-vn">
                  <i class="fab fa-github"></i>
                  GitHub
              </a>
          
              <a  class="mdl-navigation__link" href="https://forum.machinelearningcoban.com/">
                  <i class="fab fa-discourse"></i>
                  Forum
              </a>
          
              <a  class="mdl-navigation__link" href="https://www.d2l.ai/">
                  <i class="fas fa-external-link-alt"></i>
                  English
              </a>
      </nav>
    </div>
</header><header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <img class="logo" src="../_static/logo-with-text-vi.png" alt="Đắm mình vào Học Sâu"/>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul>
<li class="toctree-l1"><a class="reference internal" href="../intro_vn.html">Giới thiệu từ nhóm dịch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface/index_vn.html">Lời nói đầu</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_install/index_vn.html">Cài đặt</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_notation/index_vn.html">Ký hiệu</a></li>
</ul>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../chapter_introduction/index_vn.html">1. Giới thiệu</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preliminaries/index_vn.html">2. Sơ bộ</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/ndarray_vn.html">2.1. Thao tác với Dữ liệu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/pandas_vn.html">2.2. Tiền xử lý dữ liệu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra_vn.html">2.3. Đại số tuyến tính</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/calculus_vn.html">2.4. Giải tích</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/autograd_vn.html">2.5. Tính vi phân Tự động</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/probability_vn.html">2.6. Xác suất</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/lookup-api_vn.html">2.7. Tài liệu</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_linear-networks/index_vn.html">3. Mạng nơ-ron Tuyến tính</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/linear-regression_vn.html">3.1. Hồi quy Tuyến tính</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/linear-regression-scratch_vn.html">3.2. Lập trình Hồi quy Tuyến tính từ đầu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/linear-regression-gluon_vn.html">3.3. Cách lập trình súc tích Hồi quy Tuyến tính</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/softmax-regression_vn.html">3.4. Hồi quy Softmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/fashion-mnist_vn.html">3.5. Bộ dữ liệu Phân loại Ảnh (Fashion-MNIST)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/softmax-regression-scratch_vn.html">3.6. Lập trình Hồi quy Sofmax từ đầu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/softmax-regression-gluon_vn.html">3.7. Cách lập trình súc tích Hồi quy Softmax</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_multilayer-perceptrons/index_vn.html">4. Perceptron Đa tầng</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/mlp_vn.html">4.1. Perceptron đa tầng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/mlp-scratch_vn.html">4.2. Lập trình Perceptron Đa tầng từ đầu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/mlp-gluon_vn.html">4.3. Cách lập trình súc tích Perceptron Đa tầng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/underfit-overfit_vn.html">4.4. Lựa Chọn Mô Hình, Dưới Khớp và Quá Khớp</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/weight-decay_vn.html">4.5. Suy giảm trọng số</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/dropout_vn.html">4.6. Dropout</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/backprop_vn.html">4.7. Lan truyền xuôi, Lan truyền ngược và Đồ thị tính toán</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/numerical-stability-and-init_vn.html">4.8. Ổn định Số học và Khởi tạo</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/environment_vn.html">4.9. Cân nhắc tới Môi trường</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/kaggle-house-price_vn.html">4.10. Dự đoán Giá Nhà trên Kaggle</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_deep-learning-computation/index_vn.html">5. Tính toán Học sâu</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_deep-learning-computation/model-construction_vn.html">5.1. Tầng và Khối</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_deep-learning-computation/parameters_vn.html">5.2. Quản lý Tham số</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_deep-learning-computation/deferred-init_vn.html">5.3. Khởi tạo trễ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_deep-learning-computation/custom-layer_vn.html">5.4. Các tầng Tuỳ chỉnh</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_deep-learning-computation/read-write_vn.html">5.5. Đọc/Ghi tệp</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_deep-learning-computation/use-gpu_vn.html">5.6. GPU</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_convolutional-neural-networks/index_vn.html">6. Mạng Nơ-ron Tích chập</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/why-conv_vn.html">6.1. Từ Tầng Kết nối Dày đặc đến phép Tích chập</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/conv-layer_vn.html">6.2. Phép Tích chập cho Ảnh</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/padding-and-strides_vn.html">6.3. Đệm và Sải Bước</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/channels_vn.html">6.4. Đa kênh Đầu vào và Đầu ra</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/pooling_vn.html">6.5. Gộp (<em>Pooling</em>)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/lenet_vn.html">6.6. Mạng Nơ-ron Tích chập (LeNet)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_convolutional-modern/index_vn.html">7. Mạng Nơ-ron Tích chập Hiện đại</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/alexnet_vn.html">7.1. Mạng Nơ-ron Tích chập Sâu (AlexNet)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/vgg_vn.html">7.2. Mạng sử dụng Khối (VGG)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/nin_vn.html">7.3. Mạng trong Mạng (<em>Network in Network - NiN</em>)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/googlenet_vn.html">7.4. Mạng nối song song (GoogLeNet)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/batch-norm_vn.html">7.5. Chuẩn hoá theo batch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/resnet_vn.html">7.6. Mạng phần dư (ResNet)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/densenet_vn.html">7.7. Mạng Tích chập Kết nối Dày đặc (DenseNet)</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="index_vn.html">8. Mạng Nơ-ron Hồi tiếp</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">8.1. Mô hình chuỗi</a></li>
<li class="toctree-l2"><a class="reference internal" href="text-preprocessing_vn.html">8.2. Tiền Xử lý Dữ liệu Văn bản</a></li>
<li class="toctree-l2"><a class="reference internal" href="language-models-and-dataset_vn.html">8.3. Mô hình Ngôn ngữ và Tập dữ liệu</a></li>
<li class="toctree-l2"><a class="reference internal" href="rnn_vn.html">8.4. Mạng nơ-ron Hồi tiếp</a></li>
<li class="toctree-l2"><a class="reference internal" href="rnn-scratch_vn.html">8.5. Lập trình Mạng nơ-ron Hồi tiếp từ đầu</a></li>
<li class="toctree-l2"><a class="reference internal" href="rnn-gluon_vn.html">8.6. Lập trình súc tích Mạng nơ-ron Hồi tiếp</a></li>
<li class="toctree-l2"><a class="reference internal" href="bptt_vn.html">8.7. Lan truyền Ngược qua Thời gian</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_recurrent-modern/index_vn.html">9. Mạng Nơ-ron Hồi tiếp Hiện đại</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/gru_vn.html">9.1. Nút Hồi tiếp có Cổng (GRU)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/lstm_vn.html">9.2. Bộ nhớ Ngắn hạn Dài (LSTM)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/deep-rnn_vn.html">9.3. Mạng Nơ-ron Hồi tiếp Sâu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/bi-rnn_vn.html">9.4. Mạng Nơ-ron Hồi tiếp Hai chiều</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/machine-translation-and-dataset_vn.html">9.5. Dịch Máy và Tập dữ liệu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/encoder-decoder_vn.html">9.6. Kiến trúc Mã hoá - Giải mã</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/seq2seq_vn.html">9.7. Chuỗi sang Chuỗi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/beam-search_vn.html">9.8. Tìm kiếm Chùm</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_attention-mechanisms/index_vn.html">10. Cơ chế Tập trung</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms/attention_vn.html">10.1. Cơ chế Tập trung</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms/seq2seq-attention_vn.html">10.2. Chuỗi sang Chuỗi áp dụng Cơ chế Tập trung</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms/transformer_vn.html">10.3. Kiến trúc Transformer</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_optimization/index_vn.html">11. Thuật toán Tối ưu</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/optimization-intro_vn.html">11.1. Tối ưu và Học sâu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/optimization-intro_vn.html#cac-thach-thuc-cua-toi-uu-trong-hoc-sau">11.2. Các Thách thức của Tối ưu trong Học sâu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/optimization-intro_vn.html#cac-vung-cuc-tieu">11.3. Các vùng Cực tiểu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/optimization-intro_vn.html#cac-diem-yen-ngua">11.4. Các điểm Yên ngựa</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/optimization-intro_vn.html#tieu-bien-gradient">11.5. Tiêu biến Gradient</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/convexity_vn.html">11.6. Tính lồi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/gd_vn.html">11.7. Hạ Gradient</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/sgd_vn.html">11.8. Hạ Gradient Ngẫu nhiên</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/minibatch-sgd_vn.html">11.9. Hạ Gradient Ngẫu nhiên theo Minibatch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/momentum_vn.html">11.10. Động lượng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/adagrad_vn.html">11.11. Adagrad</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/rmsprop_vn.html">11.12. RMSProp</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/adadelta_vn.html">11.13. Adadelta</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/adam_vn.html">11.14. Adam</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/lr-scheduler_vn.html">11.15. Định thời Tốc độ Học</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_computational-performance/index_vn.html">12. Hiệu năng Tính toán</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/hybridize_vn.html">12.1. Trình biên dịch và Trình thông dịch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/async-computation_vn.html">12.2. Tính toán Bất đồng bộ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/auto-parallelism_vn.html">12.3. Song song hóa Tự động</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/hardware_vn.html">12.4. Phần cứng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/multiple-gpus_vn.html">12.5. Huấn luyện đa GPU</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/multiple-gpus-concise_vn.html">12.6. Cách lập trình Súc tích đa GPU</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/parameterserver_vn.html">12.7. Máy chủ Tham số</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_computer-vision/index_vn.html">13. Thị giác Máy tính</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/image-augmentation_vn.html">13.1. Tăng cường Ảnh</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/fine-tuning_vn.html">13.2. Tinh Chỉnh</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/bounding-box_vn.html">13.3. Phát hiện Vật thể và Khoanh vùng Đối tượng (Khung chứa)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/anchor_vn.html">13.4. Khung neo</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/multiscale-object-detection_vn.html">13.5. Phát hiện Vật thể Đa tỷ lệ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/object-detection-dataset_vn.html">13.6. Tập dữ liệu Phát hiện Đối tượng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/ssd_vn.html">13.7. Phát hiện Nhiều khung Một lượt (SSD)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/rcnn_vn.html">13.8. CNN theo Vùng (R-CNN)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/semantic-segmentation-and-dataset_vn.html">13.9. Phân vùng theo Ngữ nghĩa và Tập dữ liệu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/transposed-conv_vn.html">13.10. Tích chập Chuyển vị</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/fcn_vn.html">13.11. Mạng Tích chập Đầy đủ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/neural-style_vn.html">13.12. Truyền tải Phong cách Nơ-ron</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/kaggle-cifar10_vn.html">13.13. Phân loại ảnh (CIFAR-10) trên Kaggle</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/kaggle-dog_vn.html">13.14. Nhận diện Giống Chó (ImageNet Dogs) trên Kaggle</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/index_vn.html">14. Xử lý Ngôn ngữ Tự nhiên: Tiền Huấn luyện</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/word2vec_vn.html">14.1. Embedding Từ (word2vec)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/approx-training_vn.html">14.2. Huấn luyện Gần đúng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/word-embedding-dataset_vn.html">14.3. Tập dữ liệu để Tiền Huấn luyện Embedding Từ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/word2vec-pretraining_vn.html">14.4. Tiền huấn luyện word2vec</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/glove_vn.html">14.5. Embedding từ với Vector Toàn cục (GloVe)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/subword-embedding_vn.html">14.6. Embedding từ con</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/similarity-analogy_vn.html">14.7. Tìm kiếm từ Đồng nghĩa và Loại suy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/bert_vn.html">14.8. Biểu diễn Mã hóa hai chiều từ Transformer (BERT)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/bert-dataset_vn.html">14.9. Tập dữ liệu để Tiền huấn luyện BERT</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/bert-pretraining_vn.html">14.10. Tiền Huấn luyện BERT</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_natural-language-processing-applications/index_vn.html">15. Xử lý Ngôn ngữ Tự nhiên: Ứng dụng</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/sentiment-analysis-and-dataset_vn.html">15.1. Tác vụ Phân tích Cảm xúc và Bộ Dữ liệu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/sentiment-analysis-rnn_vn.html">15.2. Phân tích Cảm xúc: Sử dụng Mạng Nơ-ron Hồi tiếp</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/sentiment-analysis-cnn_vn.html">15.3. Phân tích Cảm xúc: Sử dụng Mạng Nơ-ron Tích Chập</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/natural-language-inference-and-dataset_vn.html">15.4. Suy luận ngôn ngữ tự nhiên và Tập dữ liệu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/natural-language-inference-attention_vn.html">15.5. Suy luận Ngôn ngữ Tự nhiên: Sử dụng Cơ chế Tập trung</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/finetuning-bert_vn.html">15.6. Tinh chỉnh BERT cho các Ứng dụng Cấp Chuỗi và Cấp Token</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/natural-language-inference-bert_vn.html">15.7. Suy luận Ngôn ngữ Tự nhiên: Tinh chỉnh BERT</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_recommender-systems/index_vn.html">16. Hệ thống Đề xuất</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/recsys-intro_vn.html">16.1. Tổng quan về Hệ thống Đề xuất</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/movielens_vn.html">16.2. Tập dữ liệu MovieLens</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/mf_vn.html">16.3. Phân rã Ma trận</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/autorec_vn.html">16.4. AutoRec: Dự đoán Đánh giá với Bộ tự Mã hóa</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/ranking_vn.html">16.5. Cá nhân hóa Xếp hạng trong Hệ thống Đề xuất</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/neumf_vn.html">16.6. Lọc Cộng tác Nơ-ron cho Cá nhân hóa Xếp hạng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/seqrec_vn.html">16.7. Hệ thống Đề xuất có Nhận thức về Chuỗi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/ctr_vn.html">16.8. Hệ thống Đề xuất Giàu Đặc trưng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/fm_vn.html">16.9. Máy Phân rã ma trận</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/deepfm_vn.html">16.10. Máy Phân rã Ma trận Sâu</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_generative-adversarial-networks/index_vn.html">17. Mạng Đối sinh</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_generative-adversarial-networks/gan_vn.html">17.1. Mạng Đối sinh</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_generative-adversarial-networks/dcgan_vn.html">17.2. Mạng Đối sinh Tích chập Sâu</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/index_vn.html">18. Phụ lục: Toán học cho Học Sâu</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/geometry-linear-algebraic-ops_vn.html">18.1. Các phép toán Hình học và Đại số Tuyến tính</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/eigendecomposition_vn.html">18.2. Phân rã trị riêng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/single-variable-calculus_vn.html">18.3. Giải tích một biến</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/multivariable-calculus_vn.html">18.4. Giải tích Nhiều biến</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/integral-calculus_vn.html">18.5. Giải tích Tích phân</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/random-variables_vn.html">18.6. Biến Ngẫu nhiên</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/maximum-likelihood_vn.html">18.7. Hợp lý Cực đại</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/distributions_vn.html">18.8. Các Phân phối Xác suất</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/naive-bayes_vn.html">18.9. Bộ phân loại Naive Bayes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/statistics_vn.html">18.10. Thống kê</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/information-theory_vn.html">18.11. Lý thuyết Thông tin</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/index_vn.html">19. Phụ lục: Công cụ cho Học Sâu</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/jupyter_vn.html">19.1. Sử dụng Jupyter</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/sagemaker_vn.html">19.2. Sử dụng Amazon SageMaker</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/aws_vn.html">19.3. Sử dụng Máy ảo AWS EC2</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/colab_vn.html">19.4. Sử dụng Google Colab</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/selecting-servers-gpus_vn.html">19.5. Lựa chọn Máy chủ &amp; GPU</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/contributing_vn.html">19.6. Đóng góp cho Quyển sách</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/d2l_vn.html">19.7. Tài liệu API của <code class="docutils literal notranslate"><span class="pre">d2l</span></code></a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../chapter_references/zreferences.html">Tài liệu tham khảo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../glossary.html">Bảng thuật ngữ</a></li>
</ul>

            </nav>
        
        </div>
    
</header>
        <main class="mdl-layout__content" tabIndex="0">

	<script type="text/javascript" src="../_static/sphinx_materialdesign_theme.js "></script>
    <header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <img class="logo" src="../_static/logo-with-text-vi.png" alt="Đắm mình vào Học Sâu"/>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul>
<li class="toctree-l1"><a class="reference internal" href="../intro_vn.html">Giới thiệu từ nhóm dịch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface/index_vn.html">Lời nói đầu</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_install/index_vn.html">Cài đặt</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_notation/index_vn.html">Ký hiệu</a></li>
</ul>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../chapter_introduction/index_vn.html">1. Giới thiệu</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preliminaries/index_vn.html">2. Sơ bộ</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/ndarray_vn.html">2.1. Thao tác với Dữ liệu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/pandas_vn.html">2.2. Tiền xử lý dữ liệu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra_vn.html">2.3. Đại số tuyến tính</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/calculus_vn.html">2.4. Giải tích</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/autograd_vn.html">2.5. Tính vi phân Tự động</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/probability_vn.html">2.6. Xác suất</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/lookup-api_vn.html">2.7. Tài liệu</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_linear-networks/index_vn.html">3. Mạng nơ-ron Tuyến tính</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/linear-regression_vn.html">3.1. Hồi quy Tuyến tính</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/linear-regression-scratch_vn.html">3.2. Lập trình Hồi quy Tuyến tính từ đầu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/linear-regression-gluon_vn.html">3.3. Cách lập trình súc tích Hồi quy Tuyến tính</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/softmax-regression_vn.html">3.4. Hồi quy Softmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/fashion-mnist_vn.html">3.5. Bộ dữ liệu Phân loại Ảnh (Fashion-MNIST)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/softmax-regression-scratch_vn.html">3.6. Lập trình Hồi quy Sofmax từ đầu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/softmax-regression-gluon_vn.html">3.7. Cách lập trình súc tích Hồi quy Softmax</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_multilayer-perceptrons/index_vn.html">4. Perceptron Đa tầng</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/mlp_vn.html">4.1. Perceptron đa tầng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/mlp-scratch_vn.html">4.2. Lập trình Perceptron Đa tầng từ đầu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/mlp-gluon_vn.html">4.3. Cách lập trình súc tích Perceptron Đa tầng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/underfit-overfit_vn.html">4.4. Lựa Chọn Mô Hình, Dưới Khớp và Quá Khớp</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/weight-decay_vn.html">4.5. Suy giảm trọng số</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/dropout_vn.html">4.6. Dropout</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/backprop_vn.html">4.7. Lan truyền xuôi, Lan truyền ngược và Đồ thị tính toán</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/numerical-stability-and-init_vn.html">4.8. Ổn định Số học và Khởi tạo</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/environment_vn.html">4.9. Cân nhắc tới Môi trường</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/kaggle-house-price_vn.html">4.10. Dự đoán Giá Nhà trên Kaggle</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_deep-learning-computation/index_vn.html">5. Tính toán Học sâu</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_deep-learning-computation/model-construction_vn.html">5.1. Tầng và Khối</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_deep-learning-computation/parameters_vn.html">5.2. Quản lý Tham số</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_deep-learning-computation/deferred-init_vn.html">5.3. Khởi tạo trễ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_deep-learning-computation/custom-layer_vn.html">5.4. Các tầng Tuỳ chỉnh</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_deep-learning-computation/read-write_vn.html">5.5. Đọc/Ghi tệp</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_deep-learning-computation/use-gpu_vn.html">5.6. GPU</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_convolutional-neural-networks/index_vn.html">6. Mạng Nơ-ron Tích chập</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/why-conv_vn.html">6.1. Từ Tầng Kết nối Dày đặc đến phép Tích chập</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/conv-layer_vn.html">6.2. Phép Tích chập cho Ảnh</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/padding-and-strides_vn.html">6.3. Đệm và Sải Bước</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/channels_vn.html">6.4. Đa kênh Đầu vào và Đầu ra</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/pooling_vn.html">6.5. Gộp (<em>Pooling</em>)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/lenet_vn.html">6.6. Mạng Nơ-ron Tích chập (LeNet)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_convolutional-modern/index_vn.html">7. Mạng Nơ-ron Tích chập Hiện đại</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/alexnet_vn.html">7.1. Mạng Nơ-ron Tích chập Sâu (AlexNet)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/vgg_vn.html">7.2. Mạng sử dụng Khối (VGG)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/nin_vn.html">7.3. Mạng trong Mạng (<em>Network in Network - NiN</em>)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/googlenet_vn.html">7.4. Mạng nối song song (GoogLeNet)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/batch-norm_vn.html">7.5. Chuẩn hoá theo batch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/resnet_vn.html">7.6. Mạng phần dư (ResNet)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/densenet_vn.html">7.7. Mạng Tích chập Kết nối Dày đặc (DenseNet)</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="index_vn.html">8. Mạng Nơ-ron Hồi tiếp</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">8.1. Mô hình chuỗi</a></li>
<li class="toctree-l2"><a class="reference internal" href="text-preprocessing_vn.html">8.2. Tiền Xử lý Dữ liệu Văn bản</a></li>
<li class="toctree-l2"><a class="reference internal" href="language-models-and-dataset_vn.html">8.3. Mô hình Ngôn ngữ và Tập dữ liệu</a></li>
<li class="toctree-l2"><a class="reference internal" href="rnn_vn.html">8.4. Mạng nơ-ron Hồi tiếp</a></li>
<li class="toctree-l2"><a class="reference internal" href="rnn-scratch_vn.html">8.5. Lập trình Mạng nơ-ron Hồi tiếp từ đầu</a></li>
<li class="toctree-l2"><a class="reference internal" href="rnn-gluon_vn.html">8.6. Lập trình súc tích Mạng nơ-ron Hồi tiếp</a></li>
<li class="toctree-l2"><a class="reference internal" href="bptt_vn.html">8.7. Lan truyền Ngược qua Thời gian</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_recurrent-modern/index_vn.html">9. Mạng Nơ-ron Hồi tiếp Hiện đại</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/gru_vn.html">9.1. Nút Hồi tiếp có Cổng (GRU)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/lstm_vn.html">9.2. Bộ nhớ Ngắn hạn Dài (LSTM)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/deep-rnn_vn.html">9.3. Mạng Nơ-ron Hồi tiếp Sâu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/bi-rnn_vn.html">9.4. Mạng Nơ-ron Hồi tiếp Hai chiều</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/machine-translation-and-dataset_vn.html">9.5. Dịch Máy và Tập dữ liệu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/encoder-decoder_vn.html">9.6. Kiến trúc Mã hoá - Giải mã</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/seq2seq_vn.html">9.7. Chuỗi sang Chuỗi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/beam-search_vn.html">9.8. Tìm kiếm Chùm</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_attention-mechanisms/index_vn.html">10. Cơ chế Tập trung</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms/attention_vn.html">10.1. Cơ chế Tập trung</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms/seq2seq-attention_vn.html">10.2. Chuỗi sang Chuỗi áp dụng Cơ chế Tập trung</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms/transformer_vn.html">10.3. Kiến trúc Transformer</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_optimization/index_vn.html">11. Thuật toán Tối ưu</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/optimization-intro_vn.html">11.1. Tối ưu và Học sâu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/optimization-intro_vn.html#cac-thach-thuc-cua-toi-uu-trong-hoc-sau">11.2. Các Thách thức của Tối ưu trong Học sâu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/optimization-intro_vn.html#cac-vung-cuc-tieu">11.3. Các vùng Cực tiểu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/optimization-intro_vn.html#cac-diem-yen-ngua">11.4. Các điểm Yên ngựa</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/optimization-intro_vn.html#tieu-bien-gradient">11.5. Tiêu biến Gradient</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/convexity_vn.html">11.6. Tính lồi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/gd_vn.html">11.7. Hạ Gradient</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/sgd_vn.html">11.8. Hạ Gradient Ngẫu nhiên</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/minibatch-sgd_vn.html">11.9. Hạ Gradient Ngẫu nhiên theo Minibatch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/momentum_vn.html">11.10. Động lượng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/adagrad_vn.html">11.11. Adagrad</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/rmsprop_vn.html">11.12. RMSProp</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/adadelta_vn.html">11.13. Adadelta</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/adam_vn.html">11.14. Adam</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/lr-scheduler_vn.html">11.15. Định thời Tốc độ Học</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_computational-performance/index_vn.html">12. Hiệu năng Tính toán</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/hybridize_vn.html">12.1. Trình biên dịch và Trình thông dịch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/async-computation_vn.html">12.2. Tính toán Bất đồng bộ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/auto-parallelism_vn.html">12.3. Song song hóa Tự động</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/hardware_vn.html">12.4. Phần cứng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/multiple-gpus_vn.html">12.5. Huấn luyện đa GPU</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/multiple-gpus-concise_vn.html">12.6. Cách lập trình Súc tích đa GPU</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/parameterserver_vn.html">12.7. Máy chủ Tham số</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_computer-vision/index_vn.html">13. Thị giác Máy tính</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/image-augmentation_vn.html">13.1. Tăng cường Ảnh</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/fine-tuning_vn.html">13.2. Tinh Chỉnh</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/bounding-box_vn.html">13.3. Phát hiện Vật thể và Khoanh vùng Đối tượng (Khung chứa)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/anchor_vn.html">13.4. Khung neo</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/multiscale-object-detection_vn.html">13.5. Phát hiện Vật thể Đa tỷ lệ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/object-detection-dataset_vn.html">13.6. Tập dữ liệu Phát hiện Đối tượng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/ssd_vn.html">13.7. Phát hiện Nhiều khung Một lượt (SSD)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/rcnn_vn.html">13.8. CNN theo Vùng (R-CNN)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/semantic-segmentation-and-dataset_vn.html">13.9. Phân vùng theo Ngữ nghĩa và Tập dữ liệu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/transposed-conv_vn.html">13.10. Tích chập Chuyển vị</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/fcn_vn.html">13.11. Mạng Tích chập Đầy đủ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/neural-style_vn.html">13.12. Truyền tải Phong cách Nơ-ron</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/kaggle-cifar10_vn.html">13.13. Phân loại ảnh (CIFAR-10) trên Kaggle</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/kaggle-dog_vn.html">13.14. Nhận diện Giống Chó (ImageNet Dogs) trên Kaggle</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/index_vn.html">14. Xử lý Ngôn ngữ Tự nhiên: Tiền Huấn luyện</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/word2vec_vn.html">14.1. Embedding Từ (word2vec)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/approx-training_vn.html">14.2. Huấn luyện Gần đúng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/word-embedding-dataset_vn.html">14.3. Tập dữ liệu để Tiền Huấn luyện Embedding Từ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/word2vec-pretraining_vn.html">14.4. Tiền huấn luyện word2vec</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/glove_vn.html">14.5. Embedding từ với Vector Toàn cục (GloVe)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/subword-embedding_vn.html">14.6. Embedding từ con</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/similarity-analogy_vn.html">14.7. Tìm kiếm từ Đồng nghĩa và Loại suy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/bert_vn.html">14.8. Biểu diễn Mã hóa hai chiều từ Transformer (BERT)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/bert-dataset_vn.html">14.9. Tập dữ liệu để Tiền huấn luyện BERT</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/bert-pretraining_vn.html">14.10. Tiền Huấn luyện BERT</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_natural-language-processing-applications/index_vn.html">15. Xử lý Ngôn ngữ Tự nhiên: Ứng dụng</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/sentiment-analysis-and-dataset_vn.html">15.1. Tác vụ Phân tích Cảm xúc và Bộ Dữ liệu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/sentiment-analysis-rnn_vn.html">15.2. Phân tích Cảm xúc: Sử dụng Mạng Nơ-ron Hồi tiếp</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/sentiment-analysis-cnn_vn.html">15.3. Phân tích Cảm xúc: Sử dụng Mạng Nơ-ron Tích Chập</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/natural-language-inference-and-dataset_vn.html">15.4. Suy luận ngôn ngữ tự nhiên và Tập dữ liệu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/natural-language-inference-attention_vn.html">15.5. Suy luận Ngôn ngữ Tự nhiên: Sử dụng Cơ chế Tập trung</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/finetuning-bert_vn.html">15.6. Tinh chỉnh BERT cho các Ứng dụng Cấp Chuỗi và Cấp Token</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/natural-language-inference-bert_vn.html">15.7. Suy luận Ngôn ngữ Tự nhiên: Tinh chỉnh BERT</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_recommender-systems/index_vn.html">16. Hệ thống Đề xuất</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/recsys-intro_vn.html">16.1. Tổng quan về Hệ thống Đề xuất</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/movielens_vn.html">16.2. Tập dữ liệu MovieLens</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/mf_vn.html">16.3. Phân rã Ma trận</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/autorec_vn.html">16.4. AutoRec: Dự đoán Đánh giá với Bộ tự Mã hóa</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/ranking_vn.html">16.5. Cá nhân hóa Xếp hạng trong Hệ thống Đề xuất</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/neumf_vn.html">16.6. Lọc Cộng tác Nơ-ron cho Cá nhân hóa Xếp hạng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/seqrec_vn.html">16.7. Hệ thống Đề xuất có Nhận thức về Chuỗi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/ctr_vn.html">16.8. Hệ thống Đề xuất Giàu Đặc trưng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/fm_vn.html">16.9. Máy Phân rã ma trận</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/deepfm_vn.html">16.10. Máy Phân rã Ma trận Sâu</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_generative-adversarial-networks/index_vn.html">17. Mạng Đối sinh</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_generative-adversarial-networks/gan_vn.html">17.1. Mạng Đối sinh</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_generative-adversarial-networks/dcgan_vn.html">17.2. Mạng Đối sinh Tích chập Sâu</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/index_vn.html">18. Phụ lục: Toán học cho Học Sâu</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/geometry-linear-algebraic-ops_vn.html">18.1. Các phép toán Hình học và Đại số Tuyến tính</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/eigendecomposition_vn.html">18.2. Phân rã trị riêng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/single-variable-calculus_vn.html">18.3. Giải tích một biến</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/multivariable-calculus_vn.html">18.4. Giải tích Nhiều biến</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/integral-calculus_vn.html">18.5. Giải tích Tích phân</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/random-variables_vn.html">18.6. Biến Ngẫu nhiên</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/maximum-likelihood_vn.html">18.7. Hợp lý Cực đại</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/distributions_vn.html">18.8. Các Phân phối Xác suất</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/naive-bayes_vn.html">18.9. Bộ phân loại Naive Bayes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/statistics_vn.html">18.10. Thống kê</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/information-theory_vn.html">18.11. Lý thuyết Thông tin</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/index_vn.html">19. Phụ lục: Công cụ cho Học Sâu</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/jupyter_vn.html">19.1. Sử dụng Jupyter</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/sagemaker_vn.html">19.2. Sử dụng Amazon SageMaker</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/aws_vn.html">19.3. Sử dụng Máy ảo AWS EC2</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/colab_vn.html">19.4. Sử dụng Google Colab</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/selecting-servers-gpus_vn.html">19.5. Lựa chọn Máy chủ &amp; GPU</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/contributing_vn.html">19.6. Đóng góp cho Quyển sách</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/d2l_vn.html">19.7. Tài liệu API của <code class="docutils literal notranslate"><span class="pre">d2l</span></code></a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../chapter_references/zreferences.html">Tài liệu tham khảo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../glossary.html">Bảng thuật ngữ</a></li>
</ul>

            </nav>
        
        </div>
    
</header>

    <div class="document">
        <div class="page-content" role="main">
        
  <!-- ===================== Bắt đầu dịch Phần 1 ==================== --><!-- ========================================= REVISE PHẦN 1 - BẮT ĐẦU =================================== --><!--
# Sequence Models
--><div class="section" id="mo-hinh-chuoi">
<span id="sec-sequence"></span><h1><span class="section-number">8.1. </span>Mô hình chuỗi<a class="headerlink" href="#mo-hinh-chuoi" title="Permalink to this headline">¶</a></h1>
<!--
Imagine that you are watching movies on Netflix.
As a good Netflix user, you decide to rate each of the movies religiously.
After all, a good movie is a good movie, and you want to watch more of them, right?
As it turns out, things are not quite so simple.
People's opinions on movies can change quite significantly over time.
In fact, psychologists even have names for some of the effects:
--><p>Hãy tưởng tượng rằng bạn đang xem phim trên Netflix. Là một người dùng
Netflix tốt, bạn quyết định đánh giá từng bộ phim một cách cẩn thận. Xét
cho cùng, bạn muốn xem thêm nhiều bộ phim hay phải không? Nhưng hóa ra,
mọi thứ không hề đơn giản như vậy. Đánh giá của mỗi người về một bộ phim
có thể thay đổi đáng kể theo thời gian. Trên thực tế, các nhà tâm lý học
thậm chí còn đặt tên cho một số hiệu ứng:</p>
<!--
* There is [anchoring](https://en.wikipedia.org/wiki/Anchoring), based on someone else's opinion.
For instance after the Oscar awards, ratings for the corresponding movie go up, even though it is still the same movie.
This effect persists for a few months until the award is forgotten.
:cite:`Wu.Ahmed.Beutel.ea.2017` showed that the effect lifts rating by over half a point.
* There is the [Hedonic adaptation](https://en.wikipedia.org/wiki/Hedonic_treadmill), where humans quickly adapt to accept an improved (or a bad) situation as the new normal.
For instance, after watching many good movies,
the expectations that the next movie is equally good or better are high, hence even an average movie might be considered a bad movie after many great ones.
* There is seasonality. Very few viewers like to watch a Santa Claus movie in August.
* In some cases movies become unpopular due to the misbehaviors of directors or actors in the production.
* Some movies become cult movies, because they were almost comically bad. *Plan 9 from Outer Space* and *Troll 2* achieved a high degree of notoriety for this reason.
--><ul class="simple">
<li><a class="reference external" href="https://en.wikipedia.org/wiki/Anchoring">Hiệu ứng mỏ neo</a>: dựa
trên ý kiến của người khác. Ví dụ, xếp hạng của một bộ phim sẽ tăng
lên sau khi nó thắng giải Oscar, mặc dù đoàn làm phim này không có
bất kỳ tác động nào về mặt quảng bá đến bộ phim. Hiệu ứng này kéo dài
trong vòng một vài tháng cho đến khi giải thưởng bị lãng quên.
<a class="bibtex reference internal" href="../chapter_references/zreferences.html#wu-ahmed-beutel-ea-2017" id="id1">[Wu et al., 2017]</a> chỉ ra rằng hiệu ứng này tăng chỉ
số xếp hạng thêm hơn nửa điểm.</li>
<li><a class="reference external" href="https://en.wikipedia.org/wiki/Hedonic_treadmill">Hiệu ứng vòng xoáy khoái
lạc</a>: con người
nhanh chóng thích nghi để chấp nhận một tình huống tốt hơn (hoặc xấu
đi) như một điều bình thường mới. Chẳng hạn, sau khi xem nhiều bộ
phim hay, sự kỳ vọng rằng bộ phim tiếp theo sẽ hay tương đương hoặc
thậm chí phải hay hơn trở nên khá cao, do đó ngay cả một bộ phim
trung bình cũng có thể bị coi là một bộ phim tồi.</li>
<li>Tính thời vụ: rất ít khán giả thích xem một bộ phim về ông già Noel
vào tháng 8.</li>
<li>Trong một số trường hợp, các bộ phim trở nên không được ưa chuộng do
những hành động sai trái của các đạo diễn hoặc diễn viên tham gia vào
quá trình sản xuất phim.</li>
<li>Một số phim trở thành “phim cult” vì chúng gần như tệ đến mức phát
cười. <em>Plan 9 from Outer Space</em> và <em>Troll 2</em> là hai ví dụ nổi tiếng.</li>
</ul>
<!--
In short, ratings are anything but stationary.
Using temporal dynamics helped :cite:`Koren.2009` to recommend movies more accurately.
But it is not just about movies.
--><p>Tóm lại, thứ bậc xếp hạng không hề cố định. Sử dụng các động lực dựa
trên thời gian đã giúp <a class="bibtex reference internal" href="../chapter_references/zreferences.html#koren-2009" id="id2">[Koren, 2009]</a> đề xuất phim chính xác hơn.
Tuy nhiên, vấn đề không chỉ là về phim ảnh.</p>
<!--
* Many users have highly particular behavior when it comes to the time when they open apps.
For instance, social media apps are much more popular after school with students.
Stock market trading apps are more commonly used when the markets are open.
* It is much harder to predict tomorrow's stock prices than to fill in the blanks for a stock price we missed yesterday, even though both are just a matter of estimating one number.
After all, hindsight is so much easier than foresight.
In statistics the former is called *extrapolation* whereas the latter is called *interpolation*.
* Music, speech, text, movies, steps, etc. are all sequential in nature.
If we were to permute them they would make little sense.
The headline *dog bites man* is much less surprising than *man bites dog*, even though the words are identical.
* Earthquakes are strongly correlated, i.e., after a massive earthquake there are very likely several smaller aftershocks, much more so than without the strong quake.
In fact, earthquakes are spatiotemporally correlated, i.e., the aftershocks typically occur within a short time span and in close proximity.
* Humans interact with each other in a sequential nature, as can be seen in Twitter fights, dance patterns and debates.
--><ul class="simple">
<li>Nhiều người dùng có thói quen rất đặc biệt liên quan tới thời gian mở
ứng dụng. Chẳng hạn, học sinh sử dụng các ứng dụng mạng xã hội nhiều
hơn hẳn sau giờ học. Các ứng dụng giao dịch chứng khoán được sử dụng
nhiều khi thị trường mở cửa.</li>
<li>Việc dự đoán giá cổ phiếu ngày mai khó hơn nhiều so với việc dự đoán
giá cổ phiếu bị bỏ lỡ ngày hôm qua, mặc dù cả hai đều là bài toán ước
tính một con số. Rốt cuộc, nhìn lại quá khứ dễ hơn nhiều so với dự
đoán tương lai. Trong thống kê, bài toán đầu tiên được gọi là <em>ngoại
suy</em> và bài toán sau được gọi là <em>nội suy</em>.</li>
<li>Âm nhạc, giọng nói, văn bản, phim ảnh, bước đi, v.v … đều có tính
chất tuần tự. Nếu chúng ta hoán vị chúng, chúng sẽ không còn nhiều ý
nghĩa. Dòng tiêu đề <em>chó cắn người</em> ít gây ngạc nhiên hơn nhiều so
với <em>người cắn chó</em>, mặc dù các từ giống hệt nhau.</li>
<li>Các trận động đất có mối tương quan mạnh mẽ, tức sau một trận động
đất lớn, rất có thể sẽ có một số dư chấn nhỏ hơn và xác suất xảy ra
dư chấn cao hơn nhiều so với trường hợp trận động đất lớn không xảy
ra trước đó. Trên thực tế, các trận động đất có mối tương quan về mặt
không-thời gian, tức các dư chấn thường xảy ra trong một khoảng thời
gian ngắn và ở gần nhau.</li>
<li>Con người tương tác với nhau một cách tuần tự, điều này có thể được
thấy trong các cuộc tranh cãi trên Twitter, các điệu nhảy và các cuộc
tranh luận.</li>
</ul>
<!-- ===================== Kết thúc dịch Phần 1 ===================== --><!-- ===================== Bắt đầu dịch Phần 2 ===================== --><!--
## Statistical Tools
--><div class="section" id="cac-cong-cu-thong-ke">
<h2><span class="section-number">8.1.1. </span>Các công cụ thống kê<a class="headerlink" href="#cac-cong-cu-thong-ke" title="Permalink to this headline">¶</a></h2>
<!--
In short, we need statistical tools and new deep neural networks architectures to deal with sequence data.
To keep things simple, we use the stock price illustrated in :numref:`fig_ftse100` as an example.
--><p>Tóm lại, ta cần các công cụ thống kê và các kiến trúc mạng nơ-ron sâu
mới để xử lý dữ liệu chuỗi. Để đơn giản hóa mọi việc, ta sẽ sử dụng giá
cổ phiếu được minh họa trong <a class="reference internal" href="#fig-ftse100"><span class="std std-numref">Fig. 8.1.1</span></a> để làm ví dụ.</p>
<!--
![FTSE 100 index over 30 years](../img/ftse100.png)
--><div class="figure align-default" id="id5">
<span id="fig-ftse100"></span><a class="reference internal image-reference" href="../_images/ftse100.png"><img alt="../_images/ftse100.png" src="../_images/ftse100.png" style="width: 400px;" /></a>
<p class="caption"><span class="caption-number">Fig. 8.1.1 </span><span class="caption-text">Giá cổ phiếu FTSE 100 trong vòng 30 năm</span><a class="headerlink" href="#id5" title="Permalink to this image">¶</a></p>
</div>
<!--
Let us denote the prices by $x_t \geq 0$, i.e., at time $t \in \mathbb{N}$ we observe price $x_t$.
For a trader to do well in the stock market on day $t$ he should want to predict $x_t$ via
--><p>Ta sẽ gọi giá cổ phiếu là <span class="math notranslate nohighlight">\(x_t \geq 0\)</span>, tức tại thời điểm
<span class="math notranslate nohighlight">\(t \in \mathbb{N}\)</span> ta thấy giá cổ phiếu bằng <span class="math notranslate nohighlight">\(x_t\)</span>. Để có
thể kiếm lời trên thị trường chứng khoán vào ngày <span class="math notranslate nohighlight">\(t\)</span>, một nhà
giao dịch sẽ muốn dự đoán <span class="math notranslate nohighlight">\(x_t\)</span> thông qua</p>
<div class="math notranslate nohighlight" id="equation-chapter-recurrent-neural-networks-sequence-vn-0">
<span class="eqno">(8.1.1)<a class="headerlink" href="#equation-chapter-recurrent-neural-networks-sequence-vn-0" title="Permalink to this equation">¶</a></span>\[x_t \sim p(x_t \mid x_{t-1}, \ldots, x_1).\]</div>
<!--
### Autoregressive Models
--><div class="section" id="mo-hinh-tu-hoi-quy">
<h3><span class="section-number">8.1.1.1. </span>Mô hình Tự hồi quy<a class="headerlink" href="#mo-hinh-tu-hoi-quy" title="Permalink to this headline">¶</a></h3>
<!--
In order to achieve this, our trader could use a regressor such as the one we trained in :numref:`sec_linear_gluon`.
There is just a major problem: the number of inputs, $x_{t-1}, \ldots, x_1$ varies, depending on $t$.
That is, the number increases with the amount of data that we encounter, and we will need an approximation to make this computationally tractable.
Much of what follows in this chapter will revolve around how to estimate $p(x_t \mid x_{t-1}, \ldots, x_1)$ efficiently.
In a nutshell it boils down to two strategies:
--><p>Để dự đoán giá cổ phiếu, các nhà giao dịch có thể sử dụng một mô hình
hồi quy, chẳng hạn như mô hình mà ta đã huấn luyện trong
<a class="reference internal" href="../chapter_linear-networks/linear-regression-gluon_vn.html#sec-linear-gluon"><span class="std std-numref">Section 3.3</span></a>. Chỉ có một vấn đề lớn ở đây, đó là số
lượng đầu vào, <span class="math notranslate nohighlight">\(x_{t-1}, \ldots, x_1\)</span> thay đổi tùy thuộc vào
<span class="math notranslate nohighlight">\(t\)</span>. Cụ thể, số lượng đầu vào sẽ tăng cùng với lượng dữ liệu thu
được và ta sẽ cần một phép tính xấp xỉ để làm cho giải pháp này khả thi
về mặt tính toán. Phần lớn nội dung tiếp theo trong chương này sẽ xoay
quanh việc làm thế nào để ước lượng
<span class="math notranslate nohighlight">\(p(x_t \mid x_{t-1}, \ldots, x_1)\)</span> một cách hiệu quả. Nói ngắn
gọn, ta có hai chiến lược:</p>
<!--
1. Assume that the potentially rather long sequence $x_{t-1}, \ldots, x_1$ is not really necessary.
In this case we might content ourselves with some timespan $\tau$ and only use $x_{t-1}, \ldots, x_{t-\tau}$ observations.
The immediate benefit is that now the number of arguments is always the same, at least for $t > \tau$.
This allows us to train a deep network as indicated above.
Such models will be called *autoregressive* models, as they quite literally perform regression on themselves.
2. Another strategy, shown in :numref:`fig_sequence-model`, is to try and keep some summary $h_t$ of the past observations, at the same time update $h_t$ in addition to the prediction $\hat{x}_t$.
This leads to models that estimate $x_t$ with $\hat{x}_t = p(x_t \mid x_{t-1}, h_{t})$ and moreover updates of the form  $h_t = g(h_{t-1}, x_{t-1})$.
Since $h_t$ is never observed, these models are also called *latent autoregressive models*.
LSTMs and GRUs are examples of this.
--><ol class="arabic simple">
<li>Giả sử rằng việc sử dụng một chuỗi có thể rất dài
<span class="math notranslate nohighlight">\(x_{t-1}, \ldots, x_1\)</span> là không thực sự cần thiết. Trong trường
hợp này, ta có thể hài lòng với một khoảng thời gian <span class="math notranslate nohighlight">\(\tau\)</span> và
chỉ sử dụng các quan sát <span class="math notranslate nohighlight">\(x_{t-1}, \ldots, x_{t-\tau}\)</span>. Lợi ích
trước mắt là bây giờ số lượng đối số luôn bằng nhau, ít nhất là với
<span class="math notranslate nohighlight">\(t &gt; \tau\)</span>. Điều này sẽ cho phép ta huấn luyện một mạng sâu như
được đề cập ở bên trên. Các mô hình như vậy được gọi là các mô hình
<em>tự hồi quy</em> (<em>autoregressive</em>), vì chúng tự thực hiện hồi quy trên
chính mình.</li>
<li>Một chiến lược khác, được minh họa trong
<a class="reference internal" href="#fig-sequence-model"><span class="std std-numref">Fig. 8.1.2</span></a>, là giữ một giá trị <span class="math notranslate nohighlight">\(h_t\)</span> để tóm
tắt các quan sát trong quá khứ, đồng thời cập nhật <span class="math notranslate nohighlight">\(h_t\)</span> bên
cạnh việc dự đoán <span class="math notranslate nohighlight">\(\hat{x}_t\)</span>. Kết quả là mô hình sẽ ước tính
<span class="math notranslate nohighlight">\(x_t\)</span> với <span class="math notranslate nohighlight">\(\hat{x}_t = p(x_t \mid x_{t-1}, h_{t})\)</span> và cập
nhật <span class="math notranslate nohighlight">\(h_t = g(h_{t-1}, x_{t-1})\)</span>. Do <span class="math notranslate nohighlight">\(h_t\)</span> không bao giờ
được quan sát nên các mô hình này còn được gọi là các <em>mô hình tự hồi
quy tiềm ẩn</em> (<em>latent autoregressive model</em>). LSTM và GRU là hai ví
dụ cho kiểu mô hình này.</li>
</ol>
<!--
![A latent autoregressive model. ](../img/sequence-model.svg)
--><div class="figure align-default" id="id6">
<span id="fig-sequence-model"></span><img alt="../_images/sequence-model.svg" src="../_images/sequence-model.svg" /><p class="caption"><span class="caption-number">Fig. 8.1.2 </span><span class="caption-text">Một mô hình tự hồi quy tiềm ẩn.</span><a class="headerlink" href="#id6" title="Permalink to this image">¶</a></p>
</div>
<!-- ===================== Kết thúc dịch Phần 2 ===================== --><!-- ===================== Bắt đầu dịch Phần 3 ===================== --><!--
Both cases raise the obvious question of how to generate training data.
One typically uses historical observations to predict the next observation given the ones up to right now.
Obviously we do not expect time to stand still.
However, a common assumption is that while the specific values of $x_t$ might change, at least the dynamics of the time series itself will not.
This is reasonable, since novel dynamics are just that, novel and thus not predictable using data that we have so far.
Statisticians call dynamics that do not change *stationary*.
Regardless of what we do, we will thus get an estimate of the entire time series via
--><p>Cả hai trường hợp đều đặt ra câu hỏi về cách tạo ra dữ liệu huấn luyện.
Người ta thường sử dụng các quan sát từ quá khứ cho đến hiện tại để dự
đoán các quan sát xảy ra trong tương lai. Rõ ràng chúng ta không thể
trông đợi thời gian sẽ đứng yên. Tuy nhiên, một giả định phổ biến là:
tuy các giá trị cụ thể của <span class="math notranslate nohighlight">\(x_t\)</span> có thể thay đổi, ít ra động lực
của chuỗi thời gian sẽ không đổi. Điều này khá hợp lý, vì nếu động lực
thay đổi thì ta sẽ không thể dự đoán được nó bằng cách sử dụng dữ liệu
mà ta đang có. Các nhà thống kê gọi các động lực không thay đổi này là
<em>cố định</em> (<em>stationary</em>). Dù có làm gì đi chăng nữa, chúng ta vẫn sẽ tìm
được ước lượng của toàn bộ chuỗi thời gian thông qua</p>
<div class="math notranslate nohighlight" id="equation-chapter-recurrent-neural-networks-sequence-vn-1">
<span class="eqno">(8.1.2)<a class="headerlink" href="#equation-chapter-recurrent-neural-networks-sequence-vn-1" title="Permalink to this equation">¶</a></span>\[p(x_1, \ldots, x_T) = \prod_{t=1}^T p(x_t \mid x_{t-1}, \ldots, x_1).\]</div>
<!--
Note that the above considerations still hold if we deal with discrete objects, such as words, rather than numbers.
The only difference is that in such a situation we need to use a classifier rather than a regressor to estimate $p(x_t \mid  x_{t-1}, \ldots, x_1)$.
--><p>Lưu ý rằng các xem xét trên vẫn đúng trong trường hợp chúng ta làm việc
với các đối tượng rời rạc, chẳng hạn như từ ngữ thay vì số. Sự khác biệt
duy nhất trong trường hợp này là chúng ta cần sử dụng một bộ phân loại
thay vì một bộ hồi quy để ước lượng
<span class="math notranslate nohighlight">\(p(x_t \mid x_{t-1}, \ldots, x_1)\)</span>.</p>
<!-- ========================================= REVISE PHẦN 1 - KẾT THÚC ===================================--><!-- ========================================= REVISE PHẦN 2 - BẮT ĐẦU ===================================--><!--
### Markov Model
--></div>
<div class="section" id="mo-hinh-markov">
<h3><span class="section-number">8.1.1.2. </span>Mô hình Markov<a class="headerlink" href="#mo-hinh-markov" title="Permalink to this headline">¶</a></h3>
<!--
Recall the approximation that in an autoregressive model we use only $(x_{t-1}, \ldots, x_{t-\tau})$ instead of $(x_{t-1}, \ldots, x_1)$ to estimate $x_t$.
Whenever this approximation is accurate we say that the sequence satisfies a *Markov condition*.
In particular, if $\tau = 1$, we have a *first order* Markov model and $p(x)$ is given by
--><p>Nhắc lại phép xấp xỉ trong một mô hình tự hồi quy, chúng ta chỉ sử dụng
<span class="math notranslate nohighlight">\((x_{t-1}, \ldots, x_{t-\tau})\)</span> thay vì
<span class="math notranslate nohighlight">\((x_{t-1}, \ldots, x_1)\)</span> để ước lượng <span class="math notranslate nohighlight">\(x_t\)</span>. Bất cứ khi nào
phép xấp xỉ này là chính xác, chúng ta nói rằng chuỗi thỏa mãn <em>điều
kiện Markov</em>. Cụ thể, nếu <span class="math notranslate nohighlight">\(\tau = 1\)</span>, chúng ta có mô hình Markov
<em>bậc một</em> và <span class="math notranslate nohighlight">\(p(x)\)</span> như sau</p>
<div class="math notranslate nohighlight" id="equation-chapter-recurrent-neural-networks-sequence-vn-2">
<span class="eqno">(8.1.3)<a class="headerlink" href="#equation-chapter-recurrent-neural-networks-sequence-vn-2" title="Permalink to this equation">¶</a></span>\[p(x_1, \ldots, x_T) = \prod_{t=1}^T p(x_t \mid x_{t-1}).\]</div>
<!--
Such models are particularly nice whenever $x_t$ assumes only a discrete value, since in this case dynamic programming can be used to compute values along the chain exactly.
For instance, we can compute $p(x_{t+1} \mid x_{t-1})$ efficiently using the fact that we only need to take into account a very short history of past observations:
--><p>Các mô hình như trên rất hữu dụng bất cứ khi nào <span class="math notranslate nohighlight">\(x_t\)</span> chỉ là các
giá trị rời rạc, vì trong trường hợp này, quy hoạch động có thể được sử
dụng để tính toán chính xác các giá trị theo chuỗi. Ví dụ, chúng ta có
thể tính toán <span class="math notranslate nohighlight">\(p(x_{t+1} \mid x_{t-1})\)</span> một cách hiệu quả bằng
cách chỉ sử dụng các quan sát trong một khoảng thời gian ngắn tại quá
khứ:</p>
<div class="math notranslate nohighlight" id="equation-chapter-recurrent-neural-networks-sequence-vn-3">
<span class="eqno">(8.1.4)<a class="headerlink" href="#equation-chapter-recurrent-neural-networks-sequence-vn-3" title="Permalink to this equation">¶</a></span>\[p(x_{t+1} \mid x_{t-1}) = \sum_{x_t} p(x_{t+1} \mid x_t) p(x_t \mid x_{t-1}).\]</div>
<!--
Going into details of dynamic programming is beyond the scope of this section, but we will introduce it in :numref:`sec_bi_rnn`.
Control and reinforcement learning algorithms use such tools extensively.
--><p>Chi tiết về quy hoạch động nằm ngoài phạm vi của phần này, nhưng chúng
tôi sẽ giới thiệu nó trong <a class="reference internal" href="../chapter_recurrent-modern/bi-rnn_vn.html#sec-bi-rnn"><span class="std std-numref">Section 9.4</span></a>. Các công cụ trên được
sử dụng rất phổ biến trong các thuật toán điều khiển và học tăng cường.</p>
<!-- ===================== Kết thúc dịch Phần 3 ===================== --><!-- ===================== Bắt đầu dịch Phần 4 ===================== --><!--
### Causality
--></div>
<div class="section" id="quan-he-nhan-qua">
<h3><span class="section-number">8.1.1.3. </span>Quan hệ Nhân quả<a class="headerlink" href="#quan-he-nhan-qua" title="Permalink to this headline">¶</a></h3>
<!--
In principle, there is nothing wrong with unfolding $p(x_1, \ldots, x_T)$ in reverse order.
After all, by conditioning we can always write it via
--><p>Về nguyên tắc, không có gì sai khi trải (<em>unfolding</em>)
<span class="math notranslate nohighlight">\(p(x_1, \ldots, x_T)\)</span> theo thứ tự ngược lại. Bằng cách đặt điều
kiện như vậy, chúng ta luôn có thể viết chúng như sau</p>
<div class="math notranslate nohighlight" id="equation-chapter-recurrent-neural-networks-sequence-vn-4">
<span class="eqno">(8.1.5)<a class="headerlink" href="#equation-chapter-recurrent-neural-networks-sequence-vn-4" title="Permalink to this equation">¶</a></span>\[p(x_1, \ldots, x_T) = \prod_{t=T}^1 p(x_t \mid x_{t+1}, \ldots, x_T).\]</div>
<!--
In fact, if we have a Markov model, we can obtain a reverse conditional probability distribution, too.
In many cases, however, there exists a natural direction for the data, namely going forward in time.
It is clear that future events cannot influence the past.
Hence, if we change $x_t$, we may be able to influence what happens for $x_{t+1}$ going forward but not the converse.
That is, if we change $x_t$, the distribution over past events will not change.
Consequently, it ought to be easier to explain $p(x_{t+1} \mid x_t)$ rather than $p(x_t \mid x_{t+1})$.
For instance, :cite:`Hoyer.Janzing.Mooij.ea.2009` show that in some cases we can find $x_{t+1} = f(x_t) + \epsilon$ for some additive noise, whereas the converse is not true.
This is great news, since it is typically the forward direction that we are interested in estimating.
For more on this topic see e.g., the book by :cite:`Peters.Janzing.Scholkopf.2017`.
We are barely scratching the surface of it.
--><p>Trên thực tế, nếu có một mô hình Markov, chúng ta cũng có thể thu được
một phân phối xác suất có điều kiện ngược. Tuy nhiên trong nhiều trường
hợp vẫn tồn tại một trật tự tự nhiên cho dữ liệu, cụ thể đó là chiều
thuận theo thời gian. Rõ ràng là các sự kiện trong tương lai không thể
ảnh hưởng đến quá khứ. Do đó, nếu thay đổi <span class="math notranslate nohighlight">\(x_t\)</span> thì ta có thể ảnh
hưởng đến những gì xảy ra tại <span class="math notranslate nohighlight">\(x_{t+1}\)</span> trong tương lai, nhưng lại
không thể ảnh hưởng tới quá khứ theo chiều ngược lại. Nếu chúng ta thay
đổi <span class="math notranslate nohighlight">\(x_t\)</span>, phân phối trên các sự kiện trong quá khứ sẽ không thay
đổi. Do đó, việc giải thích <span class="math notranslate nohighlight">\(p(x_{t+1} \mid x_t)\)</span> sẽ đơn giản hơn
là <span class="math notranslate nohighlight">\(p(x_t \mid x_{t+1})\)</span>. Ví dụ:
<a class="bibtex reference internal" href="../chapter_references/zreferences.html#hoyer-janzing-mooij-ea-2009" id="id3">[Hoyer et al., 2009]</a> chỉ ra rằng trong một số trường
hợp chúng ta có thể tìm <span class="math notranslate nohighlight">\(x_{t+1} = f(x_t) + \epsilon\)</span> khi có thêm
nhiễu, trong khi điều ngược lại thì không đúng. Đây là một tin tuyệt vời
vì chúng ta thường quan tâm tới việc ước lượng theo chiều thuận hơn. Để
tìm hiểu thêm về chủ đề này, có thể tìm đọc cuốn sách
<a class="bibtex reference internal" href="../chapter_references/zreferences.html#peters-janzing-scholkopf-2017" id="id4">[Peters et al., 2017a]</a>. Chúng ta sẽ chỉ tìm hiểu sơ qua
trong phần này.</p>
<!--
## A Toy Example
--></div>
</div>
<div class="section" id="mot-vi-du-don-gian">
<h2><span class="section-number">8.1.2. </span>Một ví dụ đơn giản<a class="headerlink" href="#mot-vi-du-don-gian" title="Permalink to this headline">¶</a></h2>
<!--
After so much theory, let us try this out in practice.
Let us begin by generating some data.
To keep things simple we generate our time series by using a sine function with some additive noise.
--><p>Sau khi đề cập nhiều về lý thuyết, bây giờ chúng ta hãy thử lập trình
minh họa. Đầu tiên, hãy khởi tạo một vài dữ liệu như sau. Để đơn giản,
chúng ta tạo chuỗi thời gian bằng cách sử dụng hàm sin cộng thêm một
chút nhiễu.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="kn">from</span> <span class="nn">d2l</span> <span class="kn">import</span> <span class="n">mxnet</span> <span class="k">as</span> <span class="n">d2l</span>
<span class="kn">from</span> <span class="nn">mxnet</span> <span class="kn">import</span> <span class="n">autograd</span><span class="p">,</span> <span class="n">np</span><span class="p">,</span> <span class="n">npx</span><span class="p">,</span> <span class="n">gluon</span><span class="p">,</span> <span class="n">init</span>
<span class="kn">from</span> <span class="nn">mxnet.gluon</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="n">npx</span><span class="o">.</span><span class="n">set_np</span><span class="p">()</span>

<span class="n">T</span> <span class="o">=</span> <span class="mi">1000</span>  <span class="c1"># Generate a total of 1000 points</span>
<span class="n">time</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">T</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mf">0.01</span> <span class="o">*</span> <span class="n">time</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">T</span><span class="p">)</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">time</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">])</span>
</pre></div>
</div>
<div class="figure align-default">
<img alt="../_images/output_sequence_vn_c0eed1_1_0.svg" src="../_images/output_sequence_vn_c0eed1_1_0.svg" /></div>
<!--
Next we need to turn this time series into features and labels that the network can train on.
Based on the embedding dimension $\tau$ we map the data into pairs $y_t = x_t$ and $\mathbf{z}_t = (x_{t-1}, \ldots, x_{t-\tau})$.
The astute reader might have noticed that this gives us $\tau$ fewer data points, since we do not have sufficient history for the first $\tau$ of them.
A simple fix, in particular if the time series is long is to discard those few terms.
Alternatively we could pad the time series with zeros.
The code below is essentially identical to the training code in previous sections.
We kept the architecture fairly simple.
A few layers of a fully connected network, ReLU activation and $\ell_2$ loss.
Since much of the modeling is identical to the previous sections when we built regression estimators in Gluon, we will not delve into much detail.
--><p>Tiếp theo, chúng ta cần biến chuỗi thời gian này thành các đặc trưng và
nhãn có thể được sử dụng để huấn luyện mạng. Dựa trên kích thước
embedding <span class="math notranslate nohighlight">\(\tau\)</span>, chúng ta ánh xạ dữ liệu thành các cặp
<span class="math notranslate nohighlight">\(y_t = x_t\)</span> và
<span class="math notranslate nohighlight">\(\mathbf{z}_t = (x_{t-1}, \ldots, x_{t-\tau})\)</span>. Để ý kĩ, có thể
thấy rằng ta sẽ mất <span class="math notranslate nohighlight">\(\tau\)</span> điểm dữ liệu đầu tiên, vì chúng ta
không có đủ <span class="math notranslate nohighlight">\(\tau\)</span> điểm dữ liệu trong quá khứ để làm đặc trưng cho
chúng. Một cách đơn giản để khắc phục điều này, đặc biệt là khi chuỗi
thời gian rất dài, là loại bỏ đi số ít các phần tử đó. Một cách khác là
đệm giá trị 0 vào chuỗi thời gian. Mã nguồn dưới đây về cơ bản là giống
hệt với mã nguồn huấn luyện trong các phần trước. Chúng tôi cố gắng giữ
cho kiến trúc đơn giản với vài tầng kết nối đầy đủ, hàm kích hoạt ReLU
và hàm mất mát <span class="math notranslate nohighlight">\(\ell_2\)</span>. Do việc mô hình hóa phần lớn là giống với
khi ta xây dựng các bộ ước lượng hồi quy viết bằng Gluon trong các phần
trước, nên chúng ta sẽ không đi sâu vào chi tiết trong phần này.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">tau</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">features</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">T</span><span class="o">-</span><span class="n">tau</span><span class="p">,</span> <span class="n">tau</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">tau</span><span class="p">):</span>
    <span class="n">features</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">:</span> <span class="n">T</span><span class="o">-</span><span class="n">tau</span><span class="o">+</span><span class="n">i</span><span class="p">]</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">tau</span><span class="p">:]</span>

<span class="n">batch_size</span><span class="p">,</span> <span class="n">n_train</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">600</span>
<span class="n">train_iter</span> <span class="o">=</span> <span class="n">d2l</span><span class="o">.</span><span class="n">load_array</span><span class="p">((</span><span class="n">features</span><span class="p">[:</span><span class="n">n_train</span><span class="p">],</span> <span class="n">labels</span><span class="p">[:</span><span class="n">n_train</span><span class="p">]),</span>
                            <span class="n">batch_size</span><span class="p">,</span> <span class="n">is_train</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">test_iter</span> <span class="o">=</span> <span class="n">d2l</span><span class="o">.</span><span class="n">load_array</span><span class="p">((</span><span class="n">features</span><span class="p">[:</span><span class="n">n_train</span><span class="p">],</span> <span class="n">labels</span><span class="p">[:</span><span class="n">n_train</span><span class="p">]),</span>
                           <span class="n">batch_size</span><span class="p">,</span> <span class="n">is_train</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># Vanilla MLP architecture</span>
<span class="k">def</span> <span class="nf">get_net</span><span class="p">():</span>
    <span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
    <span class="n">net</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">net</span><span class="o">.</span><span class="n">initialize</span><span class="p">(</span><span class="n">init</span><span class="o">.</span><span class="n">Xavier</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">net</span>

<span class="c1"># Least mean squares loss</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">gluon</span><span class="o">.</span><span class="n">loss</span><span class="o">.</span><span class="n">L2Loss</span><span class="p">()</span>
</pre></div>
</div>
<!--
Now we are ready to train.
--><p>Bây giờ chúng ta đã sẵn sàng để huấn luyện.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">train_net</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">train_iter</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">epochs</span><span class="p">,</span> <span class="n">lr</span><span class="p">):</span>
    <span class="n">trainer</span> <span class="o">=</span> <span class="n">gluon</span><span class="o">.</span><span class="n">Trainer</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">collect_params</span><span class="p">(),</span> <span class="s1">&#39;adam&#39;</span><span class="p">,</span>
                            <span class="p">{</span><span class="s1">&#39;learning_rate&#39;</span><span class="p">:</span> <span class="n">lr</span><span class="p">})</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">epochs</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">train_iter</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">autograd</span><span class="o">.</span><span class="n">record</span><span class="p">():</span>
                <span class="n">l</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">net</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span>
            <span class="n">l</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">trainer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;epoch </span><span class="si">%d</span><span class="s1">, loss: </span><span class="si">%f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span>
            <span class="n">epoch</span><span class="p">,</span> <span class="n">d2l</span><span class="o">.</span><span class="n">evaluate_loss</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">train_iter</span><span class="p">,</span> <span class="n">loss</span><span class="p">)))</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">get_net</span><span class="p">()</span>
<span class="n">train_net</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">train_iter</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">epoch</span> <span class="mi">1</span><span class="p">,</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">0.045946</span>
<span class="n">epoch</span> <span class="mi">2</span><span class="p">,</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">0.031594</span>
<span class="n">epoch</span> <span class="mi">3</span><span class="p">,</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">0.028504</span>
<span class="n">epoch</span> <span class="mi">4</span><span class="p">,</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">0.028767</span>
<span class="n">epoch</span> <span class="mi">5</span><span class="p">,</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">0.027109</span>
<span class="n">epoch</span> <span class="mi">6</span><span class="p">,</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">0.026090</span>
<span class="n">epoch</span> <span class="mi">7</span><span class="p">,</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">0.028275</span>
<span class="n">epoch</span> <span class="mi">8</span><span class="p">,</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">0.025843</span>
<span class="n">epoch</span> <span class="mi">9</span><span class="p">,</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">0.026051</span>
<span class="n">epoch</span> <span class="mi">10</span><span class="p">,</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">0.026025</span>
</pre></div>
</div>
<!-- ===================== Kết thúc dịch Phần 4 ===================== --><!-- ===================== Bắt đầu dịch Phần 5 ===================== --><!-- ========================================= REVISE PHẦN 2 - KẾT THÚC ===================================--><!-- ========================================= REVISE PHẦN 3 - BẮT ĐẦU ===================================--><!--
## Predictions
--></div>
<div class="section" id="du-doan-cua-mo-hinh">
<h2><span class="section-number">8.1.3. </span>Dự đoán của Mô hình<a class="headerlink" href="#du-doan-cua-mo-hinh" title="Permalink to this headline">¶</a></h2>
<!--
Since both training and test loss are small, we would expect our model to work well.
Let us see what this means in practice.
The first thing to check is how well the model is able to predict what happens in the next timestep.
--><p>Vì cả hai giá trị mất mát trên tập huấn luyện và kiểm tra đều nhỏ, chúng
ta kỳ vọng mô hình trên sẽ hoạt động tốt. Hãy cùng xác nhận điều này
trên thực tế. Điều đầu tiên cần kiểm tra là mô hình có thể dự đoán những
gì sẽ xảy ra trong bước thời gian kế tiếp tốt như thế nào.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">estimates</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">time</span><span class="p">,</span> <span class="n">time</span><span class="p">[</span><span class="n">tau</span><span class="p">:]],</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">estimates</span><span class="p">],</span>
         <span class="n">legend</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;data&#39;</span><span class="p">,</span> <span class="s1">&#39;estimate&#39;</span><span class="p">])</span>
</pre></div>
</div>
<div class="figure align-default">
<img alt="../_images/output_sequence_vn_c0eed1_7_0.svg" src="../_images/output_sequence_vn_c0eed1_7_0.svg" /></div>
<!--
This looks nice, just as we expected it.
Even beyond 600 observations the estimates still look rather trustworthy.
There is just one little problem to this: if we observe data only until timestep 600, we cannot hope to receive the ground truth for all future predictions.
Instead, we need to work our way forward one step at a time:
--><p>Kết quả khá tốt, đúng như những gì chúng ta mong đợi. Thậm chí sau hơn
600 mẫu quan sát, phép ước lượng vẫn trông khá tin cậy. Chỉ có một chút
vấn đề: nếu chúng ta quan sát dữ liệu tới bước thời gian thứ 600, chúng
ta không thể hy vọng sẽ nhận được nhãn gốc cho tất cả các dự đoán tương
lai. Thay vào đó, chúng ta cần tiến lên từng bước một:</p>
<div class="math notranslate nohighlight" id="equation-chapter-recurrent-neural-networks-sequence-vn-5">
<span class="eqno">(8.1.6)<a class="headerlink" href="#equation-chapter-recurrent-neural-networks-sequence-vn-5" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{aligned}
x_{601} &amp; = f(x_{600}, \ldots, x_{597}), \\
x_{602} &amp; = f(x_{601}, \ldots, x_{598}), \\
x_{603} &amp; = f(x_{602}, \ldots, x_{599}).
\end{aligned}\end{split}\]</div>
<!--
In other words, we will have to use our own predictions to make future predictions.
Let us see how well this goes.
--><p>Nói cách khác, chúng ta sẽ phải sử dụng những dự đoán của mình để đưa ra
dự đoán trong tương lai. Hãy cùng xem cách này có ổn không.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">T</span><span class="p">)</span>
<span class="n">predictions</span><span class="p">[:</span><span class="n">n_train</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:</span><span class="n">n_train</span><span class="p">]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_train</span><span class="p">,</span> <span class="n">T</span><span class="p">):</span>
    <span class="n">predictions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span>
        <span class="n">predictions</span><span class="p">[(</span><span class="n">i</span><span class="o">-</span><span class="n">tau</span><span class="p">):</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">time</span><span class="p">,</span> <span class="n">time</span><span class="p">[</span><span class="n">tau</span><span class="p">:],</span> <span class="n">time</span><span class="p">[</span><span class="n">n_train</span><span class="p">:]],</span>
         <span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">estimates</span><span class="p">,</span> <span class="n">predictions</span><span class="p">[</span><span class="n">n_train</span><span class="p">:]],</span>
         <span class="n">legend</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;data&#39;</span><span class="p">,</span> <span class="s1">&#39;estimate&#39;</span><span class="p">,</span> <span class="s1">&#39;multistep&#39;</span><span class="p">],</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">4.5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">))</span>
</pre></div>
</div>
<div class="figure align-default">
<img alt="../_images/output_sequence_vn_c0eed1_9_0.svg" src="../_images/output_sequence_vn_c0eed1_9_0.svg" /></div>
<!--
As the above example shows, this is a spectacular failure.
The estimates decay to a constant pretty quickly after a few prediction steps.
Why did the algorithm work so poorly?
This is ultimately due to the fact that the errors build up.
Let us say that after step 1 we have some error $\epsilon_1 = \bar\epsilon$.
Now the *input* for step 2 is perturbed by $\epsilon_1$, hence we suffer some error in the order of $\epsilon_2 = \bar\epsilon + L \epsilon_1$, and so on.
The error can diverge rather rapidly from the true observations.
This is a common phenomenon.
For instance, weather forecasts for the next 24 hours tend to be pretty accurate but beyond that the accuracy declines rapidly.
We will discuss methods for improving this throughout this chapter and beyond.
--><p>Ví dụ trên cho thấy, cách này đã thất bại thảm hại. Các giá trị ước
lượng rất nhanh chóng suy giảm thành một hằng số chỉ sau một vài bước.
Tại sao thuật toán trên hoạt động tệ đến thế? Suy cho cùng, lý do là
trên thực tế các sai số dự đoán bị chồng chất qua các bước thời gian. Cụ
thể, sau bước thời gian 1 chúng ta có nhận được sai số
<span class="math notranslate nohighlight">\(\epsilon_1 = \bar\epsilon\)</span>. Tiếp theo, <em>đầu vào</em> cho bước thời
gian 2 bị nhiễu loạn bởi <span class="math notranslate nohighlight">\(\epsilon_1\)</span>, do đó chúng ta nhận được
sai số dự đoán <span class="math notranslate nohighlight">\(\epsilon_2 = \bar\epsilon + L \epsilon_1\)</span>. Tương
tự như thế cho các bước thời gian tiếp theo. Sai số có thể phân kỳ khá
nhanh khỏi các quan sát đúng. Đây là một hiện tượng phổ biến. Ví dụ, dự
báo thời tiết trong 24 giờ tới có độ chính xác khá cao nhưng nó giảm đi
nhanh chóng với những dự báo xa hơn quãng thời gian đó. Chúng ta sẽ thảo
luận về các phương pháp để cải thiện vấn đề trên trong chương này và
những chương tiếp theo.</p>
<!--
Let us verify this observation by computing the $k$-step predictions on the entire sequence.
--><p>Chúng ta hãy kiểm chứng quan sát trên bằng cách tính toán dự đoán
<span class="math notranslate nohighlight">\(k\)</span> bước thời gian trên toàn bộ chuỗi.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">k</span> <span class="o">=</span> <span class="mi">33</span>  <span class="c1"># Look up to k - tau steps ahead</span>

<span class="n">features</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">k</span><span class="p">,</span> <span class="n">T</span><span class="o">-</span><span class="n">k</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">tau</span><span class="p">):</span>  <span class="c1"># Copy the first tau features from x</span>
    <span class="n">features</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">T</span><span class="o">-</span><span class="n">k</span><span class="o">+</span><span class="n">i</span><span class="p">]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">tau</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>  <span class="c1"># Predict the (i-tau)-th step</span>
    <span class="n">features</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">features</span><span class="p">[(</span><span class="n">i</span><span class="o">-</span><span class="n">tau</span><span class="p">):</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>

<span class="n">steps</span> <span class="o">=</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">time</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">T</span><span class="o">-</span><span class="n">k</span><span class="o">+</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">steps</span><span class="p">],</span> <span class="p">[</span><span class="n">features</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">steps</span><span class="p">],</span>
         <span class="n">legend</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;step </span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">steps</span><span class="p">],</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">4.5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">))</span>
</pre></div>
</div>
<div class="figure align-default">
<img alt="../_images/output_sequence_vn_c0eed1_11_0.svg" src="../_images/output_sequence_vn_c0eed1_11_0.svg" /></div>
<!--
This clearly illustrates how the quality of the estimates changes as we try to predict further into the future.
While the 8-step predictions are still pretty good, anything beyond that is pretty useless.
--><p>Điều này minh họa rõ ràng chất lượng của các ước lượng thay đổi như thế
nào khi chúng ta cố gắng dự đoán xa hơn trong tương lai. Mặc dù những dự
đoán có độ dài là 8 bước vẫn còn khá tốt, bất cứ kết quả dự đoán nào
vượt ra ngoài khoảng đó thì khá là vô dụng.</p>
<!-- ===================== Kết thúc dịch Phần 5 ===================== --><!-- ===================== Bắt đầu dịch Phần 6 ===================== --><!--
## Summary
--></div>
<div class="section" id="tom-tat">
<h2><span class="section-number">8.1.4. </span>Tóm tắt<a class="headerlink" href="#tom-tat" title="Permalink to this headline">¶</a></h2>
<!--
* Sequence models require specialized statistical tools for estimation.
Two popular choices are autoregressive models and latent-variable autoregressive models.
* As we predict further in time, the errors accumulate and the quality of the estimates degrades, often dramatically.
* There is quite a difference in difficulty between interpolation and extrapolation.
Consequently, if you have a time series, always respect the temporal order of the data when training, i.e., never train on future data.
* For causal models (e.g., time going forward), estimating the forward direction is typically a lot easier than the reverse direction.
--><ul class="simple">
<li>Các mô hình chuỗi thường yêu cầu các công cụ thống kê chuyên biệt để
ước lượng. Hai lựa chọn phổ biến đó là các mô hình tự hồi quy và mô
hình tự hồi quy biến tiềm ẩn.</li>
<li>Sai số bị tích lũy và chất lượng của phép ước lượng suy giảm đáng kể
khi mô hình dự đoán các bước thời gian xa hơn.</li>
<li>Khó khăn trong phép nội suy và ngoại suy khá khác biệt. Do đó, nếu
bạn có một kiểu dữ liệu chuỗi thời gian, hãy luôn để ý trình tự thời
gian của dữ liệu khi huấn luyện, hay nói cách khác, không bao giờ
huấn luyện trên dữ liệu thuộc về bước thời gian trong tương lai.</li>
<li>Đối với các mô hình nhân quả (ví dụ, ở đó thời gian đi về phía
trước), ước lượng theo chiều xuôi thường dễ dàng hơn rất nhiều so với
chiều ngược lại.</li>
</ul>
<!--
## Exercises
--></div>
<div class="section" id="bai-tap">
<h2><span class="section-number">8.1.5. </span>Bài tập<a class="headerlink" href="#bai-tap" title="Permalink to this headline">¶</a></h2>
<!--
1. Improve the above model.
    * Incorporate more than the past 4 observations? How many do you really need?
    * How many would you need if there was no noise? Hint: you can write $\sin$ and $\cos$ as a differential equation.
    * Can you incorporate older features while keeping the total number of features constant? Does this improve accuracy? Why?
    * Change the neural network architecture and see what happens.
2. An investor wants to find a good security to buy. She looks at past returns to decide which one is likely to do well. What could possibly go wrong with this strategy?
3. Does causality also apply to text? To which extent?
4. Give an example for when a latent autoregressive model might be needed to capture the dynamic of the data.
--><ol class="arabic simple">
<li>Hãy cải thiện mô hình nói trên bằng cách<ul>
<li>Kết hợp nhiều hơn 4 mẫu quan sát trong quá khứ? Bao nhiêu mẫu quan
sát là thực sự cần thiết?</li>
<li>Bạn sẽ cần bao nhiêu mẫu nếu dữ liệu không có nhiễu? Gợi ý: bạn có
thể viết <span class="math notranslate nohighlight">\(\sin\)</span> và <span class="math notranslate nohighlight">\(\cos\)</span> dưới dạng phương trình vi
phân.</li>
<li>Có thể kết hợp các đặc trưng cũ hơn trong khi đảm bảo tổng số đặc
trưng là không đổi không? Điều này có cải thiện độ chính xác
không? Tại sao?</li>
<li>Thay đổi cấu trúc mạng nơ-ron và quan sát tác động của nó.</li>
</ul>
</li>
<li>Nếu một nhà đầu tư muốn tìm một mã chứng khoán tốt để mua. Cô ta sẽ
nhìn vào lợi nhuận trong quá khứ để quyết định mã nào có khả năng
sinh lời. Điều gì có thể khiến chiến lược này trở thành sai lầm?</li>
<li>Liệu có thể áp dụng quan hệ nhân quả cho dữ liệu văn bản được không?
Nếu có thì ở mức độ nào?</li>
<li>Hãy cho một ví dụ khi mô hình tự hồi quy tiềm ẩn có thể cần được dùng
để nắm bắt động lực của dữ liệu.</li>
</ol>
<!-- ===================== Kết thúc dịch Phần 6 ===================== --><!-- ========================================= REVISE PHẦN 3 - KẾT THÚC ===================================--></div>
<div class="section" id="thao-luan">
<h2><span class="section-number">8.1.6. </span>Thảo luận<a class="headerlink" href="#thao-luan" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><a class="reference external" href="https://discuss.mxnet.io/t/2860">Tiếng Anh</a></li>
<li><a class="reference external" href="https://forum.machinelearningcoban.com/c/d2l">Tiếng Việt</a></li>
</ul>
</div>
<div class="section" id="nhung-nguoi-thuc-hien">
<h2><span class="section-number">8.1.7. </span>Những người thực hiện<a class="headerlink" href="#nhung-nguoi-thuc-hien" title="Permalink to this headline">¶</a></h2>
<p>Bản dịch trong trang này được thực hiện bởi:</p>
<ul class="simple">
<li>Đoàn Võ Duy Thanh</li>
<li>Nguyễn Duy Du</li>
<li>Nguyễn Cảnh Thướng</li>
<li>Phạm Minh Đức</li>
<li>Nguyễn Lê Quang Nhật</li>
<li>Nguyễn Văn Quang</li>
<li>Nguyễn Văn Cường</li>
<li>Lê Khắc Hồng Phúc</li>
<li>Phạm Hồng Vinh</li>
</ul>
</div>
</div>


        </div>
        <div class="side-doc-outline">
            <div class="side-doc-outline--content"> 
<div class="localtoc">
    <p class="caption">
      <span class="caption-text">Table Of Contents</span>
    </p>
    <ul>
<li><a class="reference internal" href="#">8.1. Mô hình chuỗi</a><ul>
<li><a class="reference internal" href="#cac-cong-cu-thong-ke">8.1.1. Các công cụ thống kê</a><ul>
<li><a class="reference internal" href="#mo-hinh-tu-hoi-quy">8.1.1.1. Mô hình Tự hồi quy</a></li>
<li><a class="reference internal" href="#mo-hinh-markov">8.1.1.2. Mô hình Markov</a></li>
<li><a class="reference internal" href="#quan-he-nhan-qua">8.1.1.3. Quan hệ Nhân quả</a></li>
</ul>
</li>
<li><a class="reference internal" href="#mot-vi-du-don-gian">8.1.2. Một ví dụ đơn giản</a></li>
<li><a class="reference internal" href="#du-doan-cua-mo-hinh">8.1.3. Dự đoán của Mô hình</a></li>
<li><a class="reference internal" href="#tom-tat">8.1.4. Tóm tắt</a></li>
<li><a class="reference internal" href="#bai-tap">8.1.5. Bài tập</a></li>
<li><a class="reference internal" href="#thao-luan">8.1.6. Thảo luận</a></li>
<li><a class="reference internal" href="#nhung-nguoi-thuc-hien">8.1.7. Những người thực hiện</a></li>
</ul>
</li>
</ul>

</div>
            </div>
        </div>

      <div class="clearer"></div>
    </div><div class="pagenation">
     <a id="button-prev" href="index_vn.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="P">
         <i class="pagenation-arrow-L fas fa-arrow-left fa-lg"></i>
         <div class="pagenation-text">
            <span class="pagenation-direction">Previous</span>
            <div>8. Mạng Nơ-ron Hồi tiếp</div>
         </div>
     </a>
     <a id="button-next" href="text-preprocessing_vn.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="N">
         <i class="pagenation-arrow-R fas fa-arrow-right fa-lg"></i>
        <div class="pagenation-text">
            <span class="pagenation-direction">Next</span>
            <div>8.2. Tiền Xử lý Dữ liệu Văn bản</div>
        </div>
     </a>
  </div>
        
        </main>
    </div>
  </body>
</html>