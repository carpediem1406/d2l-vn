<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    
    <title>18.3. Giải tích một biến &#8212; Đắm mình vào Học Sâu 0.14.4 documentation</title>

    <link rel="stylesheet" href="../_static/basic.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/d2l.css" />
    <link rel="stylesheet" href="../_static/material-design-lite-1.3.0/material.blue-deep_orange.min.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx_materialdesign_theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fontawesome/all.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fonts.css" type="text/css" />
    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/language_data.js"></script>
    <script src="../_static/d2l.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="shortcut icon" href="../_static/favicon.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="18.4. Giải tích Nhiều biến" href="multivariable-calculus_vn.html" />
    <link rel="prev" title="18.2. Phân rã trị riêng" href="eigendecomposition_vn.html" /> 
  </head>
<body>
    <div class="mdl-layout mdl-js-layout mdl-layout--fixed-header mdl-layout--fixed-drawer"><header class="mdl-layout__header mdl-layout__header--waterfall ">
    <div class="mdl-layout__header-row">
        
        <nav class="mdl-navigation breadcrumb">
            <a class="mdl-navigation__link" href="index_vn.html"><span class="section-number">18. </span>Phụ lục: Toán học cho Học Sâu</a><i class="material-icons">navigate_next</i>
            <a class="mdl-navigation__link is-active"><span class="section-number">18.3. </span>Giải tích một biến</a>
        </nav>
        <div class="mdl-layout-spacer"></div>
        <nav class="mdl-navigation">
        
<form class="form-inline pull-sm-right" action="../search.html" method="get">
      <div class="mdl-textfield mdl-js-textfield mdl-textfield--expandable mdl-textfield--floating-label mdl-textfield--align-right">
        <label id="quick-search-icon" class="mdl-button mdl-js-button mdl-button--icon"  for="waterfall-exp">
          <i class="material-icons">search</i>
        </label>
        <div class="mdl-textfield__expandable-holder">
          <input class="mdl-textfield__input" type="text" name="q"  id="waterfall-exp" placeholder="Search" />
          <input type="hidden" name="check_keywords" value="yes" />
          <input type="hidden" name="area" value="default" />
        </div>
      </div>
      <div class="mdl-tooltip" data-mdl-for="quick-search-icon">
      Quick search
      </div>
</form>
        
<a id="button-show-source"
    class="mdl-button mdl-js-button mdl-button--icon"
    href="../_sources/chapter_appendix-mathematics-for-deep-learning/single-variable-calculus_vn.rst.txt" rel="nofollow">
  <i class="material-icons">code</i>
</a>
<div class="mdl-tooltip" data-mdl-for="button-show-source">
Show Source
</div>
        </nav>
    </div>
    <div class="mdl-layout__header-row header-links">
      <div class="mdl-layout-spacer"></div>
      <nav class="mdl-navigation">
          
              <a  class="mdl-navigation__link" href="https://github.com/aivivn/d2l-vn">
                  <i class="fab fa-github"></i>
                  GitHub
              </a>
          
              <a  class="mdl-navigation__link" href="https://forum.machinelearningcoban.com/">
                  <i class="fab fa-discourse"></i>
                  Forum
              </a>
          
              <a  class="mdl-navigation__link" href="https://www.d2l.ai/">
                  <i class="fas fa-external-link-alt"></i>
                  English
              </a>
      </nav>
    </div>
</header><header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <img class="logo" src="../_static/logo-with-text-vi.png" alt="Đắm mình vào Học Sâu"/>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul>
<li class="toctree-l1"><a class="reference internal" href="../intro_vn.html">Giới thiệu từ nhóm dịch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface/index_vn.html">Lời nói đầu</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_install/index_vn.html">Cài đặt</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_notation/index_vn.html">Ký hiệu</a></li>
</ul>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../chapter_introduction/index_vn.html">1. Giới thiệu</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preliminaries/index_vn.html">2. Sơ bộ</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/ndarray_vn.html">2.1. Thao tác với Dữ liệu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/pandas_vn.html">2.2. Tiền xử lý dữ liệu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra_vn.html">2.3. Đại số tuyến tính</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/calculus_vn.html">2.4. Giải tích</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/autograd_vn.html">2.5. Tính vi phân Tự động</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/probability_vn.html">2.6. Xác suất</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/lookup-api_vn.html">2.7. Tài liệu</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_linear-networks/index_vn.html">3. Mạng nơ-ron Tuyến tính</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/linear-regression_vn.html">3.1. Hồi quy Tuyến tính</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/linear-regression-scratch_vn.html">3.2. Lập trình Hồi quy Tuyến tính từ đầu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/linear-regression-gluon_vn.html">3.3. Cách lập trình súc tích Hồi quy Tuyến tính</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/softmax-regression_vn.html">3.4. Hồi quy Softmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/fashion-mnist_vn.html">3.5. Bộ dữ liệu Phân loại Ảnh (Fashion-MNIST)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/softmax-regression-scratch_vn.html">3.6. Lập trình Hồi quy Sofmax từ đầu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/softmax-regression-gluon_vn.html">3.7. Cách lập trình súc tích Hồi quy Softmax</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_multilayer-perceptrons/index_vn.html">4. Perceptron Đa tầng</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/mlp_vn.html">4.1. Perceptron đa tầng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/mlp-scratch_vn.html">4.2. Lập trình Perceptron Đa tầng từ đầu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/mlp-gluon_vn.html">4.3. Cách lập trình súc tích Perceptron Đa tầng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/underfit-overfit_vn.html">4.4. Lựa Chọn Mô Hình, Dưới Khớp và Quá Khớp</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/weight-decay_vn.html">4.5. Suy giảm trọng số</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/dropout_vn.html">4.6. Dropout</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/backprop_vn.html">4.7. Lan truyền xuôi, Lan truyền ngược và Đồ thị tính toán</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/numerical-stability-and-init_vn.html">4.8. Ổn định Số học và Khởi tạo</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/environment_vn.html">4.9. Cân nhắc tới Môi trường</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/kaggle-house-price_vn.html">4.10. Dự đoán Giá Nhà trên Kaggle</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_deep-learning-computation/index_vn.html">5. Tính toán Học sâu</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_deep-learning-computation/model-construction_vn.html">5.1. Tầng và Khối</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_deep-learning-computation/parameters_vn.html">5.2. Quản lý Tham số</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_deep-learning-computation/deferred-init_vn.html">5.3. Khởi tạo trễ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_deep-learning-computation/custom-layer_vn.html">5.4. Các tầng Tuỳ chỉnh</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_deep-learning-computation/read-write_vn.html">5.5. Đọc/Ghi tệp</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_deep-learning-computation/use-gpu_vn.html">5.6. GPU</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_convolutional-neural-networks/index_vn.html">6. Mạng Nơ-ron Tích chập</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/why-conv_vn.html">6.1. Từ Tầng Kết nối Dày đặc đến phép Tích chập</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/conv-layer_vn.html">6.2. Phép Tích chập cho Ảnh</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/padding-and-strides_vn.html">6.3. Đệm và Sải Bước</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/channels_vn.html">6.4. Đa kênh Đầu vào và Đầu ra</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/pooling_vn.html">6.5. Gộp (<em>Pooling</em>)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/lenet_vn.html">6.6. Mạng Nơ-ron Tích chập (LeNet)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_convolutional-modern/index_vn.html">7. Mạng Nơ-ron Tích chập Hiện đại</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/alexnet_vn.html">7.1. Mạng Nơ-ron Tích chập Sâu (AlexNet)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/vgg_vn.html">7.2. Mạng sử dụng Khối (VGG)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/nin_vn.html">7.3. Mạng trong Mạng (<em>Network in Network - NiN</em>)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/googlenet_vn.html">7.4. Mạng nối song song (GoogLeNet)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/batch-norm_vn.html">7.5. Chuẩn hoá theo batch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/resnet_vn.html">7.6. Mạng phần dư (ResNet)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/densenet_vn.html">7.7. Mạng Tích chập Kết nối Dày đặc (DenseNet)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_recurrent-neural-networks/index_vn.html">8. Mạng Nơ-ron Hồi tiếp</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/sequence_vn.html">8.1. Mô hình chuỗi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/text-preprocessing_vn.html">8.2. Tiền Xử lý Dữ liệu Văn bản</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/language-models-and-dataset_vn.html">8.3. Mô hình Ngôn ngữ và Tập dữ liệu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/rnn_vn.html">8.4. Mạng nơ-ron Hồi tiếp</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/rnn-scratch_vn.html">8.5. Lập trình Mạng nơ-ron Hồi tiếp từ đầu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/rnn-gluon_vn.html">8.6. Lập trình súc tích Mạng nơ-ron Hồi tiếp</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/bptt_vn.html">8.7. Lan truyền Ngược qua Thời gian</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_recurrent-modern/index_vn.html">9. Mạng Nơ-ron Hồi tiếp Hiện đại</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/gru_vn.html">9.1. Nút Hồi tiếp có Cổng (GRU)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/lstm_vn.html">9.2. Bộ nhớ Ngắn hạn Dài (LSTM)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/deep-rnn_vn.html">9.3. Mạng Nơ-ron Hồi tiếp Sâu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/bi-rnn_vn.html">9.4. Mạng Nơ-ron Hồi tiếp Hai chiều</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/machine-translation-and-dataset_vn.html">9.5. Dịch Máy và Tập dữ liệu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/encoder-decoder_vn.html">9.6. Kiến trúc Mã hoá - Giải mã</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/seq2seq_vn.html">9.7. Chuỗi sang Chuỗi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/beam-search_vn.html">9.8. Tìm kiếm Chùm</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_attention-mechanisms/index_vn.html">10. Cơ chế Tập trung</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms/attention_vn.html">10.1. Cơ chế Tập trung</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms/seq2seq-attention_vn.html">10.2. Chuỗi sang Chuỗi áp dụng Cơ chế Tập trung</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms/transformer_vn.html">10.3. Kiến trúc Transformer</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_optimization/index_vn.html">11. Thuật toán Tối ưu</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/optimization-intro_vn.html">11.1. Tối ưu và Học sâu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/optimization-intro_vn.html#cac-thach-thuc-cua-toi-uu-trong-hoc-sau">11.2. Các Thách thức của Tối ưu trong Học sâu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/optimization-intro_vn.html#cac-vung-cuc-tieu">11.3. Các vùng Cực tiểu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/optimization-intro_vn.html#cac-diem-yen-ngua">11.4. Các điểm Yên ngựa</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/optimization-intro_vn.html#tieu-bien-gradient">11.5. Tiêu biến Gradient</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/convexity_vn.html">11.6. Tính lồi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/gd_vn.html">11.7. Hạ Gradient</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/sgd_vn.html">11.8. Hạ Gradient Ngẫu nhiên</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/minibatch-sgd_vn.html">11.9. Hạ Gradient Ngẫu nhiên theo Minibatch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/momentum_vn.html">11.10. Động lượng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/adagrad_vn.html">11.11. Adagrad</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/rmsprop_vn.html">11.12. RMSProp</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/adadelta_vn.html">11.13. Adadelta</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/adam_vn.html">11.14. Adam</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/lr-scheduler_vn.html">11.15. Định thời Tốc độ Học</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_computational-performance/index_vn.html">12. Hiệu năng Tính toán</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/hybridize_vn.html">12.1. Trình biên dịch và Trình thông dịch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/async-computation_vn.html">12.2. Tính toán Bất đồng bộ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/auto-parallelism_vn.html">12.3. Song song hóa Tự động</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/hardware_vn.html">12.4. Phần cứng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/multiple-gpus_vn.html">12.5. Huấn luyện đa GPU</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/multiple-gpus-concise_vn.html">12.6. Cách lập trình Súc tích đa GPU</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/parameterserver_vn.html">12.7. Máy chủ Tham số</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_computer-vision/index_vn.html">13. Thị giác Máy tính</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/image-augmentation_vn.html">13.1. Tăng cường Ảnh</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/fine-tuning_vn.html">13.2. Tinh Chỉnh</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/bounding-box_vn.html">13.3. Phát hiện Vật thể và Khoanh vùng Đối tượng (Khung chứa)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/anchor_vn.html">13.4. Khung neo</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/multiscale-object-detection_vn.html">13.5. Phát hiện Vật thể Đa tỷ lệ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/object-detection-dataset_vn.html">13.6. Tập dữ liệu Phát hiện Đối tượng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/ssd_vn.html">13.7. Phát hiện Nhiều khung Một lượt (SSD)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/rcnn_vn.html">13.8. CNN theo Vùng (R-CNN)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/semantic-segmentation-and-dataset_vn.html">13.9. Phân vùng theo Ngữ nghĩa và Tập dữ liệu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/transposed-conv_vn.html">13.10. Tích chập Chuyển vị</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/fcn_vn.html">13.11. Mạng Tích chập Đầy đủ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/neural-style_vn.html">13.12. Truyền tải Phong cách Nơ-ron</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/kaggle-cifar10_vn.html">13.13. Phân loại ảnh (CIFAR-10) trên Kaggle</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/kaggle-dog_vn.html">13.14. Nhận diện Giống Chó (ImageNet Dogs) trên Kaggle</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/index_vn.html">14. Xử lý Ngôn ngữ Tự nhiên: Tiền Huấn luyện</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/word2vec_vn.html">14.1. Embedding Từ (word2vec)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/approx-training_vn.html">14.2. Huấn luyện Gần đúng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/word-embedding-dataset_vn.html">14.3. Tập dữ liệu để Tiền Huấn luyện Embedding Từ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/word2vec-pretraining_vn.html">14.4. Tiền huấn luyện word2vec</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/glove_vn.html">14.5. Embedding từ với Vector Toàn cục (GloVe)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/subword-embedding_vn.html">14.6. Embedding từ con</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/similarity-analogy_vn.html">14.7. Tìm kiếm từ Đồng nghĩa và Loại suy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/bert_vn.html">14.8. Biểu diễn Mã hóa hai chiều từ Transformer (BERT)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/bert-dataset_vn.html">14.9. Tập dữ liệu để Tiền huấn luyện BERT</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/bert-pretraining_vn.html">14.10. Tiền Huấn luyện BERT</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_natural-language-processing-applications/index_vn.html">15. Xử lý Ngôn ngữ Tự nhiên: Ứng dụng</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/sentiment-analysis-and-dataset_vn.html">15.1. Tác vụ Phân tích Cảm xúc và Bộ Dữ liệu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/sentiment-analysis-rnn_vn.html">15.2. Phân tích Cảm xúc: Sử dụng Mạng Nơ-ron Hồi tiếp</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/sentiment-analysis-cnn_vn.html">15.3. Phân tích Cảm xúc: Sử dụng Mạng Nơ-ron Tích Chập</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/natural-language-inference-and-dataset_vn.html">15.4. Suy luận ngôn ngữ tự nhiên và Tập dữ liệu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/natural-language-inference-attention_vn.html">15.5. Suy luận Ngôn ngữ Tự nhiên: Sử dụng Cơ chế Tập trung</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/finetuning-bert_vn.html">15.6. Tinh chỉnh BERT cho các Ứng dụng Cấp Chuỗi và Cấp Token</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/natural-language-inference-bert_vn.html">15.7. Suy luận Ngôn ngữ Tự nhiên: Tinh chỉnh BERT</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_recommender-systems/index_vn.html">16. Hệ thống Đề xuất</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/recsys-intro_vn.html">16.1. Tổng quan về Hệ thống Đề xuất</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/movielens_vn.html">16.2. Tập dữ liệu MovieLens</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/mf_vn.html">16.3. Phân rã Ma trận</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/autorec_vn.html">16.4. AutoRec: Dự đoán Đánh giá với Bộ tự Mã hóa</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/ranking_vn.html">16.5. Cá nhân hóa Xếp hạng trong Hệ thống Đề xuất</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/neumf_vn.html">16.6. Lọc Cộng tác Nơ-ron cho Cá nhân hóa Xếp hạng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/seqrec_vn.html">16.7. Hệ thống Đề xuất có Nhận thức về Chuỗi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/ctr_vn.html">16.8. Hệ thống Đề xuất Giàu Đặc trưng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/fm_vn.html">16.9. Máy Phân rã ma trận</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/deepfm_vn.html">16.10. Máy Phân rã Ma trận Sâu</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_generative-adversarial-networks/index_vn.html">17. Mạng Đối sinh</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_generative-adversarial-networks/gan_vn.html">17.1. Mạng Đối sinh</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_generative-adversarial-networks/dcgan_vn.html">17.2. Mạng Đối sinh Tích chập Sâu</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="index_vn.html">18. Phụ lục: Toán học cho Học Sâu</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="geometry-linear-algebraic-ops_vn.html">18.1. Các phép toán Hình học và Đại số Tuyến tính</a></li>
<li class="toctree-l2"><a class="reference internal" href="eigendecomposition_vn.html">18.2. Phân rã trị riêng</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">18.3. Giải tích một biến</a></li>
<li class="toctree-l2"><a class="reference internal" href="multivariable-calculus_vn.html">18.4. Giải tích Nhiều biến</a></li>
<li class="toctree-l2"><a class="reference internal" href="integral-calculus_vn.html">18.5. Giải tích Tích phân</a></li>
<li class="toctree-l2"><a class="reference internal" href="random-variables_vn.html">18.6. Biến Ngẫu nhiên</a></li>
<li class="toctree-l2"><a class="reference internal" href="maximum-likelihood_vn.html">18.7. Hợp lý Cực đại</a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions_vn.html">18.8. Các Phân phối Xác suất</a></li>
<li class="toctree-l2"><a class="reference internal" href="naive-bayes_vn.html">18.9. Bộ phân loại Naive Bayes</a></li>
<li class="toctree-l2"><a class="reference internal" href="statistics_vn.html">18.10. Thống kê</a></li>
<li class="toctree-l2"><a class="reference internal" href="information-theory_vn.html">18.11. Lý thuyết Thông tin</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/index_vn.html">19. Phụ lục: Công cụ cho Học Sâu</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/jupyter_vn.html">19.1. Sử dụng Jupyter</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/sagemaker_vn.html">19.2. Sử dụng Amazon SageMaker</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/aws_vn.html">19.3. Sử dụng Máy ảo AWS EC2</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/colab_vn.html">19.4. Sử dụng Google Colab</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/selecting-servers-gpus_vn.html">19.5. Lựa chọn Máy chủ &amp; GPU</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/contributing_vn.html">19.6. Đóng góp cho Quyển sách</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/d2l_vn.html">19.7. Tài liệu API của <code class="docutils literal notranslate"><span class="pre">d2l</span></code></a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../chapter_references/zreferences.html">Tài liệu tham khảo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../glossary.html">Bảng thuật ngữ</a></li>
</ul>

            </nav>
        
        </div>
    
</header>
        <main class="mdl-layout__content" tabIndex="0">

	<script type="text/javascript" src="../_static/sphinx_materialdesign_theme.js "></script>
    <header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <img class="logo" src="../_static/logo-with-text-vi.png" alt="Đắm mình vào Học Sâu"/>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul>
<li class="toctree-l1"><a class="reference internal" href="../intro_vn.html">Giới thiệu từ nhóm dịch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface/index_vn.html">Lời nói đầu</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_install/index_vn.html">Cài đặt</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_notation/index_vn.html">Ký hiệu</a></li>
</ul>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../chapter_introduction/index_vn.html">1. Giới thiệu</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preliminaries/index_vn.html">2. Sơ bộ</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/ndarray_vn.html">2.1. Thao tác với Dữ liệu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/pandas_vn.html">2.2. Tiền xử lý dữ liệu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra_vn.html">2.3. Đại số tuyến tính</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/calculus_vn.html">2.4. Giải tích</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/autograd_vn.html">2.5. Tính vi phân Tự động</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/probability_vn.html">2.6. Xác suất</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/lookup-api_vn.html">2.7. Tài liệu</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_linear-networks/index_vn.html">3. Mạng nơ-ron Tuyến tính</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/linear-regression_vn.html">3.1. Hồi quy Tuyến tính</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/linear-regression-scratch_vn.html">3.2. Lập trình Hồi quy Tuyến tính từ đầu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/linear-regression-gluon_vn.html">3.3. Cách lập trình súc tích Hồi quy Tuyến tính</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/softmax-regression_vn.html">3.4. Hồi quy Softmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/fashion-mnist_vn.html">3.5. Bộ dữ liệu Phân loại Ảnh (Fashion-MNIST)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/softmax-regression-scratch_vn.html">3.6. Lập trình Hồi quy Sofmax từ đầu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/softmax-regression-gluon_vn.html">3.7. Cách lập trình súc tích Hồi quy Softmax</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_multilayer-perceptrons/index_vn.html">4. Perceptron Đa tầng</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/mlp_vn.html">4.1. Perceptron đa tầng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/mlp-scratch_vn.html">4.2. Lập trình Perceptron Đa tầng từ đầu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/mlp-gluon_vn.html">4.3. Cách lập trình súc tích Perceptron Đa tầng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/underfit-overfit_vn.html">4.4. Lựa Chọn Mô Hình, Dưới Khớp và Quá Khớp</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/weight-decay_vn.html">4.5. Suy giảm trọng số</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/dropout_vn.html">4.6. Dropout</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/backprop_vn.html">4.7. Lan truyền xuôi, Lan truyền ngược và Đồ thị tính toán</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/numerical-stability-and-init_vn.html">4.8. Ổn định Số học và Khởi tạo</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/environment_vn.html">4.9. Cân nhắc tới Môi trường</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/kaggle-house-price_vn.html">4.10. Dự đoán Giá Nhà trên Kaggle</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_deep-learning-computation/index_vn.html">5. Tính toán Học sâu</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_deep-learning-computation/model-construction_vn.html">5.1. Tầng và Khối</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_deep-learning-computation/parameters_vn.html">5.2. Quản lý Tham số</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_deep-learning-computation/deferred-init_vn.html">5.3. Khởi tạo trễ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_deep-learning-computation/custom-layer_vn.html">5.4. Các tầng Tuỳ chỉnh</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_deep-learning-computation/read-write_vn.html">5.5. Đọc/Ghi tệp</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_deep-learning-computation/use-gpu_vn.html">5.6. GPU</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_convolutional-neural-networks/index_vn.html">6. Mạng Nơ-ron Tích chập</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/why-conv_vn.html">6.1. Từ Tầng Kết nối Dày đặc đến phép Tích chập</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/conv-layer_vn.html">6.2. Phép Tích chập cho Ảnh</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/padding-and-strides_vn.html">6.3. Đệm và Sải Bước</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/channels_vn.html">6.4. Đa kênh Đầu vào và Đầu ra</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/pooling_vn.html">6.5. Gộp (<em>Pooling</em>)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/lenet_vn.html">6.6. Mạng Nơ-ron Tích chập (LeNet)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_convolutional-modern/index_vn.html">7. Mạng Nơ-ron Tích chập Hiện đại</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/alexnet_vn.html">7.1. Mạng Nơ-ron Tích chập Sâu (AlexNet)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/vgg_vn.html">7.2. Mạng sử dụng Khối (VGG)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/nin_vn.html">7.3. Mạng trong Mạng (<em>Network in Network - NiN</em>)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/googlenet_vn.html">7.4. Mạng nối song song (GoogLeNet)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/batch-norm_vn.html">7.5. Chuẩn hoá theo batch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/resnet_vn.html">7.6. Mạng phần dư (ResNet)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/densenet_vn.html">7.7. Mạng Tích chập Kết nối Dày đặc (DenseNet)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_recurrent-neural-networks/index_vn.html">8. Mạng Nơ-ron Hồi tiếp</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/sequence_vn.html">8.1. Mô hình chuỗi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/text-preprocessing_vn.html">8.2. Tiền Xử lý Dữ liệu Văn bản</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/language-models-and-dataset_vn.html">8.3. Mô hình Ngôn ngữ và Tập dữ liệu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/rnn_vn.html">8.4. Mạng nơ-ron Hồi tiếp</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/rnn-scratch_vn.html">8.5. Lập trình Mạng nơ-ron Hồi tiếp từ đầu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/rnn-gluon_vn.html">8.6. Lập trình súc tích Mạng nơ-ron Hồi tiếp</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/bptt_vn.html">8.7. Lan truyền Ngược qua Thời gian</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_recurrent-modern/index_vn.html">9. Mạng Nơ-ron Hồi tiếp Hiện đại</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/gru_vn.html">9.1. Nút Hồi tiếp có Cổng (GRU)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/lstm_vn.html">9.2. Bộ nhớ Ngắn hạn Dài (LSTM)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/deep-rnn_vn.html">9.3. Mạng Nơ-ron Hồi tiếp Sâu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/bi-rnn_vn.html">9.4. Mạng Nơ-ron Hồi tiếp Hai chiều</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/machine-translation-and-dataset_vn.html">9.5. Dịch Máy và Tập dữ liệu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/encoder-decoder_vn.html">9.6. Kiến trúc Mã hoá - Giải mã</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/seq2seq_vn.html">9.7. Chuỗi sang Chuỗi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/beam-search_vn.html">9.8. Tìm kiếm Chùm</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_attention-mechanisms/index_vn.html">10. Cơ chế Tập trung</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms/attention_vn.html">10.1. Cơ chế Tập trung</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms/seq2seq-attention_vn.html">10.2. Chuỗi sang Chuỗi áp dụng Cơ chế Tập trung</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms/transformer_vn.html">10.3. Kiến trúc Transformer</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_optimization/index_vn.html">11. Thuật toán Tối ưu</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/optimization-intro_vn.html">11.1. Tối ưu và Học sâu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/optimization-intro_vn.html#cac-thach-thuc-cua-toi-uu-trong-hoc-sau">11.2. Các Thách thức của Tối ưu trong Học sâu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/optimization-intro_vn.html#cac-vung-cuc-tieu">11.3. Các vùng Cực tiểu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/optimization-intro_vn.html#cac-diem-yen-ngua">11.4. Các điểm Yên ngựa</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/optimization-intro_vn.html#tieu-bien-gradient">11.5. Tiêu biến Gradient</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/convexity_vn.html">11.6. Tính lồi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/gd_vn.html">11.7. Hạ Gradient</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/sgd_vn.html">11.8. Hạ Gradient Ngẫu nhiên</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/minibatch-sgd_vn.html">11.9. Hạ Gradient Ngẫu nhiên theo Minibatch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/momentum_vn.html">11.10. Động lượng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/adagrad_vn.html">11.11. Adagrad</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/rmsprop_vn.html">11.12. RMSProp</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/adadelta_vn.html">11.13. Adadelta</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/adam_vn.html">11.14. Adam</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/lr-scheduler_vn.html">11.15. Định thời Tốc độ Học</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_computational-performance/index_vn.html">12. Hiệu năng Tính toán</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/hybridize_vn.html">12.1. Trình biên dịch và Trình thông dịch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/async-computation_vn.html">12.2. Tính toán Bất đồng bộ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/auto-parallelism_vn.html">12.3. Song song hóa Tự động</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/hardware_vn.html">12.4. Phần cứng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/multiple-gpus_vn.html">12.5. Huấn luyện đa GPU</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/multiple-gpus-concise_vn.html">12.6. Cách lập trình Súc tích đa GPU</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/parameterserver_vn.html">12.7. Máy chủ Tham số</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_computer-vision/index_vn.html">13. Thị giác Máy tính</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/image-augmentation_vn.html">13.1. Tăng cường Ảnh</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/fine-tuning_vn.html">13.2. Tinh Chỉnh</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/bounding-box_vn.html">13.3. Phát hiện Vật thể và Khoanh vùng Đối tượng (Khung chứa)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/anchor_vn.html">13.4. Khung neo</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/multiscale-object-detection_vn.html">13.5. Phát hiện Vật thể Đa tỷ lệ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/object-detection-dataset_vn.html">13.6. Tập dữ liệu Phát hiện Đối tượng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/ssd_vn.html">13.7. Phát hiện Nhiều khung Một lượt (SSD)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/rcnn_vn.html">13.8. CNN theo Vùng (R-CNN)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/semantic-segmentation-and-dataset_vn.html">13.9. Phân vùng theo Ngữ nghĩa và Tập dữ liệu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/transposed-conv_vn.html">13.10. Tích chập Chuyển vị</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/fcn_vn.html">13.11. Mạng Tích chập Đầy đủ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/neural-style_vn.html">13.12. Truyền tải Phong cách Nơ-ron</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/kaggle-cifar10_vn.html">13.13. Phân loại ảnh (CIFAR-10) trên Kaggle</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/kaggle-dog_vn.html">13.14. Nhận diện Giống Chó (ImageNet Dogs) trên Kaggle</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/index_vn.html">14. Xử lý Ngôn ngữ Tự nhiên: Tiền Huấn luyện</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/word2vec_vn.html">14.1. Embedding Từ (word2vec)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/approx-training_vn.html">14.2. Huấn luyện Gần đúng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/word-embedding-dataset_vn.html">14.3. Tập dữ liệu để Tiền Huấn luyện Embedding Từ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/word2vec-pretraining_vn.html">14.4. Tiền huấn luyện word2vec</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/glove_vn.html">14.5. Embedding từ với Vector Toàn cục (GloVe)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/subword-embedding_vn.html">14.6. Embedding từ con</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/similarity-analogy_vn.html">14.7. Tìm kiếm từ Đồng nghĩa và Loại suy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/bert_vn.html">14.8. Biểu diễn Mã hóa hai chiều từ Transformer (BERT)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/bert-dataset_vn.html">14.9. Tập dữ liệu để Tiền huấn luyện BERT</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/bert-pretraining_vn.html">14.10. Tiền Huấn luyện BERT</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_natural-language-processing-applications/index_vn.html">15. Xử lý Ngôn ngữ Tự nhiên: Ứng dụng</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/sentiment-analysis-and-dataset_vn.html">15.1. Tác vụ Phân tích Cảm xúc và Bộ Dữ liệu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/sentiment-analysis-rnn_vn.html">15.2. Phân tích Cảm xúc: Sử dụng Mạng Nơ-ron Hồi tiếp</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/sentiment-analysis-cnn_vn.html">15.3. Phân tích Cảm xúc: Sử dụng Mạng Nơ-ron Tích Chập</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/natural-language-inference-and-dataset_vn.html">15.4. Suy luận ngôn ngữ tự nhiên và Tập dữ liệu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/natural-language-inference-attention_vn.html">15.5. Suy luận Ngôn ngữ Tự nhiên: Sử dụng Cơ chế Tập trung</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/finetuning-bert_vn.html">15.6. Tinh chỉnh BERT cho các Ứng dụng Cấp Chuỗi và Cấp Token</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/natural-language-inference-bert_vn.html">15.7. Suy luận Ngôn ngữ Tự nhiên: Tinh chỉnh BERT</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_recommender-systems/index_vn.html">16. Hệ thống Đề xuất</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/recsys-intro_vn.html">16.1. Tổng quan về Hệ thống Đề xuất</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/movielens_vn.html">16.2. Tập dữ liệu MovieLens</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/mf_vn.html">16.3. Phân rã Ma trận</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/autorec_vn.html">16.4. AutoRec: Dự đoán Đánh giá với Bộ tự Mã hóa</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/ranking_vn.html">16.5. Cá nhân hóa Xếp hạng trong Hệ thống Đề xuất</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/neumf_vn.html">16.6. Lọc Cộng tác Nơ-ron cho Cá nhân hóa Xếp hạng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/seqrec_vn.html">16.7. Hệ thống Đề xuất có Nhận thức về Chuỗi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/ctr_vn.html">16.8. Hệ thống Đề xuất Giàu Đặc trưng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/fm_vn.html">16.9. Máy Phân rã ma trận</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/deepfm_vn.html">16.10. Máy Phân rã Ma trận Sâu</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_generative-adversarial-networks/index_vn.html">17. Mạng Đối sinh</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_generative-adversarial-networks/gan_vn.html">17.1. Mạng Đối sinh</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_generative-adversarial-networks/dcgan_vn.html">17.2. Mạng Đối sinh Tích chập Sâu</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="index_vn.html">18. Phụ lục: Toán học cho Học Sâu</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="geometry-linear-algebraic-ops_vn.html">18.1. Các phép toán Hình học và Đại số Tuyến tính</a></li>
<li class="toctree-l2"><a class="reference internal" href="eigendecomposition_vn.html">18.2. Phân rã trị riêng</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">18.3. Giải tích một biến</a></li>
<li class="toctree-l2"><a class="reference internal" href="multivariable-calculus_vn.html">18.4. Giải tích Nhiều biến</a></li>
<li class="toctree-l2"><a class="reference internal" href="integral-calculus_vn.html">18.5. Giải tích Tích phân</a></li>
<li class="toctree-l2"><a class="reference internal" href="random-variables_vn.html">18.6. Biến Ngẫu nhiên</a></li>
<li class="toctree-l2"><a class="reference internal" href="maximum-likelihood_vn.html">18.7. Hợp lý Cực đại</a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions_vn.html">18.8. Các Phân phối Xác suất</a></li>
<li class="toctree-l2"><a class="reference internal" href="naive-bayes_vn.html">18.9. Bộ phân loại Naive Bayes</a></li>
<li class="toctree-l2"><a class="reference internal" href="statistics_vn.html">18.10. Thống kê</a></li>
<li class="toctree-l2"><a class="reference internal" href="information-theory_vn.html">18.11. Lý thuyết Thông tin</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/index_vn.html">19. Phụ lục: Công cụ cho Học Sâu</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/jupyter_vn.html">19.1. Sử dụng Jupyter</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/sagemaker_vn.html">19.2. Sử dụng Amazon SageMaker</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/aws_vn.html">19.3. Sử dụng Máy ảo AWS EC2</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/colab_vn.html">19.4. Sử dụng Google Colab</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/selecting-servers-gpus_vn.html">19.5. Lựa chọn Máy chủ &amp; GPU</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/contributing_vn.html">19.6. Đóng góp cho Quyển sách</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/d2l_vn.html">19.7. Tài liệu API của <code class="docutils literal notranslate"><span class="pre">d2l</span></code></a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../chapter_references/zreferences.html">Tài liệu tham khảo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../glossary.html">Bảng thuật ngữ</a></li>
</ul>

            </nav>
        
        </div>
    
</header>

    <div class="document">
        <div class="page-content" role="main">
        
  <!--
# Single Variable Calculus
--><div class="section" id="giai-tich-mot-bien">
<span id="sec-single-variable-calculus"></span><h1><span class="section-number">18.3. </span>Giải tích một biến<a class="headerlink" href="#giai-tich-mot-bien" title="Permalink to this headline">¶</a></h1>
<!--
In :numref:`sec_calculus`, we saw the basic elements of differential calculus.
This section takes a deeper dive into the fundamentals of calculus and how we can understand and apply it in the context of machine learning.
--><p>Trong <a class="reference internal" href="../chapter_preliminaries/calculus_vn.html#sec-calculus"><span class="std std-numref">Section 2.4</span></a>, chúng ta đã thấy những thành phần cơ bản
của giải tích vi phân. Trong mục này chúng ta sẽ đi sâu vào kiến thức
nền tảng của giải tích và cách áp dụng chúng trong trong ngữ cảnh học
máy.</p>
<!--
## Differential Calculus
--><div class="section" id="giai-tich-vi-phan">
<h2><span class="section-number">18.3.1. </span>Giải tích Vi phân<a class="headerlink" href="#giai-tich-vi-phan" title="Permalink to this headline">¶</a></h2>
<!--
Differential calculus is fundamentally the study of how functions behave under small changes.  To see why this is so core to deep learning, let us consider an example.
--><p>Giải tích vi phân là nhánh toán học nghiên cứu về hành vi của các hàm số
dưới các biến đổi nhỏ. Để thấy được tại sao đây lại là phần cốt lõi của
học sâu, hãy cùng xem xét một ví dụ dưới đây.</p>
<!--
Suppose that we have a deep neural network where the weights are, for convenience, concatenated into a single vector $\mathbf{w} = (w_1, \ldots, w_n)$.
Given a training dataset, we consider the loss of our neural network on this dataset, which we will write as $\mathcal{L}(\mathbf{w})$.
--><p>Giả sử chúng ta có một mạng nơ-ron sâu với các trọng số được biễu diễn
bằng một vector duy nhất <span class="math notranslate nohighlight">\(\mathbf{w} = (w_1, \ldots, w_n)\)</span>. Cho
trước một tập huấn luyện, chúng ta sẽ tập trung vào giá trị mất mát
<span class="math notranslate nohighlight">\(\mathcal{L}(\mathbf{w})\)</span> của mạng nơ-ron trên tập huấn luyện đó.</p>
<!--
This function is extraordinarily complex, encoding the performance of all possible models of the given architecture on this dataset,
so it is nearly impossible to tell what set of weights $\mathbf{w}$ will minimize the loss.
Thus, in practice, we often start by initializing our weights *randomly*,
and then iteratively take small steps in the direction which makes the loss decrease as rapidly as possible.
--><p>Đây là một hàm số cực kì phức tạp, biểu diễn chất lượng của tất cả các
mô hình khả dĩ của một cấu trúc mạng cho trước trên tập dữ liệu này, nên
gần như không thể chỉ ra được ngay một tập các trọng số
<span class="math notranslate nohighlight">\(\mathbf{w}\)</span> để cực tiểu hóa mất mát. Do vậy trên thực tế, chúng
ta thường bắt đầu bằng việc khởi tạo <em>ngẫu nhiên</em> các trọng số, và tiến
từng bước nhỏ theo hướng mà sẽ giảm giá trị mất mát nhanh nhất có thể.</p>
<!--
The question then becomes something that on the surface is no easier: how do we find the direction which makes the weights decrease as quickly as possible?
To dig into this, let us first examine the case with only a single weight: $L(\mathbf{w}) = L(x)$ for a single real value $x$.
--><p>Vấn đề bây giờ thoạt nhìn cũng không dễ hơn bao nhiêu: làm thế nào để
tìm được hướng đi sẽ giảm giá trị hàm mất mát nhanh nhất có thể? Để trả
lời câu hỏi này, trước hết ta hãy xét trường hợp chỉ có một trọng số:
<span class="math notranslate nohighlight">\(L(\mathbf{w}) = L(x)\)</span> với một số thực <span class="math notranslate nohighlight">\(x\)</span> duy nhất.</p>
<!--
Let us take $x$ and try to understand what happens when we change it by a small amount to $x + \epsilon$.
If you wish to be concrete, think a number like $\epsilon = 0.0000001$.
To help us visualize what happens, Let us graph an example function, $f(x) = \sin(x^x)$, over the $[0, 3]$.
--><p>Hãy cùng tìm hiểu xem chuyện gì sẽ xảy ra khi ta lấy giá trị <span class="math notranslate nohighlight">\(x\)</span>
và thay đổi nó với một lượng rất nhỏ thành <span class="math notranslate nohighlight">\(x + \epsilon\)</span>. Nếu bạn
muốn một con số rõ ràng, hãy nghĩ về một số như
<span class="math notranslate nohighlight">\(\epsilon = 0.0000001\)</span>. Để minh họa chuyện gì sẽ diễn ra, hãy vẽ
ví dụ đồ thị của hàm số <span class="math notranslate nohighlight">\(f(x) = \sin(x^x)\)</span>, trên khoảng
<span class="math notranslate nohighlight">\([0, 3]\)</span>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="kn">from</span> <span class="nn">d2l</span> <span class="kn">import</span> <span class="n">mxnet</span> <span class="k">as</span> <span class="n">d2l</span>
<span class="kn">from</span> <span class="nn">IPython</span> <span class="kn">import</span> <span class="n">display</span>
<span class="kn">from</span> <span class="nn">mxnet</span> <span class="kn">import</span> <span class="n">np</span><span class="p">,</span> <span class="n">npx</span>
<span class="n">npx</span><span class="o">.</span><span class="n">set_np</span><span class="p">()</span>
<span class="c1"># Plot a function in a normal range</span>
<span class="n">x_big</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">3.01</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>
<span class="n">ys</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x_big</span><span class="o">**</span><span class="n">x_big</span><span class="p">)</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_big</span><span class="p">,</span> <span class="n">ys</span><span class="p">,</span> <span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;f(x)&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="figure align-default">
<img alt="../_images/output_single-variable-calculus_vn_99f0a2_1_0.svg" src="../_images/output_single-variable-calculus_vn_99f0a2_1_0.svg" /></div>
<!--
At this large scale, the function's behavior is not simple.
However, if we reduce our range to something smaller like $[1.75,2.25]$, we see that the graph becomes much simpler.
--><p>Trong một khoảng lớn thế này, cách hàm số biến đổi rất khó nắm bắt. Tuy
nhiên, nếu ta thu nhỏ khoảng xuống ví dụ như thành <span class="math notranslate nohighlight">\([1.75,2.25]\)</span>,
ta thấy đồ thị trở nên đơn giản hơn rất nhiều.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot a the same function in a tiny range</span>
<span class="n">x_med</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">1.75</span><span class="p">,</span> <span class="mf">2.25</span><span class="p">,</span> <span class="mf">0.001</span><span class="p">)</span>
<span class="n">ys</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x_med</span><span class="o">**</span><span class="n">x_med</span><span class="p">)</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_med</span><span class="p">,</span> <span class="n">ys</span><span class="p">,</span> <span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;f(x)&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="figure align-default">
<img alt="../_images/output_single-variable-calculus_vn_99f0a2_3_0.svg" src="../_images/output_single-variable-calculus_vn_99f0a2_3_0.svg" /></div>
<!--
Taking this to an extreme, if we zoom into a tiny segment, the behavior becomes far simpler: it is just a straight line.
--><p>Đỉnh điểm, nếu ta phóng gần vào một đoạn rất nhỏ, cách hàm số biến đổi
trở nên đơn giản hơn rất nhiều: chỉ là một đường thẳng.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot a the same function in a tiny range</span>
<span class="n">x_small</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.01</span><span class="p">,</span> <span class="mf">0.0001</span><span class="p">)</span>
<span class="n">ys</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x_small</span><span class="o">**</span><span class="n">x_small</span><span class="p">)</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_small</span><span class="p">,</span> <span class="n">ys</span><span class="p">,</span> <span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;f(x)&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="figure align-default">
<img alt="../_images/output_single-variable-calculus_vn_99f0a2_5_0.svg" src="../_images/output_single-variable-calculus_vn_99f0a2_5_0.svg" /></div>
<!--
This is the key observation of single variable calculus: the behavior of familiar functions can be modeled by a line in a small enough range.
This means that for most functions, it is reasonable to expect that as we shift the $x$ value of the function by a little bit, the output $f(x)$ will also be shifted by a little bit.
The only question we need to answer is, "How large is the change in the output compared to the change in the input?
Is it half as large?  Twice as large?"
--><p>Đây là một trong những quan sát cốt lõi nhất trong giải tích: hành vi
của các hàm số phổ biến có thể được mô hình hóa bằng một đường thẳng
trên một khoảng đủ nhỏ. Điều này nghĩa là với hầu hết các hàm số, chúng
ta có thể trông đợi rằng khi dịch chuyển <span class="math notranslate nohighlight">\(x\)</span> một khoảng nhỏ,
<span class="math notranslate nohighlight">\(f(x)\)</span> cũng sẽ dịch chuyển một khoảng nhỏ. Câu hỏi duy nhất mà
chúng ta cần trả lời là “Sự thay đổi của giá trị đầu ra lớn gấp bao
nhiêu lần so với sự thay đổi của giá trị đầu vào? Bằng một nửa? Hay sẽ
lớn gấp đôi?”</p>
<!--
Thus, we can consider the ratio of the change in the output of a function for a small change in the input of the function.  We can write this formally as
--><p>Ta cũng có thể xét nó như tỷ lệ giữa sự thay đổi của đầu ra so với sự
thay đổi nhỏ trong đầu vào của một hàm số. Chúng ta có thể biễu diễn nó
dưới dạng toán học là:</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-single-variable-calculus-vn-0">
<span class="eqno">(18.3.1)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-single-variable-calculus-vn-0" title="Permalink to this equation">¶</a></span>\[\frac{L(x+\epsilon) - L(x)}{(x+\epsilon) - x} = \frac{L(x+\epsilon) - L(x)}{\epsilon}.\]</div>
<!--
This is already enough to start to play around with in code.
For instance, suppose that we know that $L(x) = x^{2} + 1701(x-4)^3$, then we can see how large this value is at the point $x = 4$ as follows.
--><p>Những kiến thức trên đã đủ để chúng ta bắt đầu thực hành lập trình. Ví
dụ, giả sử <span class="math notranslate nohighlight">\(L(x) = x^{2} + 1701(x-4)^3\)</span>, ta có thể biết được độ
lớn của giá trị này tại điểm <span class="math notranslate nohighlight">\(x = 4\)</span> như sau:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define our function</span>
<span class="k">def</span> <span class="nf">L</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">1701</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="mi">4</span><span class="p">)</span><span class="o">**</span><span class="mi">3</span>
<span class="c1"># Print the difference divided by epsilon for several epsilon</span>
<span class="k">for</span> <span class="n">epsilon</span> <span class="ow">in</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.0001</span><span class="p">,</span> <span class="mf">0.00001</span><span class="p">]:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;epsilon = </span><span class="si">{</span><span class="n">epsilon</span><span class="si">:</span><span class="s1">.5f</span><span class="si">}</span><span class="s1"> -&gt; </span><span class="si">{</span><span class="p">(</span><span class="n">L</span><span class="p">(</span><span class="mi">4</span><span class="o">+</span><span class="n">epsilon</span><span class="p">)</span> <span class="o">-</span> <span class="n">L</span><span class="p">(</span><span class="mi">4</span><span class="p">))</span> <span class="o">/</span> <span class="n">epsilon</span><span class="si">:</span><span class="s1">.5f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">epsilon</span> <span class="o">=</span> <span class="mf">0.10000</span> <span class="o">-&gt;</span> <span class="mf">25.11000</span>
<span class="n">epsilon</span> <span class="o">=</span> <span class="mf">0.00100</span> <span class="o">-&gt;</span> <span class="mf">8.00270</span>
<span class="n">epsilon</span> <span class="o">=</span> <span class="mf">0.00010</span> <span class="o">-&gt;</span> <span class="mf">8.00012</span>
<span class="n">epsilon</span> <span class="o">=</span> <span class="mf">0.00001</span> <span class="o">-&gt;</span> <span class="mf">8.00001</span>
</pre></div>
</div>
<!--
Now, if we are observant, we will notice that the output of this number is suspiciously close to $8$.
Indeed, if we decrease $\epsilon$, we will see value becomes progressively closer to $8$.
Thus we may conclude, correctly, that the value we seek (the degree a change in the input changes the output) should be $8$ at the point $x=4$.
The way that a mathematician encodes this fact is
--><p>Nếu để ý kĩ, chúng ta sẽ nhận ra rằng kết quả của con số này xấp xỉ
<span class="math notranslate nohighlight">\(8\)</span>. Trong trường hợp ta giảm <span class="math notranslate nohighlight">\(\epsilon\)</span> thì giá trị đầu ra
ngày càng tiến gần đến <span class="math notranslate nohighlight">\(8\)</span>. Vì vậy chúng ta có thể kết luận một
cách chính xác, rằng mức độ thay đổi của đầu ra khi đầu vào thay đổi là
<span class="math notranslate nohighlight">\(8\)</span> tại điểm <span class="math notranslate nohighlight">\(x=4\)</span>. Có thể viết dưới dạng toán học như sau:</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-single-variable-calculus-vn-1">
<span class="eqno">(18.3.2)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-single-variable-calculus-vn-1" title="Permalink to this equation">¶</a></span>\[\lim_{\epsilon \rightarrow 0}\frac{L(4+\epsilon) - L(4)}{\epsilon} = 8.\]</div>
<!--
As a bit of a historical digression: in the first few decades of neural network research,
scientists used this algorithm (the *method of finite differences*) to evaluate how a loss function changed under small perturbation:
just change the weights and see how the loss changed.
This is computationally inefficient, requiring two evaluations of the loss function to see how a single change of one variable influenced the loss.
If we tried to do this with even a paltry few thousand parameters, it would require several thousand evaluations of the network over the entire dataset!
It was not solved until 1986 that the *backpropagation algorithm* introduced in :cite:`Rumelhart.Hinton.Williams.ea.1988` provided
a way to calculate how *any* change of the weights together would change the loss in the same computation time as a single prediction of the network over the dataset.
--><p>Một chút bàn luận ngoài lề về lịch sử: trong những thập kỷ đầu tiên của
các nghiên cứu mạng nơ-ron, các nhà khoa học đã sử dụng thuật toán này
(<em>sai phân hữu hạn - finite differences</em>) để đánh giá một hàm mất mát
dưới các nhiễu loạn nhỏ: chỉ cần thay đổi trọng số và xem cách thức mà
hàm mất mát thay đổi. Đây là một cách tính toán không hiệu quả, đòi hỏi
đến hai lần tính hàm mất mát để thấy được sự tác động của một thay đổi
lên hàm mất mát đó. Thậm chí nếu chúng ta sử dụng phương pháp này với
vài nghìn tham số nhỏ, nó cũng sẽ đòi hỏi phải chạy mạng nơ-ron hàng
nghìn lần trên toàn bộ dữ liệu. Phải đến năm 1986 thì vấn đề này với
được giải quyết khi <em>thuật toán lan truyền ngược</em> (<em>backpropagation
algorithm</em>) được giới thiệu ở
<a class="bibtex reference internal" href="../chapter_references/zreferences.html#rumelhart-hinton-williams-ea-1988" id="id1">[Rumelhart et al., 1988]</a> đã đem đến một giải pháp để
tính toán sức ảnh hưởng của những thay đổi <em>bất kỳ</em> từ các trọng số lên
hàm mất mát với thời gian tính toán chỉ bằng thời gian mô hình đưa ra dự
đoán trên tập dữ liệu.</p>
<!--
Back in our example, this value $8$ is different for different values of $x$, so it makes sense to define it as a function of $x$.
More formally, this value dependent rate of change is referred to as the *derivative* which is written as
--><p>Quay lại với ví dụ của chúng ta, giá trị <span class="math notranslate nohighlight">\(8\)</span> này biến thiên với
các trị khác nhau của <span class="math notranslate nohighlight">\(x\)</span>, vậy nên sẽ là hợp lý nếu chúng ta định
nghĩa nó như là một hàm của <span class="math notranslate nohighlight">\(x\)</span>. Một cách chính thống hơn, độ biến
thiên của giá trị này được gọi là <em>đạo hàm</em> và được viết là:</p>
<div class="math notranslate nohighlight" id="equation-eq-der-def">
<span class="eqno">(18.3.3)<a class="headerlink" href="#equation-eq-der-def" title="Permalink to this equation">¶</a></span>\[\frac{df}{dx}(x) = \lim_{\epsilon \rightarrow 0}\frac{f(x+\epsilon) - f(x)}{\epsilon}.\]</div>
<!--
Different texts will use different notations for the derivative.
For instance, all of the below notations indicate the same thing:
--><p>Các văn bản khác nhau sẽ sử dụng các ký hiệu khác nhau cho đạo hàm.
Chẳng hạn, tất cả các ký hiệu dưới đây đều diễn giải cùng một ý nghĩa:</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-single-variable-calculus-vn-2">
<span class="eqno">(18.3.4)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-single-variable-calculus-vn-2" title="Permalink to this equation">¶</a></span>\[\frac{df}{dx} = \frac{d}{dx}f = f' = \nabla_xf = D_xf = f_x.\]</div>
<!--
Most authors will pick a single notation and stick with it, however even that is not guaranteed.
It is best to be familiar with all of these.
We will use the notation $\frac{df}{dx}$ throughout this text, unless we want to take the derivative of a complex expression, in which case we will use $\frac{d}{dx}f$ to write expressions like
--><p>Phần lớn các tác giả sẽ chọn một ký hiệu duy nhất để sử dụng xuyên suốt,
tuy nhiên không phải lúc nào điều này cũng được đảm bảo. Tốt hơn hết là
chúng ta nên làm quen với tất cả các ký hiệu này. Ký hiệu
<span class="math notranslate nohighlight">\(\frac{df}{dx}\)</span> sẽ được sử dụng trong toàn bộ cuốn sách này, trừ
trường hợp chúng ta cần lấy đạo hàm của một biểu thức phức tạp, khi đó
chúng ta sẽ sử dụng <span class="math notranslate nohighlight">\(\frac{d}{dx}f\)</span> để biểu diễn những biểu thức
như</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-single-variable-calculus-vn-3">
<span class="eqno">(18.3.5)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-single-variable-calculus-vn-3" title="Permalink to this equation">¶</a></span>\[\frac{d}{dx}\left[x^4+\cos\left(\frac{x^2+1}{2x-1}\right)\right].\]</div>
<!--
Oftentimes, it is intuitively useful to unravel the definition of derivative :eqref:`eq_der_def` again to see how a function changes when we make a small change of $x$:
--><p>Đôi khi, việc sử dụng định nghĩa của đạo hàm <a class="reference internal" href="#equation-eq-der-def">(18.3.3)</a> để
thấy một cách trực quan cách một hàm thay đổi khi <span class="math notranslate nohighlight">\(x\)</span> thay đổi một
khoảng nhỏ là rất hữu ích:</p>
<div class="math notranslate nohighlight" id="equation-eq-small-change">
<span class="eqno">(18.3.6)<a class="headerlink" href="#equation-eq-small-change" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{aligned} \frac{df}{dx}(x) = \lim_{\epsilon \rightarrow 0}\frac{f(x+\epsilon) - f(x)}{\epsilon} &amp; \implies \frac{df}{dx}(x) \approx \frac{f(x+\epsilon) - f(x)}{\epsilon} \\ &amp; \implies \epsilon \frac{df}{dx}(x) \approx f(x+\epsilon) - f(x) \\ &amp; \implies f(x+\epsilon) \approx f(x) + \epsilon \frac{df}{dx}(x). \end{aligned}\end{split}\]</div>
<!--
The last equation is worth explicitly calling out.
It tells us that if you take any function and change the input by a small amount, the output would change by that small amount scaled by the derivative.
--><p>Cần phải nói rõ hơn về phương trình cuối cùng. Nó cho chúng ta biết rằng
nếu ta chọn một hàm số bất kỳ và thay đổi đầu vào một lượng nhỏ, sự thay
đổi của đầu ra sẽ bằng với lượng nhỏ đó nhân với đạo hàm.</p>
<!--
In this way, we can understand the derivative as the scaling factor that tells us how large of change we get in the output from a change in the input.
--><p>Bằng cách này, chúng ta có thể hiểu đạo hàm là hệ số tỷ lệ cho biết mức
độ biến thiên của đầu ra khi đầu vào thay đổi.</p>
<!--
## Rules of Calculus
--></div>
<div class="section" id="quy-tac-giai-tich">
<span id="sec-derivative-table"></span><h2><span class="section-number">18.3.2. </span>Quy tắc Giải tích<a class="headerlink" href="#quy-tac-giai-tich" title="Permalink to this headline">¶</a></h2>
<!--
We now turn to the task of understanding how to compute the derivative of an explicit function.
A full formal treatment of calculus would derive everything from first principles.
We will not indulge in this temptation here, but rather provide an understanding of the common rules encountered.
--><p>Bây giờ chúng ta sẽ học cách để tính đạo hàm của một hàm cụ thể. Dạy
giải tích một cách chính quy sẽ phải chứng minh lại tất cả mọi thứ từ
những định đề căn bản nhất. Tuy nhiên chúng tôi sẽ không làm như vậy mà
sẽ cung cấp các quy tắc tính đạo hàm phổ biến thường gặp.</p>
<!--
### Common Derivatives
--><div class="section" id="cac-dao-ham-pho-bien">
<h3><span class="section-number">18.3.2.1. </span>Các Đạo hàm phổ biến<a class="headerlink" href="#cac-dao-ham-pho-bien" title="Permalink to this headline">¶</a></h3>
<!--
As was seen in :numref:`sec_calculus`, when computing derivatives one can oftentimes use a series of rules to reduce the computation to a few core functions.
We repeat them here for ease of reference.
--><p>Như ở <a class="reference internal" href="../chapter_preliminaries/calculus_vn.html#sec-calculus"><span class="std std-numref">Section 2.4</span></a>, khi tính đạo hàm ta có thể sử dụng một
chuỗi các quy tắc để chia nhỏ tính toán thành các hàm cơ bản. Chúng tôi
sẽ nhắc lại chúng ở đây để bạn đọc dễ tham khảo.</p>
<!--
* **Derivative of constants.** $\frac{d}{dx}c = 0$.
* **Derivative of linear functions.** $\frac{d}{dx}(ax) = a$.
* **Power rule.** $\frac{d}{dx}x^n = nx^{n-1}$.
* **Derivative of exponentials.** $\frac{d}{dx}e^x = e^x$.
* **Derivative of the logarithm.** $\frac{d}{dx}\log(x) = \frac{1}{x}$.
--><ul class="simple">
<li><strong>Đạo hàm hằng số:</strong> <span class="math notranslate nohighlight">\(\frac{d}{dx}c = 0\)</span>.</li>
<li><strong>Đạo hàm hàm tuyến tính:</strong> <span class="math notranslate nohighlight">\(\frac{d}{dx}(ax) = a\)</span>.</li>
<li><strong>Quy tắc lũy thừa:</strong> <span class="math notranslate nohighlight">\(\frac{d}{dx}x^n = nx^{n-1}\)</span>.</li>
<li><strong>Đạo hàm hàm mũ cơ số tự nhiên:</strong> <span class="math notranslate nohighlight">\(\frac{d}{dx}e^x = e^x\)</span>.</li>
<li><strong>Đạo hàm hàm logarit cơ số tự nhiên:</strong>
<span class="math notranslate nohighlight">\(\frac{d}{dx}\log(x) = \frac{1}{x}\)</span>.</li>
</ul>
<!--
### Derivative Rules
--></div>
<div class="section" id="cac-quy-tac-tinh-dao-ham">
<h3><span class="section-number">18.3.2.2. </span>Các Quy tắc tính Đạo hàm<a class="headerlink" href="#cac-quy-tac-tinh-dao-ham" title="Permalink to this headline">¶</a></h3>
<!--
If every derivative needed to be separately computed and stored in a table, differential calculus would be near impossible.
It is a gift of mathematics that we can generalize the above derivatives and compute more complex derivatives like finding the derivative of $f(x) = \log\left(1+(x-1)^{10}\right)$.  As was mentioned in :numref:`sec_calculus`, the key to doing so is to codify what happens when we take functions and combine them in various ways, most importantly: sums, products, and compositions.
--><p>Nếu mọi đạo hàm cần được tính một cách riêng biệt và lưu vào một bảng,
giải tích vi phân sẽ gần như bất khả thi. Toán học đã mang lại một món
quà giúp tổng quát hóa các đạo hàm ở phần trên và giúp tính các đạo hàm
phức tạp hơn như đạo hàm của
<span class="math notranslate nohighlight">\(f(x) = \log\left(1+(x-1)^{10}\right)\)</span>. Như được đề cập trong
<a class="reference internal" href="../chapter_preliminaries/calculus_vn.html#sec-calculus"><span class="std std-numref">Section 2.4</span></a>, chìa khóa để thực hiện việc này là hệ thống
hóa việc tính đạo hàm cho các hàm kết hợp theo nhiều cách: tổng, tích và
hợp.</p>
<!--
* **Sum rule.** $\frac{d}{dx}\left(g(x) + h(x)\right) = \frac{dg}{dx}(x) + \frac{dh}{dx}(x)$.
* **Product rule.** $\frac{d}{dx}\left(g(x)\cdot h(x)\right) = g(x)\frac{dh}{dx}(x) + \frac{dg}{dx}(x)h(x)$.
* **Chain rule.** $\frac{d}{dx}g(h(x)) = \frac{dg}{dh}(h(x))\cdot \frac{dh}{dx}(x)$.
--><ul class="simple">
<li><strong>Quy tắc tổng.</strong>
<span class="math notranslate nohighlight">\(\frac{d}{dx}\left(g(x) + h(x)\right) = \frac{dg}{dx}(x) + \frac{dh}{dx}(x)\)</span>.</li>
<li><strong>Quy tắc tích.</strong>
<span class="math notranslate nohighlight">\(\frac{d}{dx}\left(g(x)\cdot h(x)\right) = g(x)\frac{dh}{dx}(x) + \frac{dg}{dx}(x)h(x)\)</span>.</li>
<li><strong>Quy tắc dây chuyền.</strong>
<span class="math notranslate nohighlight">\(\frac{d}{dx}g(h(x)) = \frac{dg}{dh}(h(x))\cdot \frac{dh}{dx}(x)\)</span>.</li>
</ul>
<!--
Let us see how we may use :eqref:`eq_small_change` to understand these rules.  For the sum rule, consider following chain of reasoning:
--><p>Cùng xem chúng ta có thể sử dụng <a class="reference internal" href="#equation-eq-small-change">(18.3.6)</a> như thế nào
để hiểu những quy tắc này. Với quy tắc tổng, xét chuỗi biến đổi sau đây:</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-single-variable-calculus-vn-4">
<span class="eqno">(18.3.7)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-single-variable-calculus-vn-4" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{aligned}
f(x+\epsilon) &amp; = g(x+\epsilon) + h(x+\epsilon) \\
&amp; \approx g(x) + \epsilon \frac{dg}{dx}(x) + h(x) + \epsilon \frac{dh}{dx}(x) \\
&amp; = g(x) + h(x) + \epsilon\left(\frac{dg}{dx}(x) + \frac{dh}{dx}(x)\right) \\
&amp; = f(x) + \epsilon\left(\frac{dg}{dx}(x) + \frac{dh}{dx}(x)\right).
\end{aligned}\end{split}\]</div>
<!--
By comparing this result with the fact that $f(x+\epsilon) \approx f(x) + \epsilon \frac{df}{dx}(x)$, we see that $\frac{df}{dx}(x) = \frac{dg}{dx}(x) + \frac{dh}{dx}(x)$ as desired.
The intuition here is: when we change the input $x$, $g$ and $h$ jointly contribute to the change of the output by $\frac{dg}{dx}(x)$ and $\frac{dh}{dx}(x)$.
--><p>Đồng nhất hệ số với
<span class="math notranslate nohighlight">\(f(x+\epsilon) \approx f(x) + \epsilon \frac{df}{dx}(x)\)</span>, ta có
<span class="math notranslate nohighlight">\(\frac{df}{dx}(x) = \frac{dg}{dx}(x) + \frac{dh}{dx}(x)\)</span> như mong
đợi. Một cách trực quan, ta có thể giải thích như sau: khi thay đổi đầu
vào <span class="math notranslate nohighlight">\(x\)</span>, <span class="math notranslate nohighlight">\(g\)</span> và <span class="math notranslate nohighlight">\(h\)</span> cùng đóng góp tới sự thay đổi của
<span class="math notranslate nohighlight">\(\frac{dg}{dx}(x)\)</span> và <span class="math notranslate nohighlight">\(\frac{dh}{dx}(x)\)</span> ở đầu ra.</p>
<!--
The product is more subtle, and will require a new observation about how to work with these expressions.  We will begin as before using :eqref:`eq_small_change`:
--><p>Đối với quy tắc tích thì phức tạp hơn một chút và đòi hỏi một quan sát
mới khi xử lý các biểu thức này. Cũng giống như trước, ta bắt đầu bằng
<a class="reference internal" href="#equation-eq-small-change">(18.3.6)</a>:</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-single-variable-calculus-vn-5">
<span class="eqno">(18.3.8)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-single-variable-calculus-vn-5" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{aligned}
f(x+\epsilon) &amp; = g(x+\epsilon)\cdot h(x+\epsilon) \\
&amp; \approx \left(g(x) + \epsilon \frac{dg}{dx}(x)\right)\cdot\left(h(x) + \epsilon \frac{dh}{dx}(x)\right) \\
&amp; = g(x)\cdot h(x) + \epsilon\left(g(x)\frac{dh}{dx}(x) + \frac{dg}{dx}(x)h(x)\right) + \epsilon^2\frac{dg}{dx}(x)\frac{dh}{dx}(x) \\
&amp; = f(x) + \epsilon\left(g(x)\frac{dh}{dx}(x) + \frac{dg}{dx}(x)h(x)\right) + \epsilon^2\frac{dg}{dx}(x)\frac{dh}{dx}(x). \\
\end{aligned}\end{split}\]</div>
<!--
This resembles the computation done above, and indeed we see our answer ($\frac{df}{dx}(x) = g(x)\frac{dh}{dx}(x) + \frac{dg}{dx}(x)h(x)$) sitting next to $\epsilon$, but there is the issue of that term of size $\epsilon^{2}$.
We will refer to this as a *higher-order term*, since the power of $\epsilon^2$ is higher than the power of $\epsilon^1$.
We will see in a later section that we will sometimes want to keep track of these, however for now observe that if $\epsilon = 0.0000001$, then $\epsilon^{2}= 0.0000000000001$, which is vastly smaller.
As we send $\epsilon \rightarrow 0$, we may safely ignore the higher order terms.
As a general convention in this appendix, we will use "$\approx$" to denote that the two terms are equal up to higher order terms.
However, if we wish to be more formal we may examine the difference quotient
--><p>Việc này giống với những tính toán trước đây, và dễ thấy kết quả của ta
(<span class="math notranslate nohighlight">\(\frac{df}{dx}(x) = g(x)\frac{dh}{dx}(x) + \frac{dg}{dx}(x)h(x)\)</span>)
là số hạng được nhân với <span class="math notranslate nohighlight">\(\epsilon\)</span>, nhưng vấn đề là ở số hạng
nhân với giá trị <span class="math notranslate nohighlight">\(\epsilon^{2}\)</span>. Chúng ta sẽ gọi số hạng này là
<em>số hạng bậc cao</em>, bởi số mũ của <span class="math notranslate nohighlight">\(\epsilon^2\)</span> cao hơn số mũ của
<span class="math notranslate nohighlight">\(\epsilon^1\)</span>. Về sau ta sẽ thấy rằng thi thoảng ta muốn giữ các số
hạng này, tuy nhiên hiện tại có thể thấy rằng nếu
<span class="math notranslate nohighlight">\(\epsilon = 0.0000001\)</span>, thì <span class="math notranslate nohighlight">\(\epsilon^{2}= 0.0000000000001\)</span>,
là một số nhỏ hơn rất nhiều. Khi đưa <span class="math notranslate nohighlight">\(\epsilon \rightarrow 0\)</span>, ta
có thể bỏ qua các số hạng bậc cao. Ta sẽ quy ước sử dụng
“<span class="math notranslate nohighlight">\(\approx\)</span>” để ký hiệu rằng hai số hạng bằng nhau với sai số là
các thành phần bậc cao. Nếu muốn biểu diễn chính quy hơn, ta có thể xét
phương trình</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-single-variable-calculus-vn-6">
<span class="eqno">(18.3.9)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-single-variable-calculus-vn-6" title="Permalink to this equation">¶</a></span>\[\frac{f(x+\epsilon) - f(x)}{\epsilon} = g(x)\frac{dh}{dx}(x) + \frac{dg}{dx}(x)h(x) + \epsilon \frac{dg}{dx}(x)\frac{dh}{dx}(x),\]</div>
<!--
and see that as we send $\epsilon \rightarrow 0$, the right hand term goes to zero as well.
--><p>và thấy rằng khi <span class="math notranslate nohighlight">\(\epsilon \rightarrow 0\)</span>, số hạng bên phải cũng
tiến về không.</p>
<!--
Finally, with the chain rule, we can again progress as before using :eqref:`eq_small_change` and see that
--><p>Cuối cùng, với quy tắc dây chuyền, ta vẫn có thể tiếp tục khai triển sử
dụng <a class="reference internal" href="#equation-eq-small-change">(18.3.6)</a> và thấy rằng:</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-single-variable-calculus-vn-7">
<span class="eqno">(18.3.10)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-single-variable-calculus-vn-7" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{aligned}
f(x+\epsilon) &amp; = g(h(x+\epsilon)) \\
&amp; \approx g\left(h(x) + \epsilon \frac{dh}{dx}(x)\right) \\
&amp; \approx g(h(x)) + \epsilon \frac{dh}{dx}(x) \frac{dg}{dh}(h(x))\\
&amp; = f(x) + \epsilon \frac{dg}{dh}(h(x))\frac{dh}{dx}(x).
\end{aligned}\end{split}\]</div>
<!--
where in the second line we view the function $g$ as having its input ($h(x)$) shifted by the tiny quantity $\epsilon \frac{dh}{dx}(x)$.
--><p>Chú ý là ở dòng thứ hai trong chuỗi khai triển trên, chúng ta đã xem đối
số <span class="math notranslate nohighlight">\(h(x)\)</span> của hàm <span class="math notranslate nohighlight">\(g\)</span> như là bị dịch đi bởi một lượng rất
nhỏ <span class="math notranslate nohighlight">\(\epsilon \frac{dh}{dx}(x)\)</span>.</p>
<!--
These rule provide us with a flexible set of tools to compute essentially any expression desired.  For instance,
--><p>Các quy tắc này cung cấp cho chúng ta một tập hợp các công cụ linh hoạt
để tính toán đạo hàm của hầu như bất kỳ biểu thức nào ta muốn. Chẳng hạn
như trong ví dụ sau:</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-single-variable-calculus-vn-8">
<span class="eqno">(18.3.11)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-single-variable-calculus-vn-8" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{aligned}
\frac{d}{dx}\left[\log\left(1+(x-1)^{10}\right)\right] &amp; = \left(1+(x-1)^{10}\right)^{-1}\frac{d}{dx}\left[1+(x-1)^{10}\right]\\
&amp; = \left(1+(x-1)^{10}\right)^{-1}\left(\frac{d}{dx}[1] + \frac{d}{dx}[(x-1)^{10}]\right) \\
&amp; = \left(1+(x-1)^{10}\right)^{-1}\left(0 + 10(x-1)^9\frac{d}{dx}[x-1]\right) \\
&amp; = 10\left(1+(x-1)^{10}\right)^{-1}(x-1)^9 \\
&amp; = \frac{10(x-1)^9}{1+(x-1)^{10}}.
\end{aligned}\end{split}\]</div>
<!--
Where each line has used the following rules:
--><p>Mỗi dòng của ví dụ này đã sử dụng các quy tắc sau:</p>
<!--
1. The chain rule and derivative of logarithm.
2. The sum rule.
3. The derivative of constants, chain rule, and power rule.
4. The sum rule, derivative of linear functions, derivative of constants.
--><ol class="arabic simple">
<li>Quy tắc dây chuyền và công thức đạo hàm của hàm logarit.</li>
<li>Quy tắc đạo hàm của tổng.</li>
<li>Đạo hàm của hằng số, quy tắc dây chuyền, và quy tắc đạo hàm của lũy
thừa.</li>
<li>Quy tắc đạo hàm của tổng, đạo hàm của hàm tuyến tính, đạo hàm của
hằng số.</li>
</ol>
<!--
Two things should be clear after doing this example:
--><p>Từ ví dụ trên, chúng ta có thể dễ dàng rút ra được hai điều:</p>
<!--
1. Any function we can write down using sums, products, constants, powers, exponentials, and logarithms can have its derivate computed mechanically by following these rules.
2. Having a human follow these rules can be tedious and error prone!
--><ol class="arabic simple">
<li>Chúng ta có thể lấy đạo hàm của bất kỳ hàm số nào mà có thể diễn tả
được bằng tổng, tích, hằng số, lũy thừa, hàm mũ, và hàm logarit bằng
cách sử dụng những quy tắc trên một cách máy móc.</li>
<li>Quá trình dùng những quy tắc này để tính đạo hàm bằng tay có thể sẽ
rất tẻ nhạt và dễ mắc lỗi.</li>
</ol>
<!--
Thankfully, these two facts together hint towards a way forward: this is a perfect candidate for mechanization!
Indeed backpropagation, which we will revisit later in this section, is exactly that.
--><p>Rất may là hai điều này gộp chung lại gợi ý cho chúng ta một hướng phát
triển: đây chính là cơ hội lý tưởng để tự động hóa bằng máy tính! Thật
vậy, kỹ thuật lan truyền ngược, mà chúng ta sẽ gặp lại sau ở mục này, là
một cách hiện thực hóa ý tưởng này.</p>
<!--
### Linear Approximation
--></div>
<div class="section" id="xap-xi-tuyen-tinh">
<h3><span class="section-number">18.3.2.3. </span>Xấp xỉ Tuyến tính<a class="headerlink" href="#xap-xi-tuyen-tinh" title="Permalink to this headline">¶</a></h3>
<!--
When working with derivatives, it is often useful to geometrically interpret the approximation used above.  In particular, note that the equation
--><p>Thông thường khi làm việc với đạo hàm, sẽ rất hữu ích nếu chúng ta có
thể diễn tả sự xấp xỉ ở trên theo phương diện hình học. Nói một cách cụ
thể, phương trình này</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-single-variable-calculus-vn-9">
<span class="eqno">(18.3.12)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-single-variable-calculus-vn-9" title="Permalink to this equation">¶</a></span>\[f(x+\epsilon) \approx f(x) + \epsilon \frac{df}{dx}(x),\]</div>
<!--
approximates the value of $f$ by a line which passes through the point $(x, f(x))$ and has slope $\frac{df}{dx}(x)$.
In this way we say that the derivative gives a linear approximation to the function $f$, as illustrated below:
--><p>xấp xỉ giá trị của <span class="math notranslate nohighlight">\(f\)</span> bằng một đường thẳng đi qua điểm
<span class="math notranslate nohighlight">\((x, f(x))\)</span> và có độ dốc <span class="math notranslate nohighlight">\(\frac{df}{dx}(x)\)</span>. Với cách hiểu
này, ta nói rằng đạo hàm cho ta một xấp xỉ tuyến tính của hàm số
<span class="math notranslate nohighlight">\(f\)</span>, như minh họa dưới đây:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Compute sin</span>
<span class="n">xs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>
<span class="n">plots</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">xs</span><span class="p">)]</span>
<span class="c1"># Compute some linear approximations. Use d(sin(x)) / dx = cos(x)</span>
<span class="k">for</span> <span class="n">x0</span> <span class="ow">in</span> <span class="p">[</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">]:</span>
    <span class="n">plots</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x0</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">xs</span> <span class="o">-</span> <span class="n">x0</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">x0</span><span class="p">))</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">plots</span><span class="p">,</span> <span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;f(x)&#39;</span><span class="p">,</span> <span class="n">ylim</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">])</span>
</pre></div>
</div>
<div class="figure align-default">
<img alt="../_images/output_single-variable-calculus_vn_99f0a2_9_0.svg" src="../_images/output_single-variable-calculus_vn_99f0a2_9_0.svg" /></div>
<!--
### Higher Order Derivatives
--></div>
<div class="section" id="dao-ham-cap-cao">
<h3><span class="section-number">18.3.2.4. </span>Đạo hàm Cấp cao<a class="headerlink" href="#dao-ham-cap-cao" title="Permalink to this headline">¶</a></h3>
<!--
Let us now do something that may on the surface seem strange.
Take a function $f$ and compute the derivative $\frac{df}{dx}$.
This gives us the rate of change of $f$ at any point.
--><p>Bây giờ, hãy cùng làm một việc mà nhìn sơ qua thì có vẻ kỳ quặc. Bắt đầu
bằng việc lấy một hàm số <span class="math notranslate nohighlight">\(f\)</span> và tính đạo hàm
<span class="math notranslate nohighlight">\(\frac{df}{dx}\)</span>. Nó sẽ cho chúng ta tốc độ thay đổi của <span class="math notranslate nohighlight">\(f\)</span>
tại bất cứ điểm nào.</p>
<!--
However, the derivative, $\frac{df}{dx}$, can be viewed as a function itself, so nothing stops us from computing
 the derivative of $\frac{df}{dx}$ to get $\frac{d^2f}{dx^2} = \frac{df}{dx}\left(\frac{df}{dx}\right)$.
We will call this the second derivative of $f$.
This function is the rate of change of the rate of change of $f$, or in other words, how the rate of change is changing.
We may apply the derivative any number of times to obtain what is called the $n$-th derivative.
To keep the notation clean, we will denote the $n$-th derivative as
--><p>Tuy nhiên, vì bản thân đạo hàm <span class="math notranslate nohighlight">\(\frac{df}{dx}\)</span> cũng là một hàm số,
không có gì ngăn cản chúng ta tiếp tục tính đạo hàm của
<span class="math notranslate nohighlight">\(\frac{df}{dx}\)</span> để có
<span class="math notranslate nohighlight">\(\frac{d^2f}{dx^2} = \frac{df}{dx}\left(\frac{df}{dx}\right)\)</span>.
Chúng ta sẽ gọi đây là đạo hàm cấp hai của <span class="math notranslate nohighlight">\(f\)</span>. Hàm số này là tốc
độ thay đổi của tốc độ thay đổi của <span class="math notranslate nohighlight">\(f\)</span>, hay nói cách khác, nó thể
hiện tốc độ thay đổi của <span class="math notranslate nohighlight">\(f\)</span> đang thay đổi như thế nào. Chúng ta
có thể tiếp tục lấy đạo hàm như vậy thêm nhiều lần nữa để có được thứ
gọi là đạo hàm cấp <span class="math notranslate nohighlight">\(n\)</span>. Để ký hiệu được gọn gàng, chúng ta sẽ biểu
thị đạo hàm cấp <span class="math notranslate nohighlight">\(n\)</span> như sau:</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-single-variable-calculus-vn-10">
<span class="eqno">(18.3.13)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-single-variable-calculus-vn-10" title="Permalink to this equation">¶</a></span>\[f^{(n)}(x) = \frac{d^{n}f}{dx^{n}} = \left(\frac{d}{dx}\right)^{n} f.\]</div>
<!--
Let us try to understand *why* this is a useful notion.
Below, we visualize $f^{(2)}(x)$, $f^{(1)}(x)$, and $f(x)$.
--><p>Hãy tìm hiểu xem <em>tại sao</em> đây lại là một khái niệm hữu ích. Các hàm số
<span class="math notranslate nohighlight">\(f^{(2)}(x)\)</span>, <span class="math notranslate nohighlight">\(f^{(1)}(x)\)</span>, và <span class="math notranslate nohighlight">\(f(x)\)</span> được biểu diễn
trong các đồ thị dưới đây.</p>
<!--
First, consider the case that the second derivative $f^{(2)}(x)$ is a positive constant.
This means that the slope of the first derivative is positive.
As a result, the first derivative $f^{(1)}(x)$ may start out negative, becomes zero at a point, and then becomes positive in the end.
This tells us the slope of our original function $f$ and therefore, the function $f$ itself decreases, flattens out, then increases.
In other words, the function $f$ curves up, and has a single minimum as is shown in :numref:`fig_positive-second`.
--><p>Đầu tiên, xét trường hợp đạo hàm bậc hai <span class="math notranslate nohighlight">\(f^{(2)}(x)\)</span> là một hằng
số dương. Điều này nghĩa là độ dốc của đạo hàm bậc nhất là dương. Hệ quả
là, đạo hàm bậc nhất <span class="math notranslate nohighlight">\(f^{(1)}(x)\)</span> có thể khởi đầu ở âm, bằng không
tại một điểm nào đó, rồi cuối cùng tăng lên dương. Điều này cho chúng ta
biết độ dốc của hàm <span class="math notranslate nohighlight">\(f\)</span> ban đầu và do đó, giá trị hàm <span class="math notranslate nohighlight">\(f\)</span> sẽ
giảm xuống đến điểm nào đó rồi tăng lên. Nói cách khác, đồ thị hàm
<span class="math notranslate nohighlight">\(f\)</span> là đường cong đi lên, có một cực tiểu như trong
<a class="reference internal" href="#fig-positive-second"><span class="std std-numref">Fig. 18.3.1</span></a>.</p>
<!--
![If we assume the second derivative is a positive constant, then the fist derivative in increasing, which implies the function itself has a minimum.](../img/posSecDer.svg)
--><div class="figure align-default" id="id2">
<span id="fig-positive-second"></span><img alt="../_images/posSecDer.svg" src="../_images/posSecDer.svg" /><p class="caption"><span class="caption-number">Fig. 18.3.1 </span><span class="caption-text">Nếu giả định rằng đạo hàm bậc hai là một hằng số dương, thì đạo hàm
bậc nhất đồng biến, nghĩa là bản thân hàm đó có một cực tiểu.</span><a class="headerlink" href="#id2" title="Permalink to this image">¶</a></p>
</div>
<!--
Second, if the second derivative is a negative constant, that means that the first derivative is decreasing.
This implies the first derivative may start out positive, becomes zero at a point, and then becomes negative.
Hence, the function $f$ itself increases, flattens out, then decreases.
In other words, the function $f$ curves down, and has a single maximum as is shown in :numref:`fig_negative-second`.
--><p>Thứ hai là, nếu đạo hàm bậc hai là một hằng số âm, nghĩa là đạo hàm bậc
nhất nghịch biến. Vậy tức là đạo hàm bậc nhất có thể khời đầu là dương,
bằng không ở điểm nào đó, rồi giảm xuống âm. Do vậy, giá trị hàm
<span class="math notranslate nohighlight">\(f\)</span> tăng lên đến điểm nào đó rồi giảm xuống. Nói cách khác, đồ thị
hàm <span class="math notranslate nohighlight">\(f\)</span> là đường cong đi xuống, có một cực đại như trong
<a class="reference internal" href="#fig-negative-second"><span class="std std-numref">Fig. 18.3.2</span></a>.</p>
<!--
![If we assume the second derivative is a negative constant, then the fist derivative in decreasing, which implies the function itself has a maximum.](../img/negSecDer.svg)
--><div class="figure align-default" id="id3">
<span id="fig-negative-second"></span><img alt="../_images/negSecDer.svg" src="../_images/negSecDer.svg" /><p class="caption"><span class="caption-number">Fig. 18.3.2 </span><span class="caption-text">Nếu giả định đạo hàm bậc hai là một hằng số âm, thì đạo hàm bậc nhất
nghịch biến, nghĩa là hàm số có một cực đại.</span><a class="headerlink" href="#id3" title="Permalink to this image">¶</a></p>
</div>
<!--
Third, if the second derivative is a always zero, then the first derivative will never change---it is constant!
This means that $f$ increases (or decreases) at a fixed rate, and $f$ is itself a straight line  as is shown in :numref:`fig_zero-second`.
--><p>Thứ ba là, nếu đạo hàm bậc hai luôn luôn bằng không, thì đạo hàm bậc
nhất là hằng số! Nghĩa là hàm <span class="math notranslate nohighlight">\(f\)</span> tăng (hoặc giảm) với tốc độ cố
định, và đồ thị <span class="math notranslate nohighlight">\(f\)</span> là một đường thẳng giống như trong
<a class="reference internal" href="#fig-zero-second"><span class="std std-numref">Fig. 18.3.3</span></a>.</p>
<!--
![If we assume the second derivative is zero, then the fist derivative is constant, which implies the function itself is a straight line.](../img/zeroSecDer.svg)
--><div class="figure align-default" id="id4">
<span id="fig-zero-second"></span><img alt="../_images/zeroSecDer.svg" src="../_images/zeroSecDer.svg" /><p class="caption"><span class="caption-number">Fig. 18.3.3 </span><span class="caption-text">Nếu ta giả định đạo hàm bậc hai bằng không, thì đạo hàm bậc nhất là
hằng số, nên đồ thị hàm này là một đường thẳng.</span><a class="headerlink" href="#id4" title="Permalink to this image">¶</a></p>
</div>
<!--
To summarize, the second derivative can be interpreted as describing the way that the function $f$ curves.
A positive second derivative leads to a upwards curve, while a negative second derivative means that $f$ curves downwards, and a zero second derivative means that $f$ does not curve at all.
--><p>Tóm lại, đạo hàm bậc hai có thể được hiểu như một cách miêu tả đường
cong của đồ thị hàm <span class="math notranslate nohighlight">\(f\)</span>. Đạo hàm bậc hai dương thì đồ thị cong
lên, đạo hàm bậc hai âm thì hàm <span class="math notranslate nohighlight">\(f\)</span> cong xuống, và nếu bằng không
thì <span class="math notranslate nohighlight">\(f\)</span> là một đường thẳng.</p>
<!--
Let us take this one step further. Consider the function $g(x) = ax^{2}+ bx + c$.  We can then compute that
--><p>Hãy thử tiến xa hơn một bước. Xét hàm <span class="math notranslate nohighlight">\(g(x) = ax^{2}+ bx + c\)</span>. Ta
có thể tính được</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-single-variable-calculus-vn-11">
<span class="eqno">(18.3.14)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-single-variable-calculus-vn-11" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{aligned}
\frac{dg}{dx}(x) &amp; = 2ax + b \\
\frac{d^2g}{dx^2}(x) &amp; = 2a.
\end{aligned}\end{split}\]</div>
<!--
If we have some original function $f(x)$ in mind, we may compute the first two derivatives and find the values for $a, b$, and $c$ that make them match this computation.
Similarly to the previous section where we saw that the first derivative gave the best approximation with a straight line,
this construction provides the best approximation by a quadratic.  Let us visualize this for $f(x) = \sin(x)$.
--><p>Nếu đã có sẵn một hàm <span class="math notranslate nohighlight">\(f(x)\)</span>, ta có thể tính đạo hàm cấp một và
cấp hai của nó để tìm các giá trị <span class="math notranslate nohighlight">\(a, b\)</span>, và <span class="math notranslate nohighlight">\(c\)</span> thỏa mãn hệ
phương trình này. Cũng giống như ở mục trước ta đã thấy đạo hàm bậc một
cho ra xấp xỉ tốt nhất bằng một đường thẳng, đạo hàm bậc hai cung cấp
một xấp xỉ tốt nhất bằng một parabol. Hãy minh họa với trường hợp
<span class="math notranslate nohighlight">\(f(x) = \sin(x)\)</span>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Compute sin</span>
<span class="n">xs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>
<span class="n">plots</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">xs</span><span class="p">)]</span>
<span class="c1"># Compute some quadratic approximations. Use d(sin(x)) / dx = cos(x)</span>
<span class="k">for</span> <span class="n">x0</span> <span class="ow">in</span> <span class="p">[</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">]:</span>
    <span class="n">plots</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x0</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">xs</span> <span class="o">-</span> <span class="n">x0</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">x0</span><span class="p">)</span> <span class="o">-</span>
                              <span class="p">(</span><span class="n">xs</span> <span class="o">-</span> <span class="n">x0</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x0</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">plots</span><span class="p">,</span> <span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;f(x)&#39;</span><span class="p">,</span> <span class="n">ylim</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">])</span>
</pre></div>
</div>
<div class="figure align-default">
<img alt="../_images/output_single-variable-calculus_vn_99f0a2_11_0.svg" src="../_images/output_single-variable-calculus_vn_99f0a2_11_0.svg" /></div>
<!--
We will extend this idea to the idea of a *Taylor series* in the next section.
--><p>Ta sẽ mở rộng ý tưởng này thành ý tưởng của <em>chuỗi Taylor</em> trong mục
tiếp theo.</p>
<!--
### Taylor Series
--></div>
<div class="section" id="chuoi-taylor">
<h3><span class="section-number">18.3.2.5. </span>Chuỗi Taylor<a class="headerlink" href="#chuoi-taylor" title="Permalink to this headline">¶</a></h3>
<!--
The *Taylor series* provides a method to approximate the function $f(x)$ if we are given values for
the first $n$ derivatives at a point $x_0$, i.e., $\left\{ f(x_0), f^{(1)}(x_0), f^{(2)}(x_0), \ldots, f^{(n)}(x_0) \right\}$.
The idea will be to find a degree $n$ polynomial that matches all the given derivatives at $x_0$.
--><p><em>Chuỗi Taylor</em> cung cấp một phương pháp để xấp xỉ phương trình
<span class="math notranslate nohighlight">\(f(x)\)</span> nếu ta đã biết trước giá trị của <span class="math notranslate nohighlight">\(n\)</span> cấp đạo hàm đầu
tiên tại điểm <span class="math notranslate nohighlight">\(x_0\)</span>:
<span class="math notranslate nohighlight">\(\left\{ f(x_0), f^{(1)}(x_0), f^{(2)}(x_0), \ldots, f^{(n)}(x_0) \right\}\)</span>.
Ý tưởng là tìm một đa thức bậc <span class="math notranslate nohighlight">\(n\)</span> có các đạo hàm tại <span class="math notranslate nohighlight">\(x_0\)</span>
khớp với các đạo hàm đã biết.</p>
<!--
We saw the case of $n=2$ in the previous section and a little algebra shows this is
--><p>Ta đã thấy với trường hợp <span class="math notranslate nohighlight">\(n=2\)</span> ở chương trước và với một chút
biến đổi đại số, ta có được</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-single-variable-calculus-vn-12">
<span class="eqno">(18.3.15)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-single-variable-calculus-vn-12" title="Permalink to this equation">¶</a></span>\[f(x) \approx \frac{1}{2}\frac{d^2f}{dx^2}(x_0)(x-x_0)^{2}+ \frac{df}{dx}(x_0)(x-x_0) + f(x_0).\]</div>
<!--
As we can see above, the denominator of $2$ is there to cancel out the $2$ we get when we take two derivatives of $x^2$, while the other terms are all zero.
Same logic applies for the first derivative and the value itself.
--><p>Như ta đã thấy ở trên, mẫu số <span class="math notranslate nohighlight">\(2\)</span> là để rút gọn thừa số <span class="math notranslate nohighlight">\(2\)</span>
khi lấy đạo hàm bậc hai của <span class="math notranslate nohighlight">\(x^2\)</span>, các đạo hàm bậc cao hơn đều
bằng không. Cùng một cách lập luận cũng được áp dụng cho đạo hàm bậc một
và phần giá trị <span class="math notranslate nohighlight">\(f(x_0)\)</span>.</p>
<!--
If we push the logic further to $n=3$, we will conclude that
--><p>Nếu ta mở rộng cách lập luận này cho trường hợp <span class="math notranslate nohighlight">\(n=3\)</span>, ta sẽ kết
luận được</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-single-variable-calculus-vn-13">
<span class="eqno">(18.3.16)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-single-variable-calculus-vn-13" title="Permalink to this equation">¶</a></span>\[f(x) \approx \frac{\frac{d^3f}{dx^3}(x_0)}{6}(x-x_0)^3 + \frac{\frac{d^2f}{dx^2}(x_0)}{2}(x-x_0)^{2}+ \frac{df}{dx}(x_0)(x-x_0) + f(x_0).\]</div>
<!--
where the $6 = 3 \times 2 = 3!$ comes from the constant we get in front if we take three derivatives of $x^3$.
--><p>với <span class="math notranslate nohighlight">\(6 = 3 \times 2 = 3!\)</span> đến từ phần hằng số ta có được khi lấy
đạo hàm bậc 3 của <span class="math notranslate nohighlight">\(x^3\)</span>.</p>
<!--
Furthermore, we can get a degree $n$ polynomial by
--><p>Hơn nữa, ta có thể lấy một đa thức bậc <span class="math notranslate nohighlight">\(n\)</span> bằng cách</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-single-variable-calculus-vn-14">
<span class="eqno">(18.3.17)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-single-variable-calculus-vn-14" title="Permalink to this equation">¶</a></span>\[P_n(x) = \sum_{i = 0}^{n} \frac{f^{(i)}(x_0)}{i!}(x-x_0)^{i}.\]</div>
<!--
where the notation
--><p>với quy ước</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-single-variable-calculus-vn-15">
<span class="eqno">(18.3.18)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-single-variable-calculus-vn-15" title="Permalink to this equation">¶</a></span>\[f^{(n)}(x) = \frac{d^{n}f}{dx^{n}} = \left(\frac{d}{dx}\right)^{n} f.\]</div>
<!--
Indeed, $P_n(x)$ can be viewed as the best $n$-th degree polynomial approximation to our function $f(x)$.
--><p>Quả thật, <span class="math notranslate nohighlight">\(P_n(x)\)</span> có thể được xem là đa thức bậc <span class="math notranslate nohighlight">\(n\)</span> xấp xỉ
tốt nhất của hàm <span class="math notranslate nohighlight">\(f(x)\)</span>.</p>
<!--
While we are not going to dive all the way into the error of the above approximations, it is worth mentioning the the infinite limit.
In this case, for well behaved functions (known as real analytic functions) like $\cos(x)$ or $e^{x}$, we can write out the infinite number of terms and approximate the exactly same function
--><p>Dù ta sẽ không tìm hiểu kỹ sai số của xấp xỉ này, ta cũng nên nhắc tới
giới hạn vô cùng. Trong trường hợp này, các hàm khả vi vô hạn lần như
<span class="math notranslate nohighlight">\(\cos(x)\)</span> hoặc <span class="math notranslate nohighlight">\(e^{x}\)</span> có thể được biểu diễn xấp xỉ bằng vô
số các số hạng.</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-single-variable-calculus-vn-16">
<span class="eqno">(18.3.19)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-single-variable-calculus-vn-16" title="Permalink to this equation">¶</a></span>\[f(x) = \sum_{n = 0}^\infty \frac{f^{(n)}(x_0)}{n!}(x-x_0)^{n}.\]</div>
<!--
Take $f(x) = e^{x}$ as am example. Since $e^{x}$ is its own derivative, we know that $f^{(n)}(x) = e^{x}$.
Therefore, $e^{x}$ can be reconstructed by taking the Taylor series at $x_0 = 0$, i.e.,
--><p>Lấy hàm <span class="math notranslate nohighlight">\(f(x) = e^{x}\)</span> làm ví dụ. Vì <span class="math notranslate nohighlight">\(e^{x}\)</span> là đạo hàm của
chính nó, ta có <span class="math notranslate nohighlight">\(f^{(n)}(x) = e^{x}\)</span>. Do đó, hàm <span class="math notranslate nohighlight">\(e^{x}\)</span> có
thể được tái tạo bằng cách tính chuỗi Taylor tại <span class="math notranslate nohighlight">\(x_0 = 0\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-single-variable-calculus-vn-17">
<span class="eqno">(18.3.20)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-single-variable-calculus-vn-17" title="Permalink to this equation">¶</a></span>\[e^{x} = \sum_{n = 0}^\infty \frac{x^{n}}{n!} = 1 + x + \frac{x^2}{2} + \frac{x^3}{6} + \cdots.\]</div>
<!--
Let us see how this works in code and observe how increasing the degree of the Taylor approximation brings us closer to the desired function $e^x$.
--><p>Hãy cùng tìm hiểu cách lập trình và quan sát xem việc tăng bậc của xấp
xỉ Taylor đưa ta đến gần hơn với hàm mong muốn <span class="math notranslate nohighlight">\(e^x\)</span> như thế nào.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Compute the exponential function</span>
<span class="n">xs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>
<span class="n">ys</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span>
<span class="c1"># Compute a few Taylor series approximations</span>
<span class="n">P1</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">xs</span>
<span class="n">P2</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">xs</span> <span class="o">+</span> <span class="n">xs</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="mi">2</span>
<span class="n">P5</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">xs</span> <span class="o">+</span> <span class="n">xs</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">xs</span><span class="o">**</span><span class="mi">3</span> <span class="o">/</span> <span class="mi">6</span> <span class="o">+</span> <span class="n">xs</span><span class="o">**</span><span class="mi">4</span> <span class="o">/</span> <span class="mi">24</span> <span class="o">+</span> <span class="n">xs</span><span class="o">**</span><span class="mi">5</span> <span class="o">/</span> <span class="mi">120</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="p">[</span><span class="n">ys</span><span class="p">,</span> <span class="n">P1</span><span class="p">,</span> <span class="n">P2</span><span class="p">,</span> <span class="n">P5</span><span class="p">],</span> <span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;f(x)&#39;</span><span class="p">,</span> <span class="n">legend</span><span class="o">=</span><span class="p">[</span>
    <span class="s2">&quot;Exponential&quot;</span><span class="p">,</span> <span class="s2">&quot;Degree 1 Taylor Series&quot;</span><span class="p">,</span> <span class="s2">&quot;Degree 2 Taylor Series&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Degree 5 Taylor Series&quot;</span><span class="p">])</span>
</pre></div>
</div>
<div class="figure align-default">
<img alt="../_images/output_single-variable-calculus_vn_99f0a2_13_0.svg" src="../_images/output_single-variable-calculus_vn_99f0a2_13_0.svg" /></div>
<!--
Taylor series have two primary applications:
--><p>Chuỗi Taylor có hai ứng dụng chính:</p>
<!--
1. *Theoretical applications*: Often when we try to understand a too complex function, using Taylor series enables us to turn it into a polynomial that we can work with directly.
--><ol class="arabic simple">
<li><em>Ứng dụng lý thuyết</em>: Khi muốn tìm hiểu một hàm số quá phức tạp, ta
thường dùng chuỗi Taylor để biến nó thành một đa thức để có thể làm
việc trực tiếp.</li>
</ol>
<!--
2. *Numerical applications*: Some functions like $e^{x}$ or $\cos(x)$ are  difficult for machines to compute.
They can store tables of values at a fixed precision (and this is often done), but it still leaves open questions like "What is the 1000-th digit of $\cos(1)$?"
Taylor series are often helpful to answer such questions.
--><ol class="arabic simple" start="2">
<li><em>Ứng dụng số học</em>: Việc tính toán một số hàm như <span class="math notranslate nohighlight">\(e^x\)</span> hoặc
<span class="math notranslate nohighlight">\(\cos(x)\)</span> không đơn giản đối với máy tính. Chúng có thể lưu trữ
một bảng giá trị với độ chính xác nhất định (và thường thì chúng làm
vậy), nhưng việc đó vẫn không giải quyết được những câu hỏi như “Chữ
số thứ 1000 của <span class="math notranslate nohighlight">\(\cos(1)\)</span> là gì?”. Chuỗi Taylor thường có ích
cho việc trả lời các câu hỏi như vậy.</li>
</ol>
</div>
</div>
<div class="section" id="tom-tat">
<h2><span class="section-number">18.3.3. </span>Tóm tắt<a class="headerlink" href="#tom-tat" title="Permalink to this headline">¶</a></h2>
<!--
* Derivatives can be used to express how functions change when we change the input by a small amount.
* Elementary derivatives can be combined using derivative rules to create arbitrarily complex derivatives.
* Derivatives can be iterated to get second or higher order derivatives.  Each increase in order provides more fine grained information on the behavior of the function.
* Using information in the derivatives of a single data example, we can approximate well behaved functions by polynomials obtained from the Taylor series.
--><ul class="simple">
<li>Đạo hàm có thể được sử dụng để biểu diễn mức độ thay đổi của hàm số
khi đầu vào thay đổi một lượng nhỏ.</li>
<li>Các phép lấy đạo hàm cơ bản có thể kết hợp với nhau theo các quy tắc
đạo hàm để tính những đạo hàm phức tạp tùy ý.</li>
<li>Đạo hàm có thể được tính nhiều lần để lấy đạo hàm cấp hai hoặc các
cấp cao hơn. Mỗi lần tăng cấp đạo hàm cho ta thông tin chi tiết hơn
về hành vi của hàm số.</li>
<li>Bằng việc sử dụng thông tin từ đạo hàm của một điểm dữ liệu, ta có
thể xấp xỉ các hàm khả vi vô hạn lần bằng các đa thức lấy từ chuỗi
Taylor.</li>
</ul>
</div>
<div class="section" id="bai-tap">
<h2><span class="section-number">18.3.4. </span>Bài tập<a class="headerlink" href="#bai-tap" title="Permalink to this headline">¶</a></h2>
<!--
1. What is the derivative of $x^3-4x+1$?
2. What is the derivative of $\log(\frac{1}{x})$?
3. True or False: If $f'(x) = 0$ then $f$ has a maximum or minimum at $x$?
4. Where is the minimum of $f(x) = x\log(x)$ for $x\ge0$ (where we assume that $f$ takes the limiting value of $0$ at $f(0)$)?
--><ol class="arabic simple">
<li>Đạo hàm của <span class="math notranslate nohighlight">\(x^3-4x+1\)</span> là gì?</li>
<li>Đạo hàm của <span class="math notranslate nohighlight">\(\log(\frac{1}{x})\)</span> là gì?</li>
<li>Đúng hay Sai: Nếu <span class="math notranslate nohighlight">\(f'(x) = 0\)</span> thì <span class="math notranslate nohighlight">\(f\)</span> có cực đại hoặc cực
tiểu tại <span class="math notranslate nohighlight">\(x\)</span>?</li>
<li>Cực tiểu của <span class="math notranslate nohighlight">\(f(x) = x\log(x)\)</span> với <span class="math notranslate nohighlight">\(x\ge0\)</span> ở đâu (ở đây
ta giả sử rằng <span class="math notranslate nohighlight">\(f\)</span> có giới hạn bằng <span class="math notranslate nohighlight">\(0\)</span> tại
<span class="math notranslate nohighlight">\(f(0)\)</span>)?</li>
</ol>
</div>
<div class="section" id="thao-luan">
<h2><span class="section-number">18.3.5. </span>Thảo luận<a class="headerlink" href="#thao-luan" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li>Tiếng Anh: <a class="reference external" href="https://discuss.d2l.ai/t/412">MXNet</a>,
<a class="reference external" href="https://discuss.d2l.ai/t/1088">Pytorch</a>,
<a class="reference external" href="https://discuss.d2l.ai/t/1089">Tensorflow</a></li>
<li>Tiếng Việt: <a class="reference external" href="https://forum.machinelearningcoban.com/c/d2l">Diễn đàn Machine Learning Cơ
Bản</a></li>
</ul>
</div>
<div class="section" id="nhung-nguoi-thuc-hien">
<h2><span class="section-number">18.3.6. </span>Những người thực hiện<a class="headerlink" href="#nhung-nguoi-thuc-hien" title="Permalink to this headline">¶</a></h2>
<p>Bản dịch trong trang này được thực hiện bởi:</p>
<ul class="simple">
<li>Lê Khắc Hồng Phúc</li>
<li>Phạm Hồng Vinh</li>
<li>Vũ Hữu Tiệp</li>
<li>Nguyễn Lê Quang Nhật</li>
<li>Đoàn Võ Duy Thanh</li>
<li>Tạ H. Duy Nguyên</li>
<li>Mai Sơn Hải</li>
<li>Phạm Minh Đức</li>
<li>Nguyễn Văn Tâm</li>
<li>Nguyễn Văn Cường</li>
</ul>
</div>
</div>


        </div>
        <div class="side-doc-outline">
            <div class="side-doc-outline--content"> 
<div class="localtoc">
    <p class="caption">
      <span class="caption-text">Table Of Contents</span>
    </p>
    <ul>
<li><a class="reference internal" href="#">18.3. Giải tích một biến</a><ul>
<li><a class="reference internal" href="#giai-tich-vi-phan">18.3.1. Giải tích Vi phân</a></li>
<li><a class="reference internal" href="#quy-tac-giai-tich">18.3.2. Quy tắc Giải tích</a><ul>
<li><a class="reference internal" href="#cac-dao-ham-pho-bien">18.3.2.1. Các Đạo hàm phổ biến</a></li>
<li><a class="reference internal" href="#cac-quy-tac-tinh-dao-ham">18.3.2.2. Các Quy tắc tính Đạo hàm</a></li>
<li><a class="reference internal" href="#xap-xi-tuyen-tinh">18.3.2.3. Xấp xỉ Tuyến tính</a></li>
<li><a class="reference internal" href="#dao-ham-cap-cao">18.3.2.4. Đạo hàm Cấp cao</a></li>
<li><a class="reference internal" href="#chuoi-taylor">18.3.2.5. Chuỗi Taylor</a></li>
</ul>
</li>
<li><a class="reference internal" href="#tom-tat">18.3.3. Tóm tắt</a></li>
<li><a class="reference internal" href="#bai-tap">18.3.4. Bài tập</a></li>
<li><a class="reference internal" href="#thao-luan">18.3.5. Thảo luận</a></li>
<li><a class="reference internal" href="#nhung-nguoi-thuc-hien">18.3.6. Những người thực hiện</a></li>
</ul>
</li>
</ul>

</div>
            </div>
        </div>

      <div class="clearer"></div>
    </div><div class="pagenation">
     <a id="button-prev" href="eigendecomposition_vn.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="P">
         <i class="pagenation-arrow-L fas fa-arrow-left fa-lg"></i>
         <div class="pagenation-text">
            <span class="pagenation-direction">Previous</span>
            <div>18.2. Phân rã trị riêng</div>
         </div>
     </a>
     <a id="button-next" href="multivariable-calculus_vn.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="N">
         <i class="pagenation-arrow-R fas fa-arrow-right fa-lg"></i>
        <div class="pagenation-text">
            <span class="pagenation-direction">Next</span>
            <div>18.4. Giải tích Nhiều biến</div>
        </div>
     </a>
  </div>
        
        </main>
    </div>
  </body>
</html>