<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    
    <title>18.4. Giải tích Nhiều biến &#8212; Đắm mình vào Học Sâu 0.14.4 documentation</title>

    <link rel="stylesheet" href="../_static/basic.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/d2l.css" />
    <link rel="stylesheet" href="../_static/material-design-lite-1.3.0/material.blue-deep_orange.min.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx_materialdesign_theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fontawesome/all.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fonts.css" type="text/css" />
    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/language_data.js"></script>
    <script src="../_static/d2l.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="shortcut icon" href="../_static/favicon.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="18.5. Giải tích Tích phân" href="integral-calculus_vn.html" />
    <link rel="prev" title="18.3. Giải tích một biến" href="single-variable-calculus_vn.html" /> 
  </head>
<body>
    <div class="mdl-layout mdl-js-layout mdl-layout--fixed-header mdl-layout--fixed-drawer"><header class="mdl-layout__header mdl-layout__header--waterfall ">
    <div class="mdl-layout__header-row">
        
        <nav class="mdl-navigation breadcrumb">
            <a class="mdl-navigation__link" href="index_vn.html"><span class="section-number">18. </span>Phụ lục: Toán học cho Học Sâu</a><i class="material-icons">navigate_next</i>
            <a class="mdl-navigation__link is-active"><span class="section-number">18.4. </span>Giải tích Nhiều biến</a>
        </nav>
        <div class="mdl-layout-spacer"></div>
        <nav class="mdl-navigation">
        
<form class="form-inline pull-sm-right" action="../search.html" method="get">
      <div class="mdl-textfield mdl-js-textfield mdl-textfield--expandable mdl-textfield--floating-label mdl-textfield--align-right">
        <label id="quick-search-icon" class="mdl-button mdl-js-button mdl-button--icon"  for="waterfall-exp">
          <i class="material-icons">search</i>
        </label>
        <div class="mdl-textfield__expandable-holder">
          <input class="mdl-textfield__input" type="text" name="q"  id="waterfall-exp" placeholder="Search" />
          <input type="hidden" name="check_keywords" value="yes" />
          <input type="hidden" name="area" value="default" />
        </div>
      </div>
      <div class="mdl-tooltip" data-mdl-for="quick-search-icon">
      Quick search
      </div>
</form>
        
<a id="button-show-source"
    class="mdl-button mdl-js-button mdl-button--icon"
    href="../_sources/chapter_appendix-mathematics-for-deep-learning/multivariable-calculus_vn.rst.txt" rel="nofollow">
  <i class="material-icons">code</i>
</a>
<div class="mdl-tooltip" data-mdl-for="button-show-source">
Show Source
</div>
        </nav>
    </div>
    <div class="mdl-layout__header-row header-links">
      <div class="mdl-layout-spacer"></div>
      <nav class="mdl-navigation">
          
              <a  class="mdl-navigation__link" href="https://github.com/aivivn/d2l-vn">
                  <i class="fab fa-github"></i>
                  GitHub
              </a>
          
              <a  class="mdl-navigation__link" href="https://forum.machinelearningcoban.com/">
                  <i class="fab fa-discourse"></i>
                  Forum
              </a>
          
              <a  class="mdl-navigation__link" href="https://www.d2l.ai/">
                  <i class="fas fa-external-link-alt"></i>
                  English
              </a>
      </nav>
    </div>
</header><header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <img class="logo" src="../_static/logo-with-text-vi.png" alt="Đắm mình vào Học Sâu"/>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul>
<li class="toctree-l1"><a class="reference internal" href="../intro_vn.html">Giới thiệu từ nhóm dịch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface/index_vn.html">Lời nói đầu</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_install/index_vn.html">Cài đặt</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_notation/index_vn.html">Ký hiệu</a></li>
</ul>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../chapter_introduction/index_vn.html">1. Giới thiệu</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preliminaries/index_vn.html">2. Sơ bộ</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/ndarray_vn.html">2.1. Thao tác với Dữ liệu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/pandas_vn.html">2.2. Tiền xử lý dữ liệu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra_vn.html">2.3. Đại số tuyến tính</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/calculus_vn.html">2.4. Giải tích</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/autograd_vn.html">2.5. Tính vi phân Tự động</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/probability_vn.html">2.6. Xác suất</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/lookup-api_vn.html">2.7. Tài liệu</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_linear-networks/index_vn.html">3. Mạng nơ-ron Tuyến tính</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/linear-regression_vn.html">3.1. Hồi quy Tuyến tính</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/linear-regression-scratch_vn.html">3.2. Lập trình Hồi quy Tuyến tính từ đầu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/linear-regression-gluon_vn.html">3.3. Cách lập trình súc tích Hồi quy Tuyến tính</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/softmax-regression_vn.html">3.4. Hồi quy Softmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/fashion-mnist_vn.html">3.5. Bộ dữ liệu Phân loại Ảnh (Fashion-MNIST)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/softmax-regression-scratch_vn.html">3.6. Lập trình Hồi quy Sofmax từ đầu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/softmax-regression-gluon_vn.html">3.7. Cách lập trình súc tích Hồi quy Softmax</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_multilayer-perceptrons/index_vn.html">4. Perceptron Đa tầng</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/mlp_vn.html">4.1. Perceptron đa tầng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/mlp-scratch_vn.html">4.2. Lập trình Perceptron Đa tầng từ đầu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/mlp-gluon_vn.html">4.3. Cách lập trình súc tích Perceptron Đa tầng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/underfit-overfit_vn.html">4.4. Lựa Chọn Mô Hình, Dưới Khớp và Quá Khớp</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/weight-decay_vn.html">4.5. Suy giảm trọng số</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/dropout_vn.html">4.6. Dropout</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/backprop_vn.html">4.7. Lan truyền xuôi, Lan truyền ngược và Đồ thị tính toán</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/numerical-stability-and-init_vn.html">4.8. Ổn định Số học và Khởi tạo</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/environment_vn.html">4.9. Cân nhắc tới Môi trường</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/kaggle-house-price_vn.html">4.10. Dự đoán Giá Nhà trên Kaggle</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_deep-learning-computation/index_vn.html">5. Tính toán Học sâu</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_deep-learning-computation/model-construction_vn.html">5.1. Tầng và Khối</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_deep-learning-computation/parameters_vn.html">5.2. Quản lý Tham số</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_deep-learning-computation/deferred-init_vn.html">5.3. Khởi tạo trễ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_deep-learning-computation/custom-layer_vn.html">5.4. Các tầng Tuỳ chỉnh</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_deep-learning-computation/read-write_vn.html">5.5. Đọc/Ghi tệp</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_deep-learning-computation/use-gpu_vn.html">5.6. GPU</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_convolutional-neural-networks/index_vn.html">6. Mạng Nơ-ron Tích chập</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/why-conv_vn.html">6.1. Từ Tầng Kết nối Dày đặc đến phép Tích chập</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/conv-layer_vn.html">6.2. Phép Tích chập cho Ảnh</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/padding-and-strides_vn.html">6.3. Đệm và Sải Bước</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/channels_vn.html">6.4. Đa kênh Đầu vào và Đầu ra</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/pooling_vn.html">6.5. Gộp (<em>Pooling</em>)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/lenet_vn.html">6.6. Mạng Nơ-ron Tích chập (LeNet)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_convolutional-modern/index_vn.html">7. Mạng Nơ-ron Tích chập Hiện đại</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/alexnet_vn.html">7.1. Mạng Nơ-ron Tích chập Sâu (AlexNet)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/vgg_vn.html">7.2. Mạng sử dụng Khối (VGG)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/nin_vn.html">7.3. Mạng trong Mạng (<em>Network in Network - NiN</em>)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/googlenet_vn.html">7.4. Mạng nối song song (GoogLeNet)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/batch-norm_vn.html">7.5. Chuẩn hoá theo batch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/resnet_vn.html">7.6. Mạng phần dư (ResNet)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/densenet_vn.html">7.7. Mạng Tích chập Kết nối Dày đặc (DenseNet)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_recurrent-neural-networks/index_vn.html">8. Mạng Nơ-ron Hồi tiếp</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/sequence_vn.html">8.1. Mô hình chuỗi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/text-preprocessing_vn.html">8.2. Tiền Xử lý Dữ liệu Văn bản</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/language-models-and-dataset_vn.html">8.3. Mô hình Ngôn ngữ và Tập dữ liệu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/rnn_vn.html">8.4. Mạng nơ-ron Hồi tiếp</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/rnn-scratch_vn.html">8.5. Lập trình Mạng nơ-ron Hồi tiếp từ đầu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/rnn-gluon_vn.html">8.6. Lập trình súc tích Mạng nơ-ron Hồi tiếp</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/bptt_vn.html">8.7. Lan truyền Ngược qua Thời gian</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_recurrent-modern/index_vn.html">9. Mạng Nơ-ron Hồi tiếp Hiện đại</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/gru_vn.html">9.1. Nút Hồi tiếp có Cổng (GRU)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/lstm_vn.html">9.2. Bộ nhớ Ngắn hạn Dài (LSTM)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/deep-rnn_vn.html">9.3. Mạng Nơ-ron Hồi tiếp Sâu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/bi-rnn_vn.html">9.4. Mạng Nơ-ron Hồi tiếp Hai chiều</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/machine-translation-and-dataset_vn.html">9.5. Dịch Máy và Tập dữ liệu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/encoder-decoder_vn.html">9.6. Kiến trúc Mã hoá - Giải mã</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/seq2seq_vn.html">9.7. Chuỗi sang Chuỗi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/beam-search_vn.html">9.8. Tìm kiếm Chùm</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_attention-mechanisms/index_vn.html">10. Cơ chế Tập trung</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms/attention_vn.html">10.1. Cơ chế Tập trung</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms/seq2seq-attention_vn.html">10.2. Chuỗi sang Chuỗi áp dụng Cơ chế Tập trung</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms/transformer_vn.html">10.3. Kiến trúc Transformer</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_optimization/index_vn.html">11. Thuật toán Tối ưu</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/optimization-intro_vn.html">11.1. Tối ưu và Học sâu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/optimization-intro_vn.html#cac-thach-thuc-cua-toi-uu-trong-hoc-sau">11.2. Các Thách thức của Tối ưu trong Học sâu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/optimization-intro_vn.html#cac-vung-cuc-tieu">11.3. Các vùng Cực tiểu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/optimization-intro_vn.html#cac-diem-yen-ngua">11.4. Các điểm Yên ngựa</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/optimization-intro_vn.html#tieu-bien-gradient">11.5. Tiêu biến Gradient</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/convexity_vn.html">11.6. Tính lồi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/gd_vn.html">11.7. Hạ Gradient</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/sgd_vn.html">11.8. Hạ Gradient Ngẫu nhiên</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/minibatch-sgd_vn.html">11.9. Hạ Gradient Ngẫu nhiên theo Minibatch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/momentum_vn.html">11.10. Động lượng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/adagrad_vn.html">11.11. Adagrad</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/rmsprop_vn.html">11.12. RMSProp</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/adadelta_vn.html">11.13. Adadelta</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/adam_vn.html">11.14. Adam</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/lr-scheduler_vn.html">11.15. Định thời Tốc độ Học</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_computational-performance/index_vn.html">12. Hiệu năng Tính toán</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/hybridize_vn.html">12.1. Trình biên dịch và Trình thông dịch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/async-computation_vn.html">12.2. Tính toán Bất đồng bộ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/auto-parallelism_vn.html">12.3. Song song hóa Tự động</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/hardware_vn.html">12.4. Phần cứng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/multiple-gpus_vn.html">12.5. Huấn luyện đa GPU</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/multiple-gpus-concise_vn.html">12.6. Cách lập trình Súc tích đa GPU</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/parameterserver_vn.html">12.7. Máy chủ Tham số</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_computer-vision/index_vn.html">13. Thị giác Máy tính</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/image-augmentation_vn.html">13.1. Tăng cường Ảnh</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/fine-tuning_vn.html">13.2. Tinh Chỉnh</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/bounding-box_vn.html">13.3. Phát hiện Vật thể và Khoanh vùng Đối tượng (Khung chứa)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/anchor_vn.html">13.4. Khung neo</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/multiscale-object-detection_vn.html">13.5. Phát hiện Vật thể Đa tỷ lệ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/object-detection-dataset_vn.html">13.6. Tập dữ liệu Phát hiện Đối tượng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/ssd_vn.html">13.7. Phát hiện Nhiều khung Một lượt (SSD)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/rcnn_vn.html">13.8. CNN theo Vùng (R-CNN)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/semantic-segmentation-and-dataset_vn.html">13.9. Phân vùng theo Ngữ nghĩa và Tập dữ liệu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/transposed-conv_vn.html">13.10. Tích chập Chuyển vị</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/fcn_vn.html">13.11. Mạng Tích chập Đầy đủ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/neural-style_vn.html">13.12. Truyền tải Phong cách Nơ-ron</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/kaggle-cifar10_vn.html">13.13. Phân loại ảnh (CIFAR-10) trên Kaggle</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/kaggle-dog_vn.html">13.14. Nhận diện Giống Chó (ImageNet Dogs) trên Kaggle</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/index_vn.html">14. Xử lý Ngôn ngữ Tự nhiên: Tiền Huấn luyện</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/word2vec_vn.html">14.1. Embedding Từ (word2vec)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/approx-training_vn.html">14.2. Huấn luyện Gần đúng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/word-embedding-dataset_vn.html">14.3. Tập dữ liệu để Tiền Huấn luyện Embedding Từ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/word2vec-pretraining_vn.html">14.4. Tiền huấn luyện word2vec</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/glove_vn.html">14.5. Embedding từ với Vector Toàn cục (GloVe)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/subword-embedding_vn.html">14.6. Embedding từ con</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/similarity-analogy_vn.html">14.7. Tìm kiếm từ Đồng nghĩa và Loại suy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/bert_vn.html">14.8. Biểu diễn Mã hóa hai chiều từ Transformer (BERT)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/bert-dataset_vn.html">14.9. Tập dữ liệu để Tiền huấn luyện BERT</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/bert-pretraining_vn.html">14.10. Tiền Huấn luyện BERT</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_natural-language-processing-applications/index_vn.html">15. Xử lý Ngôn ngữ Tự nhiên: Ứng dụng</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/sentiment-analysis-and-dataset_vn.html">15.1. Tác vụ Phân tích Cảm xúc và Bộ Dữ liệu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/sentiment-analysis-rnn_vn.html">15.2. Phân tích Cảm xúc: Sử dụng Mạng Nơ-ron Hồi tiếp</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/sentiment-analysis-cnn_vn.html">15.3. Phân tích Cảm xúc: Sử dụng Mạng Nơ-ron Tích Chập</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/natural-language-inference-and-dataset_vn.html">15.4. Suy luận ngôn ngữ tự nhiên và Tập dữ liệu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/natural-language-inference-attention_vn.html">15.5. Suy luận Ngôn ngữ Tự nhiên: Sử dụng Cơ chế Tập trung</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/finetuning-bert_vn.html">15.6. Tinh chỉnh BERT cho các Ứng dụng Cấp Chuỗi và Cấp Token</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/natural-language-inference-bert_vn.html">15.7. Suy luận Ngôn ngữ Tự nhiên: Tinh chỉnh BERT</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_recommender-systems/index_vn.html">16. Hệ thống Đề xuất</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/recsys-intro_vn.html">16.1. Tổng quan về Hệ thống Đề xuất</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/movielens_vn.html">16.2. Tập dữ liệu MovieLens</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/mf_vn.html">16.3. Phân rã Ma trận</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/autorec_vn.html">16.4. AutoRec: Dự đoán Đánh giá với Bộ tự Mã hóa</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/ranking_vn.html">16.5. Cá nhân hóa Xếp hạng trong Hệ thống Đề xuất</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/neumf_vn.html">16.6. Lọc Cộng tác Nơ-ron cho Cá nhân hóa Xếp hạng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/seqrec_vn.html">16.7. Hệ thống Đề xuất có Nhận thức về Chuỗi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/ctr_vn.html">16.8. Hệ thống Đề xuất Giàu Đặc trưng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/fm_vn.html">16.9. Máy Phân rã ma trận</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/deepfm_vn.html">16.10. Máy Phân rã Ma trận Sâu</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_generative-adversarial-networks/index_vn.html">17. Mạng Đối sinh</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_generative-adversarial-networks/gan_vn.html">17.1. Mạng Đối sinh</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_generative-adversarial-networks/dcgan_vn.html">17.2. Mạng Đối sinh Tích chập Sâu</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="index_vn.html">18. Phụ lục: Toán học cho Học Sâu</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="geometry-linear-algebraic-ops_vn.html">18.1. Các phép toán Hình học và Đại số Tuyến tính</a></li>
<li class="toctree-l2"><a class="reference internal" href="eigendecomposition_vn.html">18.2. Phân rã trị riêng</a></li>
<li class="toctree-l2"><a class="reference internal" href="single-variable-calculus_vn.html">18.3. Giải tích một biến</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">18.4. Giải tích Nhiều biến</a></li>
<li class="toctree-l2"><a class="reference internal" href="integral-calculus_vn.html">18.5. Giải tích Tích phân</a></li>
<li class="toctree-l2"><a class="reference internal" href="random-variables_vn.html">18.6. Biến Ngẫu nhiên</a></li>
<li class="toctree-l2"><a class="reference internal" href="maximum-likelihood_vn.html">18.7. Hợp lý Cực đại</a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions_vn.html">18.8. Các Phân phối Xác suất</a></li>
<li class="toctree-l2"><a class="reference internal" href="naive-bayes_vn.html">18.9. Bộ phân loại Naive Bayes</a></li>
<li class="toctree-l2"><a class="reference internal" href="statistics_vn.html">18.10. Thống kê</a></li>
<li class="toctree-l2"><a class="reference internal" href="information-theory_vn.html">18.11. Lý thuyết Thông tin</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/index_vn.html">19. Phụ lục: Công cụ cho Học Sâu</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/jupyter_vn.html">19.1. Sử dụng Jupyter</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/sagemaker_vn.html">19.2. Sử dụng Amazon SageMaker</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/aws_vn.html">19.3. Sử dụng Máy ảo AWS EC2</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/colab_vn.html">19.4. Sử dụng Google Colab</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/selecting-servers-gpus_vn.html">19.5. Lựa chọn Máy chủ &amp; GPU</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/contributing_vn.html">19.6. Đóng góp cho Quyển sách</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/d2l_vn.html">19.7. Tài liệu API của <code class="docutils literal notranslate"><span class="pre">d2l</span></code></a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../chapter_references/zreferences.html">Tài liệu tham khảo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../glossary.html">Bảng thuật ngữ</a></li>
</ul>

            </nav>
        
        </div>
    
</header>
        <main class="mdl-layout__content" tabIndex="0">

	<script type="text/javascript" src="../_static/sphinx_materialdesign_theme.js "></script>
    <header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <img class="logo" src="../_static/logo-with-text-vi.png" alt="Đắm mình vào Học Sâu"/>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul>
<li class="toctree-l1"><a class="reference internal" href="../intro_vn.html">Giới thiệu từ nhóm dịch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface/index_vn.html">Lời nói đầu</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_install/index_vn.html">Cài đặt</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_notation/index_vn.html">Ký hiệu</a></li>
</ul>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../chapter_introduction/index_vn.html">1. Giới thiệu</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preliminaries/index_vn.html">2. Sơ bộ</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/ndarray_vn.html">2.1. Thao tác với Dữ liệu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/pandas_vn.html">2.2. Tiền xử lý dữ liệu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra_vn.html">2.3. Đại số tuyến tính</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/calculus_vn.html">2.4. Giải tích</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/autograd_vn.html">2.5. Tính vi phân Tự động</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/probability_vn.html">2.6. Xác suất</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/lookup-api_vn.html">2.7. Tài liệu</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_linear-networks/index_vn.html">3. Mạng nơ-ron Tuyến tính</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/linear-regression_vn.html">3.1. Hồi quy Tuyến tính</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/linear-regression-scratch_vn.html">3.2. Lập trình Hồi quy Tuyến tính từ đầu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/linear-regression-gluon_vn.html">3.3. Cách lập trình súc tích Hồi quy Tuyến tính</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/softmax-regression_vn.html">3.4. Hồi quy Softmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/fashion-mnist_vn.html">3.5. Bộ dữ liệu Phân loại Ảnh (Fashion-MNIST)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/softmax-regression-scratch_vn.html">3.6. Lập trình Hồi quy Sofmax từ đầu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/softmax-regression-gluon_vn.html">3.7. Cách lập trình súc tích Hồi quy Softmax</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_multilayer-perceptrons/index_vn.html">4. Perceptron Đa tầng</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/mlp_vn.html">4.1. Perceptron đa tầng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/mlp-scratch_vn.html">4.2. Lập trình Perceptron Đa tầng từ đầu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/mlp-gluon_vn.html">4.3. Cách lập trình súc tích Perceptron Đa tầng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/underfit-overfit_vn.html">4.4. Lựa Chọn Mô Hình, Dưới Khớp và Quá Khớp</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/weight-decay_vn.html">4.5. Suy giảm trọng số</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/dropout_vn.html">4.6. Dropout</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/backprop_vn.html">4.7. Lan truyền xuôi, Lan truyền ngược và Đồ thị tính toán</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/numerical-stability-and-init_vn.html">4.8. Ổn định Số học và Khởi tạo</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/environment_vn.html">4.9. Cân nhắc tới Môi trường</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/kaggle-house-price_vn.html">4.10. Dự đoán Giá Nhà trên Kaggle</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_deep-learning-computation/index_vn.html">5. Tính toán Học sâu</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_deep-learning-computation/model-construction_vn.html">5.1. Tầng và Khối</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_deep-learning-computation/parameters_vn.html">5.2. Quản lý Tham số</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_deep-learning-computation/deferred-init_vn.html">5.3. Khởi tạo trễ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_deep-learning-computation/custom-layer_vn.html">5.4. Các tầng Tuỳ chỉnh</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_deep-learning-computation/read-write_vn.html">5.5. Đọc/Ghi tệp</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_deep-learning-computation/use-gpu_vn.html">5.6. GPU</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_convolutional-neural-networks/index_vn.html">6. Mạng Nơ-ron Tích chập</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/why-conv_vn.html">6.1. Từ Tầng Kết nối Dày đặc đến phép Tích chập</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/conv-layer_vn.html">6.2. Phép Tích chập cho Ảnh</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/padding-and-strides_vn.html">6.3. Đệm và Sải Bước</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/channels_vn.html">6.4. Đa kênh Đầu vào và Đầu ra</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/pooling_vn.html">6.5. Gộp (<em>Pooling</em>)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/lenet_vn.html">6.6. Mạng Nơ-ron Tích chập (LeNet)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_convolutional-modern/index_vn.html">7. Mạng Nơ-ron Tích chập Hiện đại</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/alexnet_vn.html">7.1. Mạng Nơ-ron Tích chập Sâu (AlexNet)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/vgg_vn.html">7.2. Mạng sử dụng Khối (VGG)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/nin_vn.html">7.3. Mạng trong Mạng (<em>Network in Network - NiN</em>)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/googlenet_vn.html">7.4. Mạng nối song song (GoogLeNet)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/batch-norm_vn.html">7.5. Chuẩn hoá theo batch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/resnet_vn.html">7.6. Mạng phần dư (ResNet)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/densenet_vn.html">7.7. Mạng Tích chập Kết nối Dày đặc (DenseNet)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_recurrent-neural-networks/index_vn.html">8. Mạng Nơ-ron Hồi tiếp</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/sequence_vn.html">8.1. Mô hình chuỗi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/text-preprocessing_vn.html">8.2. Tiền Xử lý Dữ liệu Văn bản</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/language-models-and-dataset_vn.html">8.3. Mô hình Ngôn ngữ và Tập dữ liệu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/rnn_vn.html">8.4. Mạng nơ-ron Hồi tiếp</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/rnn-scratch_vn.html">8.5. Lập trình Mạng nơ-ron Hồi tiếp từ đầu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/rnn-gluon_vn.html">8.6. Lập trình súc tích Mạng nơ-ron Hồi tiếp</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/bptt_vn.html">8.7. Lan truyền Ngược qua Thời gian</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_recurrent-modern/index_vn.html">9. Mạng Nơ-ron Hồi tiếp Hiện đại</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/gru_vn.html">9.1. Nút Hồi tiếp có Cổng (GRU)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/lstm_vn.html">9.2. Bộ nhớ Ngắn hạn Dài (LSTM)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/deep-rnn_vn.html">9.3. Mạng Nơ-ron Hồi tiếp Sâu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/bi-rnn_vn.html">9.4. Mạng Nơ-ron Hồi tiếp Hai chiều</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/machine-translation-and-dataset_vn.html">9.5. Dịch Máy và Tập dữ liệu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/encoder-decoder_vn.html">9.6. Kiến trúc Mã hoá - Giải mã</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/seq2seq_vn.html">9.7. Chuỗi sang Chuỗi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/beam-search_vn.html">9.8. Tìm kiếm Chùm</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_attention-mechanisms/index_vn.html">10. Cơ chế Tập trung</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms/attention_vn.html">10.1. Cơ chế Tập trung</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms/seq2seq-attention_vn.html">10.2. Chuỗi sang Chuỗi áp dụng Cơ chế Tập trung</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms/transformer_vn.html">10.3. Kiến trúc Transformer</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_optimization/index_vn.html">11. Thuật toán Tối ưu</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/optimization-intro_vn.html">11.1. Tối ưu và Học sâu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/optimization-intro_vn.html#cac-thach-thuc-cua-toi-uu-trong-hoc-sau">11.2. Các Thách thức của Tối ưu trong Học sâu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/optimization-intro_vn.html#cac-vung-cuc-tieu">11.3. Các vùng Cực tiểu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/optimization-intro_vn.html#cac-diem-yen-ngua">11.4. Các điểm Yên ngựa</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/optimization-intro_vn.html#tieu-bien-gradient">11.5. Tiêu biến Gradient</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/convexity_vn.html">11.6. Tính lồi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/gd_vn.html">11.7. Hạ Gradient</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/sgd_vn.html">11.8. Hạ Gradient Ngẫu nhiên</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/minibatch-sgd_vn.html">11.9. Hạ Gradient Ngẫu nhiên theo Minibatch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/momentum_vn.html">11.10. Động lượng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/adagrad_vn.html">11.11. Adagrad</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/rmsprop_vn.html">11.12. RMSProp</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/adadelta_vn.html">11.13. Adadelta</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/adam_vn.html">11.14. Adam</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/lr-scheduler_vn.html">11.15. Định thời Tốc độ Học</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_computational-performance/index_vn.html">12. Hiệu năng Tính toán</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/hybridize_vn.html">12.1. Trình biên dịch và Trình thông dịch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/async-computation_vn.html">12.2. Tính toán Bất đồng bộ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/auto-parallelism_vn.html">12.3. Song song hóa Tự động</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/hardware_vn.html">12.4. Phần cứng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/multiple-gpus_vn.html">12.5. Huấn luyện đa GPU</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/multiple-gpus-concise_vn.html">12.6. Cách lập trình Súc tích đa GPU</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/parameterserver_vn.html">12.7. Máy chủ Tham số</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_computer-vision/index_vn.html">13. Thị giác Máy tính</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/image-augmentation_vn.html">13.1. Tăng cường Ảnh</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/fine-tuning_vn.html">13.2. Tinh Chỉnh</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/bounding-box_vn.html">13.3. Phát hiện Vật thể và Khoanh vùng Đối tượng (Khung chứa)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/anchor_vn.html">13.4. Khung neo</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/multiscale-object-detection_vn.html">13.5. Phát hiện Vật thể Đa tỷ lệ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/object-detection-dataset_vn.html">13.6. Tập dữ liệu Phát hiện Đối tượng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/ssd_vn.html">13.7. Phát hiện Nhiều khung Một lượt (SSD)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/rcnn_vn.html">13.8. CNN theo Vùng (R-CNN)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/semantic-segmentation-and-dataset_vn.html">13.9. Phân vùng theo Ngữ nghĩa và Tập dữ liệu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/transposed-conv_vn.html">13.10. Tích chập Chuyển vị</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/fcn_vn.html">13.11. Mạng Tích chập Đầy đủ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/neural-style_vn.html">13.12. Truyền tải Phong cách Nơ-ron</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/kaggle-cifar10_vn.html">13.13. Phân loại ảnh (CIFAR-10) trên Kaggle</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/kaggle-dog_vn.html">13.14. Nhận diện Giống Chó (ImageNet Dogs) trên Kaggle</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/index_vn.html">14. Xử lý Ngôn ngữ Tự nhiên: Tiền Huấn luyện</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/word2vec_vn.html">14.1. Embedding Từ (word2vec)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/approx-training_vn.html">14.2. Huấn luyện Gần đúng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/word-embedding-dataset_vn.html">14.3. Tập dữ liệu để Tiền Huấn luyện Embedding Từ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/word2vec-pretraining_vn.html">14.4. Tiền huấn luyện word2vec</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/glove_vn.html">14.5. Embedding từ với Vector Toàn cục (GloVe)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/subword-embedding_vn.html">14.6. Embedding từ con</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/similarity-analogy_vn.html">14.7. Tìm kiếm từ Đồng nghĩa và Loại suy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/bert_vn.html">14.8. Biểu diễn Mã hóa hai chiều từ Transformer (BERT)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/bert-dataset_vn.html">14.9. Tập dữ liệu để Tiền huấn luyện BERT</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/bert-pretraining_vn.html">14.10. Tiền Huấn luyện BERT</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_natural-language-processing-applications/index_vn.html">15. Xử lý Ngôn ngữ Tự nhiên: Ứng dụng</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/sentiment-analysis-and-dataset_vn.html">15.1. Tác vụ Phân tích Cảm xúc và Bộ Dữ liệu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/sentiment-analysis-rnn_vn.html">15.2. Phân tích Cảm xúc: Sử dụng Mạng Nơ-ron Hồi tiếp</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/sentiment-analysis-cnn_vn.html">15.3. Phân tích Cảm xúc: Sử dụng Mạng Nơ-ron Tích Chập</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/natural-language-inference-and-dataset_vn.html">15.4. Suy luận ngôn ngữ tự nhiên và Tập dữ liệu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/natural-language-inference-attention_vn.html">15.5. Suy luận Ngôn ngữ Tự nhiên: Sử dụng Cơ chế Tập trung</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/finetuning-bert_vn.html">15.6. Tinh chỉnh BERT cho các Ứng dụng Cấp Chuỗi và Cấp Token</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/natural-language-inference-bert_vn.html">15.7. Suy luận Ngôn ngữ Tự nhiên: Tinh chỉnh BERT</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_recommender-systems/index_vn.html">16. Hệ thống Đề xuất</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/recsys-intro_vn.html">16.1. Tổng quan về Hệ thống Đề xuất</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/movielens_vn.html">16.2. Tập dữ liệu MovieLens</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/mf_vn.html">16.3. Phân rã Ma trận</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/autorec_vn.html">16.4. AutoRec: Dự đoán Đánh giá với Bộ tự Mã hóa</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/ranking_vn.html">16.5. Cá nhân hóa Xếp hạng trong Hệ thống Đề xuất</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/neumf_vn.html">16.6. Lọc Cộng tác Nơ-ron cho Cá nhân hóa Xếp hạng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/seqrec_vn.html">16.7. Hệ thống Đề xuất có Nhận thức về Chuỗi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/ctr_vn.html">16.8. Hệ thống Đề xuất Giàu Đặc trưng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/fm_vn.html">16.9. Máy Phân rã ma trận</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/deepfm_vn.html">16.10. Máy Phân rã Ma trận Sâu</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_generative-adversarial-networks/index_vn.html">17. Mạng Đối sinh</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_generative-adversarial-networks/gan_vn.html">17.1. Mạng Đối sinh</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_generative-adversarial-networks/dcgan_vn.html">17.2. Mạng Đối sinh Tích chập Sâu</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="index_vn.html">18. Phụ lục: Toán học cho Học Sâu</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="geometry-linear-algebraic-ops_vn.html">18.1. Các phép toán Hình học và Đại số Tuyến tính</a></li>
<li class="toctree-l2"><a class="reference internal" href="eigendecomposition_vn.html">18.2. Phân rã trị riêng</a></li>
<li class="toctree-l2"><a class="reference internal" href="single-variable-calculus_vn.html">18.3. Giải tích một biến</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">18.4. Giải tích Nhiều biến</a></li>
<li class="toctree-l2"><a class="reference internal" href="integral-calculus_vn.html">18.5. Giải tích Tích phân</a></li>
<li class="toctree-l2"><a class="reference internal" href="random-variables_vn.html">18.6. Biến Ngẫu nhiên</a></li>
<li class="toctree-l2"><a class="reference internal" href="maximum-likelihood_vn.html">18.7. Hợp lý Cực đại</a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions_vn.html">18.8. Các Phân phối Xác suất</a></li>
<li class="toctree-l2"><a class="reference internal" href="naive-bayes_vn.html">18.9. Bộ phân loại Naive Bayes</a></li>
<li class="toctree-l2"><a class="reference internal" href="statistics_vn.html">18.10. Thống kê</a></li>
<li class="toctree-l2"><a class="reference internal" href="information-theory_vn.html">18.11. Lý thuyết Thông tin</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/index_vn.html">19. Phụ lục: Công cụ cho Học Sâu</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/jupyter_vn.html">19.1. Sử dụng Jupyter</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/sagemaker_vn.html">19.2. Sử dụng Amazon SageMaker</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/aws_vn.html">19.3. Sử dụng Máy ảo AWS EC2</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/colab_vn.html">19.4. Sử dụng Google Colab</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/selecting-servers-gpus_vn.html">19.5. Lựa chọn Máy chủ &amp; GPU</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/contributing_vn.html">19.6. Đóng góp cho Quyển sách</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/d2l_vn.html">19.7. Tài liệu API của <code class="docutils literal notranslate"><span class="pre">d2l</span></code></a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../chapter_references/zreferences.html">Tài liệu tham khảo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../glossary.html">Bảng thuật ngữ</a></li>
</ul>

            </nav>
        
        </div>
    
</header>

    <div class="document">
        <div class="page-content" role="main">
        
  <!--
# Multivariable Calculus
--><div class="section" id="giai-tich-nhieu-bien">
<span id="sec-multivariable-calculus"></span><h1><span class="section-number">18.4. </span>Giải tích Nhiều biến<a class="headerlink" href="#giai-tich-nhieu-bien" title="Permalink to this headline">¶</a></h1>
<!--
Now that we have a fairly strong understanding of derivatives of a function of a single variable,
let us return to our original question where we were considering a loss function of potentially billions of weights.
--><p>Bây giờ chúng ta đã có hiểu biết vững chắc về đạo hàm của một hàm đơn
biến, hãy cùng trở lại câu hỏi ban đầu về hàm mất mát của (nhiều khả
năng là) hàng tỷ trọng số.</p>
<!--
## Higher-Dimensional Differentiation
--><div class="section" id="dao-ham-trong-khong-gian-nhieu-chieu">
<h2><span class="section-number">18.4.1. </span>Đạo hàm trong Không gian Nhiều chiều<a class="headerlink" href="#dao-ham-trong-khong-gian-nhieu-chieu" title="Permalink to this headline">¶</a></h2>
<!--
What :numref:`sec_single_variable_calculus` tells us is that if we change a single one of these billions of weights leaving every other one fixed, we know what will happen!
This is nothing more than a function of a single variable, so we can write
--><p>Nhớ lại <a class="reference internal" href="single-variable-calculus_vn.html#sec-single-variable-calculus"><span class="std std-numref">Section 18.3</span></a>, ta đã bàn luận về điều
gì sẽ xảy ra nếu chỉ thay đổi một trong số hàng tỷ các trọng số và giữ
nguyên những trọng số còn lại. Điều này hoàn toàn không có gì khác với
một hàm đơn biến, nên ta có thể viết</p>
<div class="math notranslate nohighlight" id="equation-eq-part-der">
<span class="eqno">(18.4.1)<a class="headerlink" href="#equation-eq-part-der" title="Permalink to this equation">¶</a></span>\[L(w_1+\epsilon_1, w_2, \ldots, w_N) \approx L(w_1, w_2, \ldots, w_N) + \epsilon_1 \frac{d}{dw_1} L(w_1, w_2, \ldots, w_N).\]</div>
<!--
We will call the derivative in one variable while fixing the other the *partial derivative*,
and we will use the notation $\frac{\partial}{\partial w_1}$ for the derivative in :eqref:`eq_part_der`.
--><p>Chúng ta sẽ gọi đạo hàm của một biến trong khi không thay đổi những biến
còn lại là <em>đạo hàm riêng</em> (<em>partial derivative</em>), và ký hiệu đạo hàm
này là <span class="math notranslate nohighlight">\(\frac{\partial}{\partial w_1}\)</span> trong phương trình
<a class="reference internal" href="#equation-eq-part-der">(18.4.1)</a>.</p>
<!--
Now, let us take this and change $w_2$ a little bit to $w_2 + \epsilon_2$:
--><p>Bây giờ, tiếp tục thay đổi <span class="math notranslate nohighlight">\(w_2\)</span> một khoảng nhỏ thành
<span class="math notranslate nohighlight">\(w_2 + \epsilon_2\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-0">
<span class="eqno">(18.4.2)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-0" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{aligned}
L(w_1+\epsilon_1, w_2+\epsilon_2, \ldots, w_N) &amp; \approx L(w_1, w_2+\epsilon_2, \ldots, w_N) + \epsilon_1 \frac{\partial}{\partial w_1} L(w_1, w_2+\epsilon_2, \ldots, w_N) \\
&amp; \approx L(w_1, w_2, \ldots, w_N) \\
&amp; \quad + \epsilon_2\frac{\partial}{\partial w_2} L(w_1, w_2, \ldots, w_N) \\
&amp; \quad + \epsilon_1 \frac{\partial}{\partial w_1} L(w_1, w_2, \ldots, w_N) \\
&amp; \quad + \epsilon_1\epsilon_2\frac{\partial}{\partial w_2}\frac{\partial}{\partial w_1} L(w_1, w_2, \ldots, w_N) \\
&amp; \approx L(w_1, w_2, \ldots, w_N) \\
&amp; \quad + \epsilon_2\frac{\partial}{\partial w_2} L(w_1, w_2, \ldots, w_N) \\
&amp; \quad + \epsilon_1 \frac{\partial}{\partial w_1} L(w_1, w_2, \ldots, w_N).
\end{aligned}\end{split}\]</div>
<!--
We have again used the idea that $\epsilon_1\epsilon_2$ is a higher order term that we can discard in
the same way we could discard $\epsilon^{2}$ in the previous section, along with what we saw in :eqref:`eq_part_der`.
By continuing in this manner, we may write that
--><p>Một lần nữa, ta lại sử dụng ý tưởng đã thấy ở <a class="reference internal" href="#equation-eq-part-der">(18.4.1)</a>
rằng <span class="math notranslate nohighlight">\(\epsilon_1\epsilon_2\)</span> là một số hạng bậc cao và có thể được
loại bỏ tương tự như cách mà ta có thể loại bỏ <span class="math notranslate nohighlight">\(\epsilon^{2}\)</span>
trong mục trước. Cứ tiếp tục theo cách này, ta có</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-1">
<span class="eqno">(18.4.3)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-1" title="Permalink to this equation">¶</a></span>\[L(w_1+\epsilon_1, w_2+\epsilon_2, \ldots, w_N+\epsilon_N) \approx L(w_1, w_2, \ldots, w_N) + \sum_i \epsilon_i \frac{\partial}{\partial w_i} L(w_1, w_2, \ldots, w_N).\]</div>
<!--
This may look like a mess, but we can make this more familiar by noting that the sum on the right looks exactly like a dot product, so if we let
--><p>Thoạt nhìn đây có vẻ là một mớ hỗn độn, tuy nhiên chú ý rằng phép tổng
bên phải chính là biểu diễn của phép tích vô hướng và ta có thể khiến
chúng trở nên quen thuộc hơn. Với</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-2">
<span class="eqno">(18.4.4)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-2" title="Permalink to this equation">¶</a></span>\[\boldsymbol{\epsilon} = [\epsilon_1, \ldots, \epsilon_N]^\top \; \text{và} \;
\nabla_{\mathbf{x}} L = \left[\frac{\partial L}{\partial x_1}, \ldots, \frac{\partial L}{\partial x_N}\right]^\top,\]</div>
<!--
then
--><p>ta có</p>
<div class="math notranslate nohighlight" id="equation-eq-nabla-use">
<span class="eqno">(18.4.5)<a class="headerlink" href="#equation-eq-nabla-use" title="Permalink to this equation">¶</a></span>\[L(\mathbf{w} + \boldsymbol{\epsilon}) \approx L(\mathbf{w}) + \boldsymbol{\epsilon}\cdot \nabla_{\mathbf{w}} L(\mathbf{w}).\]</div>
<!--
We will call the vector $\nabla_{\mathbf{w}} L$ the *gradient* of $L$.
--><p>Ta gọi vector <span class="math notranslate nohighlight">\(\nabla_{\mathbf{w}} L\)</span> là <em>gradient</em> của <span class="math notranslate nohighlight">\(L\)</span>.</p>
<!--
Equation :eqref:`eq_nabla_use` is worth pondering for a moment.
It has exactly the format that we encountered in one dimension, just we have converted everything to vectors and dot products.
It allows us to tell approximately how the function $L$ will change given any perturbation to the input.
As we will see in the next section, this will provide us with an important tool in understanding geometrically how we can learn using information contained in the gradient.
--><p>Phương trình <a class="reference internal" href="#equation-eq-nabla-use">(18.4.5)</a> đáng để ta suy ngẫm. Nó có dạng
đúng y như những gì ta đã thấy trong trường hợp một chiều, chỉ khác là
tất cả đã được biến đổi về dạng vector và tích vô hướng. Điều này cho
chúng ta biết một cách xấp xỉ hàm <span class="math notranslate nohighlight">\(L\)</span> sẽ thay đổi như thế nào với
một nhiễu loạn bất kỳ ở đầu vào. Như ta sẽ thấy trong mục tiếp theo, đây
sẽ là một công cụ quan trọng giúp chúng ta hiểu được cách học từ thông
tin chứa trong gradient dưới góc nhìn hình học.</p>
<!--
But first, let us see this approximation at work with an example.
Suppose that we are working with the function
--><p>Nhưng trước tiên, hãy cùng kiểm tra phép xấp xỉ này với một ví dụ. Giả
sử ta đang làm việc với hàm</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-3">
<span class="eqno">(18.4.6)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-3" title="Permalink to this equation">¶</a></span>\[f(x, y) = \log(e^x + e^y) \text{ với gradient } \nabla f (x, y) = \left[\frac{e^x}{e^x+e^y}, \frac{e^y}{e^x+e^y}\right].\]</div>
<!--
If we look at a point like $(0, \log(2))$, we see that
--><p>Xét một điểm <span class="math notranslate nohighlight">\((0, \log(2))\)</span>, ta có</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-4">
<span class="eqno">(18.4.7)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-4" title="Permalink to this equation">¶</a></span>\[f(x, y) = \log(3) \text{ với gradient } \nabla f (x, y) = \left[\frac{1}{3}, \frac{2}{3}\right].\]</div>
<!--
Thus, if we want to approximate $f$ at $(\epsilon_1, \log(2) + \epsilon_2)$,
we see that we should have the specific instance of :eqref:`eq_nabla_use`:
--><p>Vì thế, nếu muốn tính xấp xỉ <span class="math notranslate nohighlight">\(f\)</span> tại
<span class="math notranslate nohighlight">\((\epsilon_1, \log(2) + \epsilon_2)\)</span>, ta có một ví dụ cụ thể của
<a class="reference internal" href="#equation-eq-nabla-use">(18.4.5)</a>:</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-5">
<span class="eqno">(18.4.8)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-5" title="Permalink to this equation">¶</a></span>\[f(\epsilon_1, \log(2) + \epsilon_2) \approx \log(3) + \frac{1}{3}\epsilon_1 + \frac{2}{3}\epsilon_2.\]</div>
<!--
We can test this in code to see how good the approximation is.
--><p>Ta có thể kiểm tra với đoạn mã bên dưới để xem phép xấp xỉ chính xác tới
mức nào.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="kn">from</span> <span class="nn">d2l</span> <span class="kn">import</span> <span class="n">mxnet</span> <span class="k">as</span> <span class="n">d2l</span>
<span class="kn">from</span> <span class="nn">IPython</span> <span class="kn">import</span> <span class="n">display</span>
<span class="kn">from</span> <span class="nn">mpl_toolkits</span> <span class="kn">import</span> <span class="n">mplot3d</span>
<span class="kn">from</span> <span class="nn">mxnet</span> <span class="kn">import</span> <span class="n">autograd</span><span class="p">,</span> <span class="n">np</span><span class="p">,</span> <span class="n">npx</span>
<span class="n">npx</span><span class="o">.</span><span class="n">set_np</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
<span class="k">def</span> <span class="nf">grad_f</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">y</span><span class="p">)),</span>
                     <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">y</span><span class="p">))])</span>

<span class="n">epsilon</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.01</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.03</span><span class="p">])</span>
<span class="n">grad_approx</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span> <span class="o">+</span> <span class="n">epsilon</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">grad_f</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">2</span><span class="p">)))</span>
<span class="n">true_value</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="mi">0</span> <span class="o">+</span> <span class="n">epsilon</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="n">epsilon</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="sa">f</span><span class="s1">&#39;approximation: </span><span class="si">{</span><span class="n">grad_approx</span><span class="si">}</span><span class="s1">, true Value: </span><span class="si">{</span><span class="n">true_value</span><span class="si">}</span><span class="s1">&#39;</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="s1">&#39;approximation: 1.0819457, true Value: 1.0821242&#39;</span>
</pre></div>
</div>
<!--
## Geometry of Gradients and Gradient Descent
--></div>
<div class="section" id="y-nghia-hinh-hoc-cua-gradient-va-thuat-toan-ha-gradient">
<h2><span class="section-number">18.4.2. </span>Ý nghĩa Hình học của Gradient và Thuật toán Hạ Gradient<a class="headerlink" href="#y-nghia-hinh-hoc-cua-gradient-va-thuat-toan-ha-gradient" title="Permalink to this headline">¶</a></h2>
<!--
Consider the again :eqref:`eq_nabla_use`:
--><p>Nhìn lại <a class="reference internal" href="#equation-eq-nabla-use">(18.4.5)</a>:</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-6">
<span class="eqno">(18.4.9)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-6" title="Permalink to this equation">¶</a></span>\[L(\mathbf{w} + \boldsymbol{\epsilon}) \approx L(\mathbf{w}) + \boldsymbol{\epsilon}\cdot \nabla_{\mathbf{w}} L(\mathbf{w}).\]</div>
<!--
Let us suppose that I want to use this to help minimize our loss $L$.
Let us understand geometrically the algorithm of gradient descent first described in :numref:`sec_autograd`.
What we will do is the following:
--><p>Giả sử ta muốn sử dụng thông tin gradient để cực tiểu hóa mất mát
<span class="math notranslate nohighlight">\(L\)</span>. Hãy cùng tìm hiểu cách hoạt động về mặt hình học của thuật
toán hạ gradient được mô tả lần đầu ở <a class="reference internal" href="../chapter_preliminaries/autograd_vn.html#sec-autograd"><span class="std std-numref">Section 2.5</span></a>. Các bước
của thuật toán được miêu tả dưới đây:</p>
<!--
1. Start with a random choice for the initial parameters $\mathbf{w}$.
2. Find the direction $\mathbf{v}$ that makes $L$ decrease the most rapidly at $\mathbf{w}$.
3. Take a small step in that direction: $\mathbf{w} \rightarrow \mathbf{w} + \epsilon\mathbf{v}$.
4. Repeat.
--><ol class="arabic simple">
<li>Bắt đầu với giá trị ban đầu ngẫu nhiên của tham số
<span class="math notranslate nohighlight">\(\mathbf{w}\)</span>.</li>
<li>Tìm một hướng <span class="math notranslate nohighlight">\(\mathbf{v}\)</span> tại <span class="math notranslate nohighlight">\(\mathbf{w}\)</span> sao cho
<span class="math notranslate nohighlight">\(L\)</span> giảm một cách nhanh nhất.</li>
<li>Tiến một bước nhỏ về hướng đó:
<span class="math notranslate nohighlight">\(\mathbf{w} \rightarrow \mathbf{w} + \epsilon\mathbf{v}\)</span>.</li>
<li>Lặp lại.</li>
</ol>
<!--
The only thing we do not know exactly how to do is to compute the vector $\mathbf{v}$ in the second step.
We will call such a direction the *direction of steepest descent*.
Using the geometric understanding of dot products from :numref:`sec_geometry-linear-algebraic-ops`, we see that we can rewrite :eqref:`eq_nabla_use` as
--><p>Thứ duy nhất mà chúng ta không biết chính xác cách làm là cách tính toán
vector <span class="math notranslate nohighlight">\(\mathbf{v}\)</span> tại bước thứ hai. Ta gọi <span class="math notranslate nohighlight">\(\mathbf{v}\)</span> là
<em>hướng hạ dốc nhất</em> (<em>direction of steepest descent</em>). Sử dụng những
hiểu biết về mặt hình học của phép tích vô hướng từ
<a class="reference internal" href="geometry-linear-algebraic-ops_vn.html#sec-geometry-linear-algebraic-ops"><span class="std std-numref">Section 18.1</span></a>, ta có thể viết lại
<a class="reference internal" href="#equation-eq-nabla-use">(18.4.5)</a> như sau</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-7">
<span class="eqno">(18.4.10)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-7" title="Permalink to this equation">¶</a></span>\[L(\mathbf{w} + \mathbf{v}) \approx L(\mathbf{w}) + \mathbf{v}\cdot \nabla_{\mathbf{w}} L(\mathbf{w}) = L(\mathbf{w}) + \|\nabla_{\mathbf{w}} L(\mathbf{w})\|\cos(\theta).\]</div>
<!--
Note that we have taken our direction to have length one for convenience, and used $\theta$ for the angle between $\mathbf{v}$ and $\nabla_{\mathbf{w}} L(\mathbf{w})$.
If we want to find the direction that decreases $L$ as rapidly as possible, we want to make this expression as negative as possible.
The only way the direction we pick enters into this equation is through $\cos(\theta)$, and thus we wish to make this cosine as negative as possible.
Now, recalling the shape of cosine, we can make this as negative as possible by making $\cos(\theta) = -1$ or equivalently
making the angle between the gradient and our chosen direction to be $\pi$ radians, or equivalently $180$ degrees.
The only way to achieve this is to head in the exact opposite direction:
pick $\mathbf{v}$ to point in the exact opposite direction to $\nabla_{\mathbf{w}} L(\mathbf{w})$!
--><p>Để thuận tiện, ta giả định hướng của chúng ta có độ dài bằng một và sử
dụng <span class="math notranslate nohighlight">\(\theta\)</span> để biểu diễn góc giữa <span class="math notranslate nohighlight">\(\mathbf{v}\)</span> và
<span class="math notranslate nohighlight">\(\nabla_{\mathbf{w}} L(\mathbf{w})\)</span>. Nếu muốn <span class="math notranslate nohighlight">\(L\)</span> giảm càng
nhanh, ta sẽ muốn giá trị của biểu thức trên càng âm càng tốt. Cách duy
nhất để chọn hướng đi trong phương trình này là thông qua
<span class="math notranslate nohighlight">\(\cos(\theta)\)</span>, vì thế ta sẽ muốn giá trị này âm nhất có thể. Nhắc
lại kiến thức của hàm cô-sin, giá trị âm nhất của hàm này là
<span class="math notranslate nohighlight">\(\cos(\theta) = -1\)</span>, là khi góc giữa vector gradient và hướng cần
chọn là <span class="math notranslate nohighlight">\(\pi\)</span> radian hay <span class="math notranslate nohighlight">\(180\)</span> độ. Cách duy nhất để đạt được
điều này là di chuyển theo hướng hoàn toàn ngược lại: chọn
<span class="math notranslate nohighlight">\(\mathbf{v}\)</span> theo hướng hoàn toàn ngược chiều với
<span class="math notranslate nohighlight">\(\nabla_{\mathbf{w}} L(\mathbf{w})\)</span>!</p>
<!--
This brings us to one of the most important mathematical concepts in machine learning:
the direction of steepest decent points in the direction of $-\nabla_{\mathbf{w}}L(\mathbf{w})$.
Thus our informal algorithm can be rewritten as follows.
--><p>Điều này dẫn ta đến với một trong những thuật toán quan trọng nhất của
học máy: hướng hạ dốc nhất cùng hướng với
<span class="math notranslate nohighlight">\(-\nabla_{\mathbf{w}}L(\mathbf{w})\)</span>. Vậy nên thuật toán của ta sẽ
được viết lại như sau.</p>
<!--
1. Start with a random choice for the initial parameters $\mathbf{w}$.
2. Compute $\nabla_{\mathbf{w}} L(\mathbf{w})$.
3. Take a small step in the opposite of that direction: $\mathbf{w} \rightarrow \mathbf{w} - \epsilon\nabla_{\mathbf{w}} L(\mathbf{w})$.
4. Repeat.
--><ol class="arabic simple">
<li>Bắt đầu với một lựa chọn ngẫu nhiên cho giá trị ban đầu của các tham
số <span class="math notranslate nohighlight">\(\mathbf{w}\)</span>.</li>
<li>Tính toán <span class="math notranslate nohighlight">\(\nabla_{\mathbf{w}} L(\mathbf{w})\)</span>.</li>
<li>Tiến một bước nhỏ về hướng ngược lại của nó:
<span class="math notranslate nohighlight">\(\mathbf{w} \rightarrow \mathbf{w} - \epsilon\nabla_{\mathbf{w}} L(\mathbf{w})\)</span>.</li>
<li>Lặp lại.</li>
</ol>
<!--
This basic algorithm has been modified and adapted many ways by many researchers, but the core concept remains the same in all of them.
Use the gradient to find the direction that decreases the loss as rapidly as possible, and update the parameters to take a step in that direction.
--><p>Thuật toán cơ bản này dù đã được chỉnh sửa và kết hợp theo nhiều cách
bởi các nhà nghiên cứu, nhưng khái niệm cốt lõi vẫn là như nhau. Sử dụng
gradient để tìm hướng giảm mất mát nhanh nhất có thể và cập nhật các
tham số để dịch chuyển về hướng đó.</p>
<!--
## A Note on Mathematical Optimization
--></div>
<div class="section" id="mot-vai-chu-y-ve-toi-uu-hoa">
<h2><span class="section-number">18.4.3. </span>Một vài chú ý về Tối ưu hóa<a class="headerlink" href="#mot-vai-chu-y-ve-toi-uu-hoa" title="Permalink to this headline">¶</a></h2>
<!--
Throughout this book, we focus squarely on numerical optimization techniques for the practical reason that all functions
we encounter in the deep learning setting are too complex to minimize explicitly.
--><p>Xuyên suốt cuốn sách, ta chỉ tập trung vào những kỹ thuật tối ưu hóa số
học vì một nguyên nhân thực tế là: mọi hàm ta gặp phải trong học sâu quá
phức tạp để có thể tối ưu hóa một cách tường minh.</p>
<!--
However, it is a useful exercise to consider what the geometric understanding we obtained above tells us about optimizing functions directly.
--><p>Tuy nhiên, sẽ rất hữu ích nếu hiểu được những kiến thức hình học ta có
được ở trên nói gì về tối ưu hóa các hàm một cách trực tiếp.</p>
<!--
Suppose that we wish to find the value of $\mathbf{x}_0$ which minimizes some function $L(\mathbf{x})$.
Let us suppose that moreover someone gives us a value and tells us that it is the value that minimizes $L$.
Is there anything we can check to see if their answer is even plausible?
--><p>Giả sử ta muốn tìm giá trị của <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span> giúp cực tiểu hóa
một hàm <span class="math notranslate nohighlight">\(L(\mathbf{x})\)</span> nào đó. Và có một người nào đó đưa ta một
giá trị và cho rằng đây là giá trị giúp cực tiểu hóa <span class="math notranslate nohighlight">\(L\)</span>. Bằng
cách nào ta có thể kiểm chứng rằng đáp án của họ là hợp lý?</p>
<!--
Again consider :eqref:`eq_nabla_use`:
--><p>Xét lại <a class="reference internal" href="#equation-eq-nabla-use">(18.4.5)</a>:</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-8">
<span class="eqno">(18.4.11)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-8" title="Permalink to this equation">¶</a></span>\[L(\mathbf{x}_0 + \boldsymbol{\epsilon}) \approx L(\mathbf{x}_0) + \boldsymbol{\epsilon}\cdot \nabla_{\mathbf{x}} L(\mathbf{x}_0).\]</div>
<!--
If the gradient is not zero, we know that we can take a step in the direction $-\epsilon \nabla_{\mathbf{x}} L(\mathbf{x}_0)$ to find a value of $L$ that is smaller.
Thus, if we truly are at a minimum, this cannot be the case!
We can conclude that if $\mathbf{x}_0$ is a minimum, then $\nabla_{\mathbf{x}} L(\mathbf{x}_0) = 0$.
We call points with $\nabla_{\mathbf{x}} L(\mathbf{x}_0) = 0$ *critical points*.
--><p>Nếu giá trị gradient khác không, ta biết rằng ta có thể bước một bước về
hướng <span class="math notranslate nohighlight">\(-\epsilon \nabla_{\mathbf{x}} L(\mathbf{x}_0)\)</span> để tìm một
giá trị <span class="math notranslate nohighlight">\(L\)</span> nhỏ hơn. Do đó, nếu ta thực sự ở điểm cực tiểu, sẽ
không thể có trường hợp đó! Ta có thể kết luận rằng nếu
<span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span> là một cực tiểu, thì
<span class="math notranslate nohighlight">\(\nabla_{\mathbf{x}} L(\mathbf{x}_0) = 0\)</span>. Ta gọi những điểm mà
tại đó <span class="math notranslate nohighlight">\(\nabla_{\mathbf{x}} L(\mathbf{x}_0) = 0\)</span> là <em>các điểm tới
hạn</em> (<em>critical points</em>).</p>
<!--
This is nice, because in some rare settings, we *can* explicitly find all the points where the gradient is zero, and find the one with the smallest value.
--><p>Điều này rất hữu ích, bởi vì trong một vài thiết lập hiếm gặp, ta <em>có
thể</em> tìm được các điểm có gradient bằng không một cách tường minh, và từ
đó tìm được điểm có giá trị nhỏ nhất.</p>
<!--
For a concrete example, consider the function
--><p>Với một ví dụ cụ thể, xét hàm</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-9">
<span class="eqno">(18.4.12)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-9" title="Permalink to this equation">¶</a></span>\[f(x) = 3x^4 - 4x^3 -12x^2.\]</div>
<!--
This function has derivative
--><p>Hàm này có đạo hàm</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-10">
<span class="eqno">(18.4.13)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-10" title="Permalink to this equation">¶</a></span>\[\frac{df}{dx} = 12x^3 - 12x^2 -24x = 12x(x-2)(x+1).\]</div>
<!--
The only possible location of minima are at $x = -1, 0, 2$, where the function takes the values $-5,0, -32$ respectively,
and thus we can conclude that we minimize our function when $x = 2$. A quick plot confirms this.
--><p>Các điểm cực trị duy nhất khả dĩ là tại <span class="math notranslate nohighlight">\(x = -1, 0, 2\)</span>, khi hàm
lấy giá trị lần lượt là <span class="math notranslate nohighlight">\(-5,0, -32\)</span>, và do đó ta có thể kết luận
rằng ta cực tiểu hóa hàm khi <span class="math notranslate nohighlight">\(x = 2\)</span>. Ta có thể kiểm chứng nhanh
bằng đồ thị.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>
<span class="n">f</span> <span class="o">=</span> <span class="p">(</span><span class="mi">3</span> <span class="o">*</span> <span class="n">x</span><span class="o">**</span><span class="mi">4</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="n">x</span><span class="o">**</span><span class="mi">3</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="mi">12</span> <span class="o">*</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="n">d2l</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;f(x)&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="figure align-default">
<img alt="../_images/output_multivariable-calculus_vn_3c62bf_3_0.svg" src="../_images/output_multivariable-calculus_vn_3c62bf_3_0.svg" /></div>
<!--
This highlights an important fact to know when working either theoretically or numerically:
the only possible points where we can minimize (or maximize) a function will have gradient equal to zero,
however, not every point with gradient zero is the true *global* minimum (or maximum).
--><p>Điều này nhấn mạnh một thực tế quan trọng cần biết kể cả khi làm việc
dưới dạng lý thuyết hay số học: các điểm khả dĩ duy nhất mà tại đó hàm
là cực tiểu (hoặc cực đại) sẽ có đạo hàm tại đó bằng không, tuy nhiên,
không phải tất cả các điểm có đạo hàm bằng không sẽ là cực tiểu (hay cực
đại) <em>toàn cục</em>.</p>
<!--
## Multivariate Chain Rule
--></div>
<div class="section" id="quy-tac-day-chuyen-cho-ham-da-bien">
<h2><span class="section-number">18.4.4. </span>Quy tắc Dây chuyền cho Hàm đa biến<a class="headerlink" href="#quy-tac-day-chuyen-cho-ham-da-bien" title="Permalink to this headline">¶</a></h2>
<!--
Let us suppose that we have a function of four variables ($w, x, y$, and $z$) which we can make by composing many terms:
--><p>Giả sử là ta có một hàm bốn biến (<span class="math notranslate nohighlight">\(w, x, y\)</span>, and <span class="math notranslate nohighlight">\(z\)</span>) được
tạo ra bằng cách kết hợp các hàm con:</p>
<div class="math notranslate nohighlight" id="equation-eq-multi-func-def">
<span class="eqno">(18.4.14)<a class="headerlink" href="#equation-eq-multi-func-def" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{aligned}f(u, v) &amp; = (u+v)^{2} \\u(a, b) &amp; = (a+b)^{2}, \qquad v(a, b) = (a-b)^{2}, \\a(w, x, y, z) &amp; = (w+x+y+z)^{2}, \qquad b(w, x, y, z) = (w+x-y-z)^2.\end{aligned}\end{split}\]</div>
<!--
Such chains of equations are common when working with neural networks, so trying to understand how to compute gradients of such functions is key.
We can start to see visual hints of this connection in :numref:`fig_chain-1` if we take a look at what variables directly relate to one another.
--><p>Các chuỗi phương trình như trên xuất hiện thường xuyên khi ta làm việc
với các mạng nơ-ron, do đó cố gắng hiểu xem làm thế nào để tính gradient
của các hàm này là thiết yếu. <a class="reference internal" href="#fig-chain-1"><span class="std std-numref">Fig. 18.4.1</span></a> biểu diễn trực
quan mỗi liên hệ trực tiếp giữa biến này với biến khác.</p>
<!--
![The function relations above where nodes represent values and edges show functional dependence.](../img/chain-net1.svg)
--><div class="figure align-default" id="id2">
<span id="fig-chain-1"></span><img alt="../_images/chain-net1.svg" src="../_images/chain-net1.svg" /><p class="caption"><span class="caption-number">Fig. 18.4.1 </span><span class="caption-text">Các quan hệ của hàm ở trên với các nút biểu diễn giá trị và mũi tên
cho biết sự phụ thuộc hàm.</span><a class="headerlink" href="#id2" title="Permalink to this image">¶</a></p>
</div>
<!--
Nothing stops us from just composing everything from :eqref:`eq_multi_func_def` and writing out that
--><p>Ta có thể kết hợp các phương trình trong <a class="reference internal" href="#equation-eq-multi-func-def">(18.4.14)</a> để
có</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-11">
<span class="eqno">(18.4.15)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-11" title="Permalink to this equation">¶</a></span>\[f(w, x, y, z) = \left(\left((w+x+y+z)^2+(w+x-y-z)^2\right)^2+\left((w+x+y+z)^2-(w+x-y-z)^2\right)^2\right)^2.\]</div>
<!--
We may then take the derivative by just using single variable derivatives,
but if we did that we would quickly find ourself swamped with terms, many of which are repeats!
Indeed, one can see that, for instance:
--><p>Tiếp theo ta có thể lấy đạo hàm bằng cách chỉ sử dụng các đạo hàm đơn
biến, nhưng nếu làm vậy ta sẽ nhanh chóng bị ngợp trong các số hạng, mà
đa phần là bị lặp lại! Thật vậy, ta có thể thấy ở ví dụ dưới đây:</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-12">
<span class="eqno">(18.4.16)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-12" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{aligned}
\frac{\partial f}{\partial w} &amp; = 2 \left(2 \left(2 (w + x + y + z) - 2 (w + x - y - z)\right) \left((w + x + y + z)^{2}- (w + x - y - z)^{2}\right) + \right.\\
&amp; \left. \quad 2 \left(2 (w + x - y - z) + 2 (w + x + y + z)\right) \left((w + x - y - z)^{2}+ (w + x + y + z)^{2}\right)\right) \times \\
&amp; \quad \left(\left((w + x + y + z)^{2}- (w + x - y - z)^2\right)^{2}+ \left((w + x - y - z)^{2}+ (w + x + y + z)^{2}\right)^{2}\right).
\end{aligned}\end{split}\]</div>
<!--
If we then also wanted to compute $\frac{\partial f}{\partial x}$, we would end up with a similar equation again with many repeated terms,
and many *shared* repeated terms between the two derivatives.
This represents a massive quantity of wasted work, and if we needed to compute derivatives this way,
the whole deep learning revolution would have stalled out before it began!
--><p>Kế đến nếu ta cũng muốn tính <span class="math notranslate nohighlight">\(\frac{\partial f}{\partial x}\)</span>, ta
sẽ lại kết thúc với một phương trình tương tự với nhiều thành phần bị
lặp lại, và nhiều thành phần lặp lại <em>chung</em> giữa hai đạo hàm. Điều này
thể hiện một khối lượng lớn công việc bị lãng phí, và nếu ta tính các
đạo hàm theo cách này, toàn bộ cuộc cách mạng học sâu sẽ chấm dứt trước
khi nó bắt đầu!</p>
<!--
Let us break up the problem.
We will start by trying to understand how $f$ changes when we change $a$, essentially assuming that $w, x, y$, and $z$ all do not exist.
We will reason as we did back when we worked with the gradient for the first time. Let us take $a$ and add a small amount $\epsilon$ to it.
--><p>Ta hãy chia nhỏ vấn đề này. Ta sẽ bắt đầu bằng cách thử hiểu <span class="math notranslate nohighlight">\(f\)</span>
thay đổi thế nào khi <span class="math notranslate nohighlight">\(a\)</span> thay đổi, giả định cần thiết là tất cả
<span class="math notranslate nohighlight">\(w, x, y\)</span>, và <span class="math notranslate nohighlight">\(z\)</span> không tồn tại. Ta sẽ lập luận giống như
lần đầu tiên ta làm việc với gradient. Hãy lấy <span class="math notranslate nohighlight">\(a\)</span> và cộng một
lượng nhỏ <span class="math notranslate nohighlight">\(\epsilon\)</span> vào nó.</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-13">
<span class="eqno">(18.4.17)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-13" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{aligned}
&amp; f(u(a+\epsilon, b), v(a+\epsilon, b)) \\
\approx &amp; f\left(u(a, b) + \epsilon\frac{\partial u}{\partial a}(a, b), v(a, b) + \epsilon\frac{\partial v}{\partial a}(a, b)\right) \\
\approx &amp; f(u(a, b), v(a, b)) + \epsilon\left[\frac{\partial f}{\partial u}(u(a, b), v(a, b))\frac{\partial u}{\partial a}(a, b) + \frac{\partial f}{\partial v}(u(a, b), v(a, b))\frac{\partial v}{\partial a}(a, b)\right].
\end{aligned}\end{split}\]</div>
<!--
The first line follows from the definition of partial derivative, and the second follows from the definition of gradient.
It is notationally burdensome to track exactly where we evaluate every derivative,
as in the expression $\frac{\partial f}{\partial u}(u(a, b), v(a, b))$, so we often abbreviate this to the much more memorable
--><p>Dòng đầu tiên theo sau từ định nghĩa đạo hàm từng phần, và dòng thứ hai
theo sau từ định nghĩa gradient. Thật khó khăn để lần theo các biến khi
tính đạo hàm, như trong biểu thức
<span class="math notranslate nohighlight">\(\frac{\partial f}{\partial u}(u(a, b), v(a, b))\)</span>, cho nên ta
thường rút gọn nó để dễ nhớ hơn</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-14">
<span class="eqno">(18.4.18)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-14" title="Permalink to this equation">¶</a></span>\[\frac{\partial f}{\partial a} = \frac{\partial f}{\partial u}\frac{\partial u}{\partial a}+\frac{\partial f}{\partial v}\frac{\partial v}{\partial a}.\]</div>
<!--
It is useful to think about the meaning of the process.
We are trying to understand how a function of the form $f(u(a, b), v(a, b))$ changes its value with a change in $a$.
There are two pathways this can occur: there is the pathway where $a \rightarrow u \rightarrow f$ and where $a \rightarrow v \rightarrow f$.
We can compute both of these contributions via the chain rule: $\frac{\partial w}{\partial u} \cdot \frac{\partial u}{\partial x}$
and $\frac{\partial w}{\partial v} \cdot \frac{\partial v}{\partial x}$ respectively, and added up.
--><p>Sẽ rất hữu ích khi ta suy nghĩ về ý nghĩa của biến đổi này. Ta đang cố
gắng hiểu làm thế nào một hàm có dạng <span class="math notranslate nohighlight">\(f(u(a, b), v(a, b))\)</span> thay
đổi giá trị của nó khi <span class="math notranslate nohighlight">\(a\)</span> thay đổi. Có hai hướng có thể xảy ra:
<span class="math notranslate nohighlight">\(a \rightarrow u \rightarrow f\)</span> và
<span class="math notranslate nohighlight">\(a \rightarrow v \rightarrow f\)</span>. Ta có thể lần lượt tính toán đóng
góp của cả hai hướng này thông qua quy tắc dây chuyền:
<span class="math notranslate nohighlight">\(\frac{\partial w}{\partial u} \cdot \frac{\partial u}{\partial x}\)</span>
và
<span class="math notranslate nohighlight">\(\frac{\partial w}{\partial v} \cdot \frac{\partial v}{\partial x}\)</span>,
rồi cộng gộp lại.</p>
<!--
Imagine we have a different network of functions where the functions on the right depend on those that are connected to on the left as is shown in :numref:`fig_chain-2`.
--><p>các hàm được kết nối ở bên trái như trong <a class="reference internal" href="#fig-chain-2"><span class="std std-numref">Fig. 18.4.2</span></a>.</p>
<!--
![Another more subtle example of the chain rule.](../img/chain-net2.svg)
--><div class="figure align-default" id="id3">
<span id="fig-chain-2"></span><img alt="../_images/chain-net2.svg" src="../_images/chain-net2.svg" /><p class="caption"><span class="caption-number">Fig. 18.4.2 </span><span class="caption-text">Một ví dụ khác về quy tắc dây chuyền.</span><a class="headerlink" href="#id3" title="Permalink to this image">¶</a></p>
</div>
<!--
To compute something like $\frac{\partial f}{\partial y}$, we need to sum over all (in this case $3$) paths from $y$ to $f$ giving
--><p>Để tính toán <span class="math notranslate nohighlight">\(\frac{\partial f}{\partial y}\)</span>, chúng ta cần tính
tổng toàn bộ đường đi từ <span class="math notranslate nohighlight">\(y\)</span> đến <span class="math notranslate nohighlight">\(f\)</span> (trường hợp này có 3
đường đi):</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-15">
<span class="eqno">(18.4.19)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-15" title="Permalink to this equation">¶</a></span>\[\frac{\partial f}{\partial y} = \frac{\partial f}{\partial a} \frac{\partial a}{\partial u} \frac{\partial u}{\partial y} + \frac{\partial f}{\partial u} \frac{\partial u}{\partial y} + \frac{\partial f}{\partial b} \frac{\partial b}{\partial v} \frac{\partial v}{\partial y}.\]</div>
<!--
Understanding the chain rule in this way will pay great dividends when trying to understand how gradients flow through networks,
and why various architectural choices like those in LSTMs (:numref:`sec_lstm`) or residual layers (:numref:`sec_resnet`) can help shape the learning process by controlling gradient flow.
--><p>Hiểu quy tắc dây chuyền theo cách này giúp chúng ta thấy được dòng chảy
của gradient xuyên suốt mạng và vì sao một số lựa chọn kiến trúc như
trong LSTM (<a class="reference internal" href="../chapter_recurrent-modern/lstm_vn.html#sec-lstm"><span class="std std-numref">Section 9.2</span></a>) hoặc các tầng phần dư
(<a class="reference internal" href="../chapter_convolutional-modern/resnet_vn.html#sec-resnet"><span class="std std-numref">Section 7.6</span></a>) có thể định hình quá trình học bằng cách kiểm
soát dòng chảy gradient.</p>
<!--
## The Backpropagation Algorithm
--></div>
<div class="section" id="thuat-toan-lan-truyen-nguoc-backpropagation">
<h2><span class="section-number">18.4.5. </span>Thuật toán Lan truyền ngược (<em>Backpropagation</em>)<a class="headerlink" href="#thuat-toan-lan-truyen-nguoc-backpropagation" title="Permalink to this headline">¶</a></h2>
<!--
Let us return to the example of :eqref:`eq_multi_func_def` the previous section where
--><p>Hãy xem lại ví dụ <a class="reference internal" href="#equation-eq-multi-func-def">(18.4.14)</a> ở phần trước:</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-16">
<span class="eqno">(18.4.20)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-16" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{aligned}
f(u, v) &amp; = (u+v)^{2} \\
u(a, b) &amp; = (a+b)^{2}, \qquad v(a, b) = (a-b)^{2}, \\
a(w, x, y, z) &amp; = (w+x+y+z)^{2}, \qquad b(w, x, y, z) = (w+x-y-z)^2.
\end{aligned}\end{split}\]</div>
<!--
If we want to compute say $\frac{\partial f}{\partial w}$ we may apply the multi-variate chain rule to see:
--><p>Nếu muốn tính <span class="math notranslate nohighlight">\(\frac{\partial f}{\partial w}\)</span> chẳng hạn, ta có thể
áp dụng quy tắc dây chuyền đa biến để thấy:</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-17">
<span class="eqno">(18.4.21)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-17" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{aligned}
\frac{\partial f}{\partial w} &amp; = \frac{\partial f}{\partial u}\frac{\partial u}{\partial w} + \frac{\partial f}{\partial v}\frac{\partial v}{\partial w}, \\
\frac{\partial u}{\partial w} &amp; = \frac{\partial u}{\partial a}\frac{\partial a}{\partial w}+\frac{\partial u}{\partial b}\frac{\partial b}{\partial w}, \\
\frac{\partial v}{\partial w} &amp; = \frac{\partial v}{\partial a}\frac{\partial a}{\partial w}+\frac{\partial v}{\partial b}\frac{\partial b}{\partial w}.
\end{aligned}\end{split}\]</div>
<!--
Let us try using this decomposition to compute $\frac{\partial f}{\partial w}$.
Notice that all we need here are the various single step partials:
--><p>Chúng ta hãy thử sử dụng cách phân tách này để tính
<span class="math notranslate nohighlight">\(\frac{\partial f}{\partial w}\)</span>. Tất cả những gì chúng ta cần ở
đây là các đạo hàm riêng:</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-18">
<span class="eqno">(18.4.22)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-18" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{aligned}
\frac{\partial f}{\partial u} = 2(u+v), &amp; \quad\frac{\partial f}{\partial v} = 2(u+v), \\
\frac{\partial u}{\partial a} = 2(a+b), &amp; \quad\frac{\partial u}{\partial b} = 2(a+b), \\
\frac{\partial v}{\partial a} = 2(a-b), &amp; \quad\frac{\partial v}{\partial b} = -2(a-b), \\
\frac{\partial a}{\partial w} = 2(w+x+y+z), &amp; \quad\frac{\partial b}{\partial w} = 2(w+x-y-z).
\end{aligned}\end{split}\]</div>
<!--
If we write this out into code this becomes a fairly manageable expression.
--><p>Khi lập trình, các tính toán này trở thành một biểu thức khá dễ quản lý.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Compute the value of the function from inputs to outputs</span>
<span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span>
<span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="p">(</span><span class="n">w</span> <span class="o">+</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span> <span class="o">+</span> <span class="n">z</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="n">w</span> <span class="o">+</span> <span class="n">x</span> <span class="o">-</span> <span class="n">y</span> <span class="o">-</span> <span class="n">z</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
<span class="n">u</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="p">(</span><span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="n">a</span> <span class="o">-</span> <span class="n">b</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
<span class="n">f</span> <span class="o">=</span> <span class="p">(</span><span class="n">u</span> <span class="o">+</span> <span class="n">v</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;    f at </span><span class="si">{</span><span class="n">w</span><span class="si">}</span><span class="s1">, </span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="s1">, </span><span class="si">{</span><span class="n">y</span><span class="si">}</span><span class="s1">, </span><span class="si">{</span><span class="n">z</span><span class="si">}</span><span class="s1"> is </span><span class="si">{</span><span class="n">f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="c1"># Compute the single step partials</span>
<span class="n">df_du</span><span class="p">,</span> <span class="n">df_dv</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">u</span> <span class="o">+</span> <span class="n">v</span><span class="p">),</span> <span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">u</span> <span class="o">+</span> <span class="n">v</span><span class="p">)</span>
<span class="n">du_da</span><span class="p">,</span> <span class="n">du_db</span><span class="p">,</span> <span class="n">dv_da</span><span class="p">,</span> <span class="n">dv_db</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="p">),</span> <span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="p">),</span> <span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">a</span> <span class="o">-</span> <span class="n">b</span><span class="p">),</span> <span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">a</span> <span class="o">-</span> <span class="n">b</span><span class="p">)</span>
<span class="n">da_dw</span><span class="p">,</span> <span class="n">db_dw</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">w</span> <span class="o">+</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span> <span class="o">+</span> <span class="n">z</span><span class="p">),</span> <span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">w</span> <span class="o">+</span> <span class="n">x</span> <span class="o">-</span> <span class="n">y</span> <span class="o">-</span> <span class="n">z</span><span class="p">)</span>

<span class="c1"># Compute the final result from inputs to outputs</span>
<span class="n">du_dw</span><span class="p">,</span> <span class="n">dv_dw</span> <span class="o">=</span> <span class="n">du_da</span><span class="o">*</span><span class="n">da_dw</span> <span class="o">+</span> <span class="n">du_db</span><span class="o">*</span><span class="n">db_dw</span><span class="p">,</span> <span class="n">dv_da</span><span class="o">*</span><span class="n">da_dw</span> <span class="o">+</span> <span class="n">dv_db</span><span class="o">*</span><span class="n">db_dw</span>
<span class="n">df_dw</span> <span class="o">=</span> <span class="n">df_du</span><span class="o">*</span><span class="n">du_dw</span> <span class="o">+</span> <span class="n">df_dv</span><span class="o">*</span><span class="n">dv_dw</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;df/dw at </span><span class="si">{</span><span class="n">w</span><span class="si">}</span><span class="s1">, </span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="s1">, </span><span class="si">{</span><span class="n">y</span><span class="si">}</span><span class="s1">, </span><span class="si">{</span><span class="n">z</span><span class="si">}</span><span class="s1"> is </span><span class="si">{</span><span class="n">df_dw</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span>    <span class="n">f</span> <span class="n">at</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span> <span class="ow">is</span> <span class="mi">1024</span>
<span class="n">df</span><span class="o">/</span><span class="n">dw</span> <span class="n">at</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span> <span class="ow">is</span> <span class="o">-</span><span class="mi">4096</span>
</pre></div>
</div>
<!--
However, note that this still does not make it easy to compute something like $\frac{\partial f}{\partial x}$.
The reason for that is the *way* we chose to apply the chain rule.
If we look at what we did above, we always kept $\partial w$ in the denominator when we could.
In this way, we chose to apply the chain rule seeing how $w$ changed every other variable.
If that is what we wanted, this would be a good idea.
However, think back to our motivation from deep learning: we want to see how every parameter changes the *loss*.
In essence, we want to apply the chain rule keeping $\partial f$ in the numerator whenever we can!
--><p>Tuy nhiên, cần lưu ý rằng điều này không làm cho các phép tính chẳng hạn
như <span class="math notranslate nohighlight">\(\frac{\partial f}{\partial x}\)</span> trở nên đơn giản. Lý do nằm ở
<em>cách</em> chúng ta chọn để áp dụng quy tắc dây chuyền. Nếu nhìn vào những
gì chúng ta đã làm ở trên, chúng ta luôn giữ <span class="math notranslate nohighlight">\(\partial w\)</span> ở mẫu
khi có thể. Với cách này, chúng ta áp dụng quy tắc dây chuyền để xem
<span class="math notranslate nohighlight">\(w\)</span> thay đổi các biến khác như thế nào. Nếu đó là những gì chúng
ta muốn thì cách này quả là một ý tưởng hay. Tuy nhiên, nghĩ lại về mục
tiêu của học sâu: chúng ta muốn thấy từng tham số thay đổi giá trị <em>mất
mát</em> như thế nào. Về cốt lõi, chúng ta luôn muốn áp dụng quy tắc dây
chuyền và giữ <span class="math notranslate nohighlight">\(\partial f\)</span> ở tử số bất cứ khi nào có thể!</p>
<!--
To be more explicit, note that we can write
--><p>Cụ thể hơn, chúng ta có thể viết như sau:</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-19">
<span class="eqno">(18.4.23)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-19" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{aligned}
\frac{\partial f}{\partial w} &amp; = \frac{\partial f}{\partial a}\frac{\partial a}{\partial w} + \frac{\partial f}{\partial b}\frac{\partial b}{\partial w}, \\
\frac{\partial f}{\partial a} &amp; = \frac{\partial f}{\partial u}\frac{\partial u}{\partial a}+\frac{\partial f}{\partial v}\frac{\partial v}{\partial a}, \\
\frac{\partial f}{\partial b} &amp; = \frac{\partial f}{\partial u}\frac{\partial u}{\partial b}+\frac{\partial f}{\partial v}\frac{\partial v}{\partial b}.
\end{aligned}\end{split}\]</div>
<!--
Note that this application of the chain rule has us explicitly compute
$\frac{\partial f}{\partial u}, \frac{\partial f}{\partial v}, \frac{\partial f}{\partial a}, \frac{\partial f}{\partial b}, \; \text{and} \; \frac{\partial f}{\partial w}$.
Nothing stops us from also including the equations:
--><p>Lưu ý rằng cách áp dụng quy tắc dây chuyền này buộc chúng ta phải tính
rõ
<span class="math notranslate nohighlight">\(\frac{\partial f}{\partial u}, \frac{\partial f}{\partial v}, \frac{\partial f}{\partial a}, \frac{\partial f}{\partial b}, \; \text{và} \; \frac{\partial f}{\partial w}\)</span>.
Chúng ta cũng có thể thêm vào các phương trình:</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-20">
<span class="eqno">(18.4.24)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-20" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{aligned}
\frac{\partial f}{\partial x} &amp; = \frac{\partial f}{\partial a}\frac{\partial a}{\partial x} + \frac{\partial f}{\partial b}\frac{\partial b}{\partial x}, \\
\frac{\partial f}{\partial y} &amp; = \frac{\partial f}{\partial a}\frac{\partial a}{\partial y}+\frac{\partial f}{\partial b}\frac{\partial b}{\partial y}, \\
\frac{\partial f}{\partial z} &amp; = \frac{\partial f}{\partial a}\frac{\partial a}{\partial z}+\frac{\partial f}{\partial b}\frac{\partial b}{\partial z}.
\end{aligned}\end{split}\]</div>
<!--
and then keeping track of how $f$ changes when we change *any* node in the entire network.  Let us implement it.
--><p>và tiếp đó theo dõi <span class="math notranslate nohighlight">\(f\)</span> biến đổi như thế nào khi chúng ta thay đổi
<em>bất kỳ</em> nút nào trong toàn bộ mạng. Hãy cùng lập trình nó.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Compute the value of the function from inputs to outputs</span>
<span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span>
<span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="p">(</span><span class="n">w</span> <span class="o">+</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span> <span class="o">+</span> <span class="n">z</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="n">w</span> <span class="o">+</span> <span class="n">x</span> <span class="o">-</span> <span class="n">y</span> <span class="o">-</span> <span class="n">z</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
<span class="n">u</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="p">(</span><span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="n">a</span> <span class="o">-</span> <span class="n">b</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
<span class="n">f</span> <span class="o">=</span> <span class="p">(</span><span class="n">u</span> <span class="o">+</span> <span class="n">v</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;f at </span><span class="si">{</span><span class="n">w</span><span class="si">}</span><span class="s1">, </span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="s1">, </span><span class="si">{</span><span class="n">y</span><span class="si">}</span><span class="s1">, </span><span class="si">{</span><span class="n">z</span><span class="si">}</span><span class="s1"> is </span><span class="si">{</span><span class="n">f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="c1"># Compute the derivative using the decomposition above</span>
<span class="c1"># First compute the single step partials</span>
<span class="n">df_du</span><span class="p">,</span> <span class="n">df_dv</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">u</span> <span class="o">+</span> <span class="n">v</span><span class="p">),</span> <span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">u</span> <span class="o">+</span> <span class="n">v</span><span class="p">)</span>
<span class="n">du_da</span><span class="p">,</span> <span class="n">du_db</span><span class="p">,</span> <span class="n">dv_da</span><span class="p">,</span> <span class="n">dv_db</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="p">),</span> <span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="p">),</span> <span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">a</span> <span class="o">-</span> <span class="n">b</span><span class="p">),</span> <span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">a</span> <span class="o">-</span> <span class="n">b</span><span class="p">)</span>
<span class="n">da_dw</span><span class="p">,</span> <span class="n">db_dw</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">w</span> <span class="o">+</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span> <span class="o">+</span> <span class="n">z</span><span class="p">),</span> <span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">w</span> <span class="o">+</span> <span class="n">x</span> <span class="o">-</span> <span class="n">y</span> <span class="o">-</span> <span class="n">z</span><span class="p">)</span>
<span class="n">da_dx</span><span class="p">,</span> <span class="n">db_dx</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">w</span> <span class="o">+</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span> <span class="o">+</span> <span class="n">z</span><span class="p">),</span> <span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">w</span> <span class="o">+</span> <span class="n">x</span> <span class="o">-</span> <span class="n">y</span> <span class="o">-</span> <span class="n">z</span><span class="p">)</span>
<span class="n">da_dy</span><span class="p">,</span> <span class="n">db_dy</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">w</span> <span class="o">+</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span> <span class="o">+</span> <span class="n">z</span><span class="p">),</span> <span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">w</span> <span class="o">+</span> <span class="n">x</span> <span class="o">-</span> <span class="n">y</span> <span class="o">-</span> <span class="n">z</span><span class="p">)</span>
<span class="n">da_dz</span><span class="p">,</span> <span class="n">db_dz</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">w</span> <span class="o">+</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span> <span class="o">+</span> <span class="n">z</span><span class="p">),</span> <span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">w</span> <span class="o">+</span> <span class="n">x</span> <span class="o">-</span> <span class="n">y</span> <span class="o">-</span> <span class="n">z</span><span class="p">)</span>

<span class="c1"># Now compute how f changes when we change any value from output to input</span>
<span class="n">df_da</span><span class="p">,</span> <span class="n">df_db</span> <span class="o">=</span> <span class="n">df_du</span><span class="o">*</span><span class="n">du_da</span> <span class="o">+</span> <span class="n">df_dv</span><span class="o">*</span><span class="n">dv_da</span><span class="p">,</span> <span class="n">df_du</span><span class="o">*</span><span class="n">du_db</span> <span class="o">+</span> <span class="n">df_dv</span><span class="o">*</span><span class="n">dv_db</span>
<span class="n">df_dw</span><span class="p">,</span> <span class="n">df_dx</span> <span class="o">=</span> <span class="n">df_da</span><span class="o">*</span><span class="n">da_dw</span> <span class="o">+</span> <span class="n">df_db</span><span class="o">*</span><span class="n">db_dw</span><span class="p">,</span> <span class="n">df_da</span><span class="o">*</span><span class="n">da_dx</span> <span class="o">+</span> <span class="n">df_db</span><span class="o">*</span><span class="n">db_dx</span>
<span class="n">df_dy</span><span class="p">,</span> <span class="n">df_dz</span> <span class="o">=</span> <span class="n">df_da</span><span class="o">*</span><span class="n">da_dy</span> <span class="o">+</span> <span class="n">df_db</span><span class="o">*</span><span class="n">db_dy</span><span class="p">,</span> <span class="n">df_da</span><span class="o">*</span><span class="n">da_dz</span> <span class="o">+</span> <span class="n">df_db</span><span class="o">*</span><span class="n">db_dz</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;df/dw at </span><span class="si">{</span><span class="n">w</span><span class="si">}</span><span class="s1">, </span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="s1">, </span><span class="si">{</span><span class="n">y</span><span class="si">}</span><span class="s1">, </span><span class="si">{</span><span class="n">z</span><span class="si">}</span><span class="s1"> is </span><span class="si">{</span><span class="n">df_dw</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;df/dx at </span><span class="si">{</span><span class="n">w</span><span class="si">}</span><span class="s1">, </span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="s1">, </span><span class="si">{</span><span class="n">y</span><span class="si">}</span><span class="s1">, </span><span class="si">{</span><span class="n">z</span><span class="si">}</span><span class="s1"> is </span><span class="si">{</span><span class="n">df_dx</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;df/dy at </span><span class="si">{</span><span class="n">w</span><span class="si">}</span><span class="s1">, </span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="s1">, </span><span class="si">{</span><span class="n">y</span><span class="si">}</span><span class="s1">, </span><span class="si">{</span><span class="n">z</span><span class="si">}</span><span class="s1"> is </span><span class="si">{</span><span class="n">df_dy</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;df/dz at </span><span class="si">{</span><span class="n">w</span><span class="si">}</span><span class="s1">, </span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="s1">, </span><span class="si">{</span><span class="n">y</span><span class="si">}</span><span class="s1">, </span><span class="si">{</span><span class="n">z</span><span class="si">}</span><span class="s1"> is </span><span class="si">{</span><span class="n">df_dz</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">f</span> <span class="n">at</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span> <span class="ow">is</span> <span class="mi">1024</span>
<span class="n">df</span><span class="o">/</span><span class="n">dw</span> <span class="n">at</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span> <span class="ow">is</span> <span class="o">-</span><span class="mi">4096</span>
<span class="n">df</span><span class="o">/</span><span class="n">dx</span> <span class="n">at</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span> <span class="ow">is</span> <span class="o">-</span><span class="mi">4096</span>
<span class="n">df</span><span class="o">/</span><span class="n">dy</span> <span class="n">at</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span> <span class="ow">is</span> <span class="o">-</span><span class="mi">4096</span>
<span class="n">df</span><span class="o">/</span><span class="n">dz</span> <span class="n">at</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span> <span class="ow">is</span> <span class="o">-</span><span class="mi">4096</span>
</pre></div>
</div>
<!--
The fact that we compute derivatives from $f$ back towards the inputs rather than from the inputs forward to the outputs
(as we did in the first code snippet above) is what gives this algorithm its name: *backpropagation*.
Note that there are two steps:
--><p>Việc tính đạo hàm từ <span class="math notranslate nohighlight">\(f\)</span> trở ngược về đầu vào thay vì từ đầu vào
đến đầu ra (như chúng ta đã thực hiện ở đoạn mã đầu tiên ở trên) là lý
do cho cái tên <em>lan truyền ngược</em> (<em>backpropagation</em>) của thuật toán. Có
hai bước:</p>
<!--
1. Compute the value of the function, and the single step partials from front to back. While not done above, this can be combined into a single *forward pass*.
2. Compute the gradient of $f$ from back to front.  We call this the *backwards pass*.
--><ol class="arabic simple">
<li>Tính giá trị của hàm và đạo hàm riêng theo từng bước đơn lẻ từ đầu
đến cuối. Mặc dù không được thực hiện ở trên, hai việc này có thể
được kết hợp vào một <em>lượt truyền xuôi</em> duy nhất.</li>
<li>Tính toán đạo hàm của <span class="math notranslate nohighlight">\(f\)</span> từ cuối về đầu. Chúng ta gọi đó là
<em>lượt truyền ngược</em>.</li>
</ol>
<!--
This is precisely what every deep learning algorithm implements to allow the computation of the gradient of the loss with respect to every weight in the network at one pass.
It is an astonishing fact that we have such a decomposition.
--><p>Đây chính xác là những gì mỗi thuật toán học sâu thực thi để tính
gradient của giá trị mất mát theo từng trọng số của mạng trong mỗi lượt
lan truyền. Thật thú vị vì chúng ta có một sự phân tách như trên.</p>
<!--
To see how to encapsulated this, let us take a quick look at this example.
--><p>Để tóm gọn phần này, hãy xem nhanh ví dụ sau.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Initialize as ndarrays, then attach gradients</span>
<span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="n">w</span><span class="o">.</span><span class="n">attach_grad</span><span class="p">()</span>
<span class="n">x</span><span class="o">.</span><span class="n">attach_grad</span><span class="p">()</span>
<span class="n">y</span><span class="o">.</span><span class="n">attach_grad</span><span class="p">()</span>
<span class="n">z</span><span class="o">.</span><span class="n">attach_grad</span><span class="p">()</span>

<span class="c1"># Do the computation like usual, tracking gradients</span>
<span class="k">with</span> <span class="n">autograd</span><span class="o">.</span><span class="n">record</span><span class="p">():</span>
    <span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="p">(</span><span class="n">w</span> <span class="o">+</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span> <span class="o">+</span> <span class="n">z</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="n">w</span> <span class="o">+</span> <span class="n">x</span> <span class="o">-</span> <span class="n">y</span> <span class="o">-</span> <span class="n">z</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
    <span class="n">u</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="p">(</span><span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="n">a</span> <span class="o">-</span> <span class="n">b</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
    <span class="n">f</span> <span class="o">=</span> <span class="p">(</span><span class="n">u</span> <span class="o">+</span> <span class="n">v</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>

<span class="c1"># Execute backward pass</span>
<span class="n">f</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;df/dw at </span><span class="si">{</span><span class="n">w</span><span class="si">}</span><span class="s1">, </span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="s1">, </span><span class="si">{</span><span class="n">y</span><span class="si">}</span><span class="s1">, </span><span class="si">{</span><span class="n">z</span><span class="si">}</span><span class="s1"> is </span><span class="si">{</span><span class="n">w</span><span class="o">.</span><span class="n">grad</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;df/dx at </span><span class="si">{</span><span class="n">w</span><span class="si">}</span><span class="s1">, </span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="s1">, </span><span class="si">{</span><span class="n">y</span><span class="si">}</span><span class="s1">, </span><span class="si">{</span><span class="n">z</span><span class="si">}</span><span class="s1"> is </span><span class="si">{</span><span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;df/dy at </span><span class="si">{</span><span class="n">w</span><span class="si">}</span><span class="s1">, </span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="s1">, </span><span class="si">{</span><span class="n">y</span><span class="si">}</span><span class="s1">, </span><span class="si">{</span><span class="n">z</span><span class="si">}</span><span class="s1"> is </span><span class="si">{</span><span class="n">y</span><span class="o">.</span><span class="n">grad</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;df/dz at </span><span class="si">{</span><span class="n">w</span><span class="si">}</span><span class="s1">, </span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="s1">, </span><span class="si">{</span><span class="n">y</span><span class="si">}</span><span class="s1">, </span><span class="si">{</span><span class="n">z</span><span class="si">}</span><span class="s1"> is </span><span class="si">{</span><span class="n">z</span><span class="o">.</span><span class="n">grad</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="o">/</span><span class="n">dw</span> <span class="n">at</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">1.0</span> <span class="ow">is</span> <span class="o">-</span><span class="mf">4096.0</span>
<span class="n">df</span><span class="o">/</span><span class="n">dx</span> <span class="n">at</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">1.0</span> <span class="ow">is</span> <span class="o">-</span><span class="mf">4096.0</span>
<span class="n">df</span><span class="o">/</span><span class="n">dy</span> <span class="n">at</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">1.0</span> <span class="ow">is</span> <span class="o">-</span><span class="mf">4096.0</span>
<span class="n">df</span><span class="o">/</span><span class="n">dz</span> <span class="n">at</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">1.0</span> <span class="ow">is</span> <span class="o">-</span><span class="mf">4096.0</span>
</pre></div>
</div>
<!--
All of what we did above can be done automatically by calling `f.backwards()`.
--><p>Tất cả những gì chúng ta làm ở trên có thể được thực hiện tự động bằng
cách gọi hàm <code class="docutils literal notranslate"><span class="pre">f.backwards()</span></code>.</p>
<!--
## Hessians
--></div>
<div class="section" id="hessian">
<h2><span class="section-number">18.4.6. </span>Hessian<a class="headerlink" href="#hessian" title="Permalink to this headline">¶</a></h2>
<!--
As with single variable calculus, it is useful to consider higher-order derivatives in order to
get a handle on how we can obtain a better approximation to a function than using the gradient alone.
--><p>Như với giải tích đơn biến, việc xem xét đạo hàm bậc cao hơn cũng hữu
ích để xấp xỉ tốt hơn một hàm so với việc chỉ sử dụng gradient.</p>
<!--
There is one immediate problem one encounters when working with higher order derivatives of functions of several variables, and that is there are a large number of them.
If we have a function $f(x_1, \ldots, x_n)$ of $n$ variables, then we can take $n^{2}$ many second derivatives, namely for any choice of $i$ and $j$:
--><p>Một vấn đề trước mắt khi làm việc với đạo hàm bậc cao hơn của hàm đa
biến đó là cần phải tính toán một số lượng lớn đạo hàm. Nếu chúng ta có
một hàm <span class="math notranslate nohighlight">\(f(x_1, \ldots, x_n)\)</span> với <span class="math notranslate nohighlight">\(n\)</span> biến, chúng ta có thể
cần <span class="math notranslate nohighlight">\(n^{2}\)</span> đạo hàm bậc 2, chẳng hạn để lựa chọn <span class="math notranslate nohighlight">\(i\)</span> và
<span class="math notranslate nohighlight">\(j\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-21">
<span class="eqno">(18.4.25)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-21" title="Permalink to this equation">¶</a></span>\[\frac{d^2f}{dx_idx_j} = \frac{d}{dx_i}\left(\frac{d}{dx_j}f\right).\]</div>
<!--
This is traditionally assembled into a matrix called the *Hessian*:
--><p>Biểu thức này được hợp thành một ma trận gọi là <em>Hessian</em>:</p>
<div class="math notranslate nohighlight" id="equation-eq-hess-def">
<span class="eqno">(18.4.26)<a class="headerlink" href="#equation-eq-hess-def" title="Permalink to this equation">¶</a></span>\[\begin{split}\mathbf{H}_f = \begin{bmatrix} \frac{d^2f}{dx_1dx_1} &amp; \cdots &amp; \frac{d^2f}{dx_1dx_n} \\ \vdots &amp; \ddots &amp; \vdots \\ \frac{d^2f}{dx_ndx_1} &amp; \cdots &amp; \frac{d^2f}{dx_ndx_n} \\ \end{bmatrix}.\end{split}\]</div>
<!--
Not every entry of this matrix is independent.
Indeed, we can show that as long as both *mixed partials* (partial derivatives with respect to more than one variable) exist
and are continuous, we can say that for any $i$, and $j$,
--><p>Không phải mọi hạng tử của ma trận này đều độc lập. Thật vậy, chúng ta
có thể chứng minh rằng miễn là cả hai <em>đạo hàm riêng hỗn hợp - mixed
partials</em> (đạo hàm riêng theo nhiều hơn một biến số) có tồn tại và liên
tục, thì hàm số luôn tồn tại và liên tục với mọi <span class="math notranslate nohighlight">\(i\)</span> và <span class="math notranslate nohighlight">\(j\)</span>,</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-22">
<span class="eqno">(18.4.27)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-22" title="Permalink to this equation">¶</a></span>\[\frac{d^2f}{dx_idx_j} = \frac{d^2f}{dx_jdx_i}.\]</div>
<!--
This follows by considering first perturbing a function in the direction of $x_i$,
and then perturbing it in $x_j$ and then comparing the result of that with what happens if we perturb first $x_j$ and then $x_i$,
with the knowledge that both of these orders lead to the same final change in the output of $f$.
--><p>Điều này suy ra được bằng việc xem xét khi ta thay đổi hàm lần lượt theo
<span class="math notranslate nohighlight">\(x_i\)</span> rồi <span class="math notranslate nohighlight">\(x_j\)</span>, và ngược lại thay đổi <span class="math notranslate nohighlight">\(x_j\)</span> rồi
<span class="math notranslate nohighlight">\(x_i\)</span>, và so sánh hai kết quả này, biết rằng cả hai thứ tự này ảnh
hưởng đến đầu ra của <span class="math notranslate nohighlight">\(f\)</span> như nhau.</p>
<!--
As with single variables, we can use these derivatives to get a far better idea of how the function behaves near a point.
In particular, we can use it to find the best fitting quadratic near a point $\mathbf{x}_0$, as we saw in a single variable.
--><p>Như với các hàm đơn biến, chúng ta có thể sử dụng những đạo hàm này để
hiểu rõ hơn về hành vi của hàm số lân cận một điểm. Cụ thể, chúng ta có
thể sử dụng nó để tìm hàm bậc hai phù hợp nhất lân cận
<span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span> tương tự như trong giải tích đơn biến.</p>
<!--
Let us see an example. Suppose that $f(x_1, x_2) = a + b_1x_1 + b_2x_2 + c_{11}x_1^{2} + c_{12}x_1x_2 + c_{22}x_2^{2}$.
This is the general form for a quadratic in two variables.
If we look at the value of the function, its gradient, and its Hessian :eqref:`eq_hess_def`, all at the point zero:
--><p>Hãy tham khảo một ví dụ. Giả sử rằng
<span class="math notranslate nohighlight">\(f(x_1, x_2) = a + b_1x_1 + b_2x_2 + c_{11}x_1^{2} + c_{12}x_1x_2 + c_{22}x_2^{2}\)</span>.
Đây là một dạng tổng quát của hàm bậc hai 2 biến. Nếu chúng ta nhìn vào
giá trị của hàm, gradient và Hessian của nó <a class="reference internal" href="#equation-eq-hess-def">(18.4.26)</a>, tất
cả tại điểm 0:</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-23">
<span class="eqno">(18.4.28)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-23" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{aligned}
f(0,0) &amp; = a, \\
\nabla f (0,0) &amp; = \begin{bmatrix}b_1 \\ b_2\end{bmatrix}, \\
\mathbf{H} f (0,0) &amp; = \begin{bmatrix}2 c_{11} &amp; c_{12} \\ c_{12} &amp; 2c_{22}\end{bmatrix},
\end{aligned}\end{split}\]</div>
<!--
we can get our original polynomial back by saying
--><p>Chúng ta có thể thu lại được đa thức ban đầu bằng cách đặt:</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-24">
<span class="eqno">(18.4.29)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-24" title="Permalink to this equation">¶</a></span>\[f(\mathbf{x}) = f(0) + \nabla f (0) \cdot \mathbf{x} + \frac{1}{2}\mathbf{x}^\top \mathbf{H} f (0) \mathbf{x}.\]</div>
<!--
In general, if we computed this expansion any point $\mathbf{x}_0$, we see that
--><p>Nhìn chung, nếu chúng ta tính toán khai triển này tại mọi điểm
<span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span>, ta có:</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-25">
<span class="eqno">(18.4.30)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-25" title="Permalink to this equation">¶</a></span>\[f(\mathbf{x}) = f(\mathbf{x}_0) + \nabla f (\mathbf{x}_0) \cdot (\mathbf{x}-\mathbf{x}_0) + \frac{1}{2}(\mathbf{x}-\mathbf{x}_0)^\top \mathbf{H} f (\mathbf{x}_0) (\mathbf{x}-\mathbf{x}_0).\]</div>
<!--
This works for any dimensional input, and provides the best approximating quadratic to any function at a point.
To give an example, let us plot the function
--><p>Cách này hoạt động cho bất cứ đầu vào thứ nguyên nào và cung cấp gần
đúng nhất hàm bậc hai cho một hàm bất kỳ tại một điểm. Lấy biểu đồ của
hàm sau làm ví dụ.</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-26">
<span class="eqno">(18.4.31)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-26" title="Permalink to this equation">¶</a></span>\[f(x, y) = xe^{-x^2-y^2}.\]</div>
<!--
One can compute that the gradient and Hessian are
--><p>Có thể tính toán gradient và Hessian như sau:</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-27">
<span class="eqno">(18.4.32)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-27" title="Permalink to this equation">¶</a></span>\[\begin{split}\nabla f(x, y) = e^{-x^2-y^2}\begin{pmatrix}1-2x^2 \\ -2xy\end{pmatrix} \; \text{and} \; \mathbf{H}f(x, y) = e^{-x^2-y^2}\begin{pmatrix} 4x^3 - 6x &amp; 4x^2y - 2y \\ 4x^2y-2y &amp;4xy^2-2x\end{pmatrix}.\end{split}\]</div>
<!--
And thus, with a little algebra, see that the approximating quadratic at $[-1,0]^\top$ is
--><p>Kết hợp một chút đại số, ta thấy rằng hàm bậc hai xấp xỉ tại
<span class="math notranslate nohighlight">\([-1,0]^\top\)</span> là:</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-28">
<span class="eqno">(18.4.33)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-28" title="Permalink to this equation">¶</a></span>\[f(x, y) \approx e^{-1}\left(-1 - (x+1) +(x+1)^2+y^2\right).\]</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Construct grid and compute function</span>
<span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">101</span><span class="p">),</span>
                   <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">101</span><span class="p">),</span> <span class="n">indexing</span><span class="o">=</span><span class="s1">&#39;ij&#39;</span><span class="p">)</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">x</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">-</span> <span class="n">y</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Compute approximating quadratic with gradient and Hessian at (1, 0)</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">y</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Plot function</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="s1">&#39;3d&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot_wireframe</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="o">**</span><span class="p">{</span><span class="s1">&#39;rstride&#39;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span> <span class="s1">&#39;cstride&#39;</span><span class="p">:</span> <span class="mi">10</span><span class="p">})</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot_wireframe</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="o">**</span><span class="p">{</span><span class="s1">&#39;rstride&#39;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span> <span class="s1">&#39;cstride&#39;</span><span class="p">:</span> <span class="mi">10</span><span class="p">},</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;purple&#39;</span><span class="p">)</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">set_figsize</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_zlim</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">dist</span> <span class="o">=</span> <span class="mi">12</span>
</pre></div>
</div>
<div class="figure align-default">
<img alt="../_images/output_multivariable-calculus_vn_3c62bf_11_0.svg" src="../_images/output_multivariable-calculus_vn_3c62bf_11_0.svg" /></div>
<!--
This forms the basis for Newton's Algorithm discussed in :numref:`sec_gd`,
where we perform numerical optimization iteratively finding the best fitting quadratic,
and then exactly minimizing that quadratic.
--><p>Điều này tạo cơ sở cho Thuật toán Newton được thảo luận ở
<a class="reference internal" href="../chapter_optimization/gd_vn.html#sec-gd"><span class="std std-numref">Section 11.7</span></a>, trong đó chúng ta lặp đi lặp lại việc tối ưu hoá để
tìm ra hàm bậc hai phù hợp nhất và sau đó cực tiểu hoá hàm bậc hai đó.</p>
<!--
## A Little Matrix Calculus
--></div>
<div class="section" id="giai-tich-ma-tran">
<h2><span class="section-number">18.4.7. </span>Giải tích Ma trận<a class="headerlink" href="#giai-tich-ma-tran" title="Permalink to this headline">¶</a></h2>
<!--
Derivatives of functions involving matrices turn out to be particularly nice.
This section can become notationally heavy, so may be skipped in a first reading,
but it is useful to know how derivatives of functions involving common matrix operations are often much cleaner than one might initially anticipate,
particularly given how central matrix operations are to deep learning applications.
--><p>Đạo hàm của các hàm có liên quan đến ma trận hoá ra rất đẹp. Phần này sẽ
nặng về mặt ký hiệu, vì vậy độc giả có thể bỏ qua trong lần đọc đầu
tiên. Tuy nhiên sẽ rất hữu ích khi biết rằng đạo hàm của các hàm liên
quan đến các phép toán ma trận thường gọn gàng hơn nhiều so với suy nghĩ
ban đầu của chúng ta, đặc biệt là bởi sự quan trọng của các phép tính ma
trận trong các ứng dụng học sâu.</p>
<!--
Let us begin with an example.  Suppose that we have some fixed column vector $\boldsymbol{\beta}$,
and we want to take the product function $f(\mathbf{x}) = \boldsymbol{\beta}^\top\mathbf{x}$,
and understand how the dot product changes when we change $\mathbf{x}$.
--><p>Hãy xem một ví dụ. Giả sử chúng ta có một vài vector cột cố định
<span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span>, và chúng ta muốn lấy hàm tích
<span class="math notranslate nohighlight">\(f(\mathbf{x}) = \boldsymbol{\beta}^\top\mathbf{x}\)</span>, và hiểu cách
tích vô hướng thay đổi khi chúng ta thay đổi <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>.</p>
<!--
A bit of notation that will be useful when working with matrix derivatives in ML is called the *denominator layout matrix derivative*
where we assemble our partial derivatives into the shape of whatever vector, matrix, or tensor is in the denominator of the differential.
In this case, we will write
--><p>Ký hiệu có tên <em>ma trận đạo hàm sắp xếp theo mẫu số - denominator layout
matrix derivative</em> sẽ hữu ích khi làm việc với ma trận đạo hàm trong học
máy, trong đó chúng ta tập hợp các đạo hàm riêng theo mẫu số của vi
phân, biểu diễn thành các dạng vector, ma trận hoặc tensor. Trong trường
hợp này, chúng ta viết:</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-29">
<span class="eqno">(18.4.34)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-29" title="Permalink to this equation">¶</a></span>\[\begin{split}\frac{df}{d\mathbf{x}} = \begin{bmatrix}
\frac{df}{dx_1} \\
\vdots \\
\frac{df}{dx_n}
\end{bmatrix},\end{split}\]</div>
<!--
where we matched the shape of the column vector $\mathbf{x}$.
--><p>mà ở đây nó khớp với hình dạng của vector cột <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>.</p>
<!--
If we write out our function into components this is
--><p>Triển khai hàm của chúng ta thành các thành tố</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-30">
<span class="eqno">(18.4.35)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-30" title="Permalink to this equation">¶</a></span>\[f(\mathbf{x}) = \sum_{i = 1}^{n} \beta_ix_i = \beta_1x_1 + \cdots + \beta_nx_n.\]</div>
<!--
If we now take the partial derivative with respect to say $\beta_1$, note that everything is zero but the first term,
which is just $x_1$ multiplied by $\beta_1$, so the we obtain that
--><p>Nếu bây giờ ta tính đạo hàm riêng theo <span class="math notranslate nohighlight">\(\beta_1\)</span> chẳng hạn, để ý
rằng tất cả các phần tử bằng không ngoại trừ số hạng đầu tiên là
<span class="math notranslate nohighlight">\(x_1\)</span> nhân với <span class="math notranslate nohighlight">\(\beta_1\)</span>. Vì thế, ta có</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-31">
<span class="eqno">(18.4.36)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-31" title="Permalink to this equation">¶</a></span>\[\frac{df}{dx_1} = \beta_1,\]</div>
<!--
or more generally that
--><p>hoặc tổng quát hơn đó là</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-32">
<span class="eqno">(18.4.37)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-32" title="Permalink to this equation">¶</a></span>\[\frac{df}{dx_i} = \beta_i.\]</div>
<!--
We can now reassemble this into a matrix to see
--><p>Bây giờ ta có thể gộp chúng lại thành một ma trận như sau</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-33">
<span class="eqno">(18.4.38)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-33" title="Permalink to this equation">¶</a></span>\[\begin{split}\frac{df}{d\mathbf{x}} = \begin{bmatrix}
\frac{df}{dx_1} \\
\vdots \\
\frac{df}{dx_n}
\end{bmatrix} = \begin{bmatrix}
\beta_1 \\
\vdots \\
\beta_n
\end{bmatrix} = \boldsymbol{\beta}.\end{split}\]</div>
<!--
This illustrates a few factors about matrix calculus that we will often counter throughout this section:
--><p>Biểu thức trên minh họa một vài yếu tố về giải tích ma trận mà ta sẽ gặp
trong suốt phần này:</p>
<!--
* First, The computations will get rather involved.
* Second, The final results are much cleaner than the intermediate process, and will always look similar to the single variable case.
In this case, note that $\frac{d}{dx}(bx) = b$ and $\frac{d}{d\mathbf{x}} (\boldsymbol{\beta}^\top\mathbf{x}) = \boldsymbol{\beta}$ are both similar.
* Third, transposes can often appear seemingly from nowhere.
The core reason for this is the convention that we match the shape of the denominator, thus when we multiply matrices,
we will need to take transposes to match back to the shape of the original term.
--><ul class="simple">
<li>Đầu tiên, các tính toán sẽ trở nên khá phức tạp.</li>
<li>Thứ hai, kết quả cuối cùng sẽ gọn gàng hơn quá trình tính toán trung
gian, và sẽ luôn có bề ngoài giống với trường hợp đơn biến. Trong
trường hợp này, hãy lưu ý rằng <span class="math notranslate nohighlight">\(\frac{d}{dx}(bx) = b\)</span> và
<span class="math notranslate nohighlight">\(\frac{d}{d\mathbf{x}} (\boldsymbol{\beta}^\top\mathbf{x}) = \boldsymbol{\beta}\)</span>
là như nhau.</li>
<li>Thứ ba, các chuyển vị có thể xuất hiện mà thoạt nhìn không biết chính
xác từ đâu ra. Lý do chủ yếu là do ta quy ước đạo hàm sẽ có cùng kích
thước với mẫu số, do đó khi nhân ma trận, ta cần lấy chuyển vị tương
ứng để khớp với kích thước ban đầu.</li>
</ul>
<!--
To keep building intuition, let us try a computation that is a little harder.
Suppose that we have a column vector $\mathbf{x}$, and a square matrix $A$ and we want to compute
--><p>Ta hãy thử một phép tính khó hơn làm ví dụ minh họa trực quan. Giả sử ta
có một vector cột <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> và một ma trận vuông <span class="math notranslate nohighlight">\(A\)</span>, và
ta ta muốn tính biểu thức sau:</p>
<div class="math notranslate nohighlight" id="equation-eq-mat-goal-1">
<span class="eqno">(18.4.39)<a class="headerlink" href="#equation-eq-mat-goal-1" title="Permalink to this equation">¶</a></span>\[\frac{d}{d\mathbf{x}}(\mathbf{x}^\top A \mathbf{x}).\]</div>
<!--
To drive towards easier to manipulate notation, let us consider this problem using Einstein notation.
In this case we can write the function as
--><p>Để thuận tiện cho việc ký hiệu, ta hãy viết lại bài toán bằng ký hiệu
Einstein.</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-34">
<span class="eqno">(18.4.40)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-34" title="Permalink to this equation">¶</a></span>\[\mathbf{x}^\top A \mathbf{x} = x_ia_{ij}x_j.\]</div>
<!--
To compute our derivative, we need to understand for every $k$, what the value of
--><p>Để tính đạo hàm, ta cần tính các giá trị sau với từng giá trị của biến
<span class="math notranslate nohighlight">\(k\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-35">
<span class="eqno">(18.4.41)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-35" title="Permalink to this equation">¶</a></span>\[\frac{d}{dx_k}(\mathbf{x}^\top A \mathbf{x}) = \frac{d}{dx_k}x_ia_{ij}x_j.\]</div>
<!--
By the product rule, this is
--><p>Theo quy tắc nhân, ta có</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-36">
<span class="eqno">(18.4.42)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-36" title="Permalink to this equation">¶</a></span>\[\frac{d}{dx_k}x_ia_{ij}x_j = \frac{dx_i}{dx_k}a_{ij}x_j + x_ia_{ij}\frac{dx_j}{dx_k}.\]</div>
<!--
For a term like $\frac{dx_i}{dx_k}$, it is not hard to see that this is one when $i=k$ and zero otherwise.
This means that every term where $i$ and $k$ are different vanish from this sum, so the only terms that remain in that first sum are the ones where $i=k$.
The same reasoning holds for the second term where we need $j=k$. This gives
--><p>Với số hạng như <span class="math notranslate nohighlight">\(\frac{dx_i}{dx_k}\)</span>, không khó để thấy rằng đạo
hàm trên có giá trị bằng 1 khi <span class="math notranslate nohighlight">\(i=k\)</span>, ngược lại nó sẽ bằng 0. Điều
này có nghĩa là mọi số hạng với <span class="math notranslate nohighlight">\(i\)</span> và <span class="math notranslate nohighlight">\(k\)</span> khác nhau sẽ biến
mất khỏi tổng trên, vì thế các số hạng duy nhất còn lại trong tổng đầu
tiên đó là những số hạng với <span class="math notranslate nohighlight">\(i=k\)</span>. Lập luận tương tự cũng áp dụng
cho số hạng thứ hai khi ta cần <span class="math notranslate nohighlight">\(j=k\)</span>. Từ đó, ta có</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-37">
<span class="eqno">(18.4.43)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-37" title="Permalink to this equation">¶</a></span>\[\frac{d}{dx_k}x_ia_{ij}x_j = a_{kj}x_j + x_ia_{ik}.\]</div>
<!--
Now, the names of the indices in Einstein notation are arbitrary---the fact that $i$ and $j$ are different is immaterial to this computation at this point,
so we can re-index so that they both use $i$ to see that
--><p>Hiện tại, tên của các chỉ số trong ký hiệu Einstein là tùy ý - việc
<span class="math notranslate nohighlight">\(i\)</span> và <span class="math notranslate nohighlight">\(j\)</span> khác nhau không quan trọng cho tính toán tại thời
điểm này, vì thế ta có thể gán lại chỉ số sao cho cả hai đều chứa
<span class="math notranslate nohighlight">\(i\)</span></p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-38">
<span class="eqno">(18.4.44)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-38" title="Permalink to this equation">¶</a></span>\[\frac{d}{dx_k}x_ia_{ij}x_j = a_{ki}x_i + x_ia_{ik} = (a_{ki} + a_{ik})x_i.\]</div>
<!--
Now, here is where we start to need some practice to go further.
Let us try and identify this outcome in terms of matrix operations.
$a_{ki} + a_{ik}$ is the $k, i$-th component of $\mathbf{A} + \mathbf{A}^\top$. This gives
--><p>Bây giờ, ta cần luyện tập một chút để có thể đi sâu hơn. Ta hãy thử xác
định kết quả trên theo các phép toán ma trận. <span class="math notranslate nohighlight">\(a_{ki} + a_{ik}\)</span> là
phần tử thứ <span class="math notranslate nohighlight">\(k, i\)</span> của <span class="math notranslate nohighlight">\(\mathbf{A} + \mathbf{A}^\top\)</span>. Từ
đó, ta có</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-39">
<span class="eqno">(18.4.45)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-39" title="Permalink to this equation">¶</a></span>\[\frac{d}{dx_k}x_ia_{ij}x_j = [\mathbf{A} + \mathbf{A}^\top]_{ki}x_i.\]</div>
<!--
Similarly, this term is now the product of the matrix $\mathbf{A} + \mathbf{A}^\top$ by the vector $\mathbf{x}$, so we see that
--><p>Tương tự, hạng tử này là tích của ma trận
<span class="math notranslate nohighlight">\(\mathbf{A} + \mathbf{A}^\top\)</span> với vector <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, nên
ta có</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-40">
<span class="eqno">(18.4.46)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-40" title="Permalink to this equation">¶</a></span>\[\left[\frac{d}{d\mathbf{x}}(\mathbf{x}^\top A \mathbf{x})\right]_k = \frac{d}{dx_k}x_ia_{ij}x_j = [(\mathbf{A} + \mathbf{A}^\top)\mathbf{x}]_k.\]</div>
<!--
Thus, we see that the $k$-th entry of the desired derivative from :eqref:`eq_mat_goal_1` is just the $k$-th entry of the vector on the right,
and thus the two are the same. Thus yields
--><p>Ta thấy phần tử thứ <span class="math notranslate nohighlight">\(k\)</span> của đạo hàm mong muốn từ
<a class="reference internal" href="#equation-eq-mat-goal-1">(18.4.39)</a> đơn giản là phần tử thứ <span class="math notranslate nohighlight">\(k\)</span> của vector
bên vế phải, và do đó hai phần tử này là như nhau. Điều này dẫn đến</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-41">
<span class="eqno">(18.4.47)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-41" title="Permalink to this equation">¶</a></span>\[\frac{d}{d\mathbf{x}}(\mathbf{x}^\top A \mathbf{x}) = (\mathbf{A} + \mathbf{A}^\top)\mathbf{x}.\]</div>
<!--
This required significantly more work than our last one, but the final result is small.
More than that, consider the following computation for traditional single variable derivatives:
--><p>Biểu thức trên cần nhiều biến đổi để suy ra được hơn ở phần trước, nhưng
kết quả cuối cùng vẫn sẽ gọn gàng. Hơn thế nữa, hãy xem xét tính toán
dưới đây cho đạo hàm đơn biến thông thường:</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-42">
<span class="eqno">(18.4.48)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-42" title="Permalink to this equation">¶</a></span>\[\frac{d}{dx}(xax) = \frac{dx}{dx}ax + xa\frac{dx}{dx} = (a+a)x.\]</div>
<!--
Equivalently $\frac{d}{dx}(ax^2) = 2ax = (a+a)x$.
Again, we get a result that looks rather like the single variable result but with a transpose tossed in.
--><p>Tương tự, <span class="math notranslate nohighlight">\(\frac{d}{dx}(ax^2) = 2ax = (a+a)x\)</span>. Một lần nữa, ta lại
thu được kết quả nhìn giống với trường hợp đơn biến nhưng với một phép
chuyển vị.</p>
<!--
At this point, the pattern should be looking rather suspicious, so let us try to figure out why.
When we take matrix derivatives like this, let us first assume that the expression we get will be another matrix expression:
an expression we can write it in terms of products and sums of matrices and their transposes.
If such an expression exists, it will need to be true for all matrices.
In particular, it will need to be true of $1 \times 1$ matrices, in which case the matrix product is just the product of the numbers,
the matrix sum is just the sum, and the transpose does nothing at all!
In other words, whatever expression we get *must* match the single variable expression.
This means that, with some practice, one can often guess matrix derivatives just by knowing what the associated single variable expression must look like!
--><p>Tại thời điểm này, cách tính trên có vẻ khá đáng ngờ, vì vậy ta hãy thử
tìm hiểu lý do tại sao. Khi ta lấy đạo hàm ma trận như trên, đầu tiên ta
giả sử biểu thức ta nhận được sẽ là một biểu thức ma trận khác: một biểu
thức mà ta có thể viết nó dưới dạng tích và tổng của các ma trận và
chuyển vị của chúng. Nếu một biểu thức như vậy tồn tại, nó sẽ phải đúng
cho tất cả các ma trận. Do đó, nó sẽ đúng với ma trận
<span class="math notranslate nohighlight">\(1 \times 1\)</span>, trong đó tích ma trận chỉ là tích của các số, tổng
ma trận chỉ là tổng, và phép chuyển vị không có tác dụng gì! Nói cách
khác, bất kỳ biểu thức nào chúng ta nhận được <em>phải</em> phù hợp với biểu
thức đơn biến. Điều này có nghĩa là khi ta biết đạo hàm đơn biến tương
ứng, với một chút luyện tập ta có thể đoán được các đạo hàm ma trận!</p>
<!--
Let us try this out.
Suppose that $\mathbf{X}$ is a $n \times m$ matrix,
$\mathbf{U}$ is an $n \times r$ and $\mathbf{V}$ is an $r \times m$.
Let us try to compute
--><p>Cùng kiểm nghiệm điều này. Giả sử <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> là ma trận
<span class="math notranslate nohighlight">\(n \times m\)</span>, <span class="math notranslate nohighlight">\(\mathbf{U}\)</span> là ma trận <span class="math notranslate nohighlight">\(n \times r\)</span> và
<span class="math notranslate nohighlight">\(\mathbf{V}\)</span> là ma trận <span class="math notranslate nohighlight">\(r \times m\)</span>. Ta sẽ tính</p>
<div class="math notranslate nohighlight" id="equation-eq-mat-goal-2">
<span class="eqno">(18.4.49)<a class="headerlink" href="#equation-eq-mat-goal-2" title="Permalink to this equation">¶</a></span>\[\frac{d}{d\mathbf{V}} \|\mathbf{X} - \mathbf{U}\mathbf{V}\|_2^{2} = \;?\]</div>
<!--
This computation is important in an area called matrix factorization.
For us, however, it is just a derivative to compute.
Let us try to imaging what this would be for $1\times1$ matrices.
In that case, we get the expression
--><p>Phép tính này khá quan trọng trong phân rã ma trận. Tuy nhiên, ở đây nó
chỉ đơn giản là một đạo hàm mà ta cần tính. Hãy thử tưởng tượng xem nó
sẽ như thế nào đối với ma trận <span class="math notranslate nohighlight">\(1\times1\)</span>. Trong trường hợp này,
ta có biểu thức sau</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-43">
<span class="eqno">(18.4.50)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-43" title="Permalink to this equation">¶</a></span>\[\frac{d}{dv} (x-uv)^{2}= -2(x-uv)u,\]</div>
<!--
where, the derivative is rather standard.
If we try to convert this back into a matrix expression we get
--><p>Có thể thấy, đây là một đạo hàm khá phổ thông. Nếu ta thử chuyển đổi nó
thành một biểu thức ma trận, ta có</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-44">
<span class="eqno">(18.4.51)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-44" title="Permalink to this equation">¶</a></span>\[\frac{d}{d\mathbf{V}} \|\mathbf{X} - \mathbf{U}\mathbf{V}\|_2^{2}= -2(\mathbf{X} - \mathbf{U}\mathbf{V})\mathbf{U}.\]</div>
<!--
However, if we look at this it does not quite work. Recall that $\mathbf{X}$ is $n \times m$, as is $\mathbf{U}\mathbf{V}$,
so the matrix $2(\mathbf{X} - \mathbf{U}\mathbf{V})$ is $n \times m$.
On the other hand $\mathbf{U}$ is $n \times r$,
and we cannot multiply a $n \times m$ and a $n \times r$ matrix since the dimensions do not match!
--><p>Tuy nhiên, nếu ta nhìn kỹ, điều này không hoàn toàn đúng. Hãy nhớ lại
<span class="math notranslate nohighlight">\(\mathbf{X}\)</span> có kích thước <span class="math notranslate nohighlight">\(n \times m\)</span>, giống
<span class="math notranslate nohighlight">\(\mathbf{U}\mathbf{V}\)</span>, nên ma trận
<span class="math notranslate nohighlight">\(2(\mathbf{X} - \mathbf{U}\mathbf{V})\)</span> có kích thước
<span class="math notranslate nohighlight">\(n \times m\)</span>. Mặt khác <span class="math notranslate nohighlight">\(\mathbf{U}\)</span> có kích thước
<span class="math notranslate nohighlight">\(n \times r\)</span>, và ta không thể nhân một ma trận <span class="math notranslate nohighlight">\(n \times m\)</span>
với một ma trận <span class="math notranslate nohighlight">\(n \times r\)</span> vì số chiều của chúng không khớp
nhau!</p>
<!--
We want to get $\frac{d}{d\mathbf{V}}$, which is the same shape of $\mathbf{V}$, which is $r \times m$.
So somehow we need to take a $n \times m$ matrix and a $n \times r$ matrix, multiply them together (perhaps with some transposes) to get a $r \times m$.
We can do this by multiplying $U^\top$ by $(\mathbf{X} - \mathbf{U}\mathbf{V})$.
Thus, we can guess the solution to :eqref:`eq_mat_goal_2` is
--><p>Ta muốn nhận <span class="math notranslate nohighlight">\(\frac{d}{d\mathbf{V}}\)</span>, cùng kích thước với
<span class="math notranslate nohighlight">\(\mathbf{V}\)</span> là <span class="math notranslate nohighlight">\(r \times m\)</span>. Vì vậy ta bằng cách nào đó cần
phải nhân một ma trận <span class="math notranslate nohighlight">\(n \times m\)</span> với một ma trận
<span class="math notranslate nohighlight">\(n \times r\)</span> (có thể phải chuyển vị) để có ma trận
<span class="math notranslate nohighlight">\(r \times m\)</span>. Ta có thể làm điều này bằng cách nhân <span class="math notranslate nohighlight">\(U^\top\)</span>
với <span class="math notranslate nohighlight">\((\mathbf{X} - \mathbf{U}\mathbf{V})\)</span>. Vì vậy, ta có thể đoán
nghiệm cho <a class="reference internal" href="#equation-eq-mat-goal-2">(18.4.49)</a> là</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-45">
<span class="eqno">(18.4.52)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-45" title="Permalink to this equation">¶</a></span>\[\frac{d}{d\mathbf{V}} \|\mathbf{X} - \mathbf{U}\mathbf{V}\|_2^{2}= -2\mathbf{U}^\top(\mathbf{X} - \mathbf{U}\mathbf{V}).\]</div>
<!--
To show that this works, we would be remiss to not provide a detailed computation.
If we already believe that this rule-of-thumb works, feel free to skip past this derivation. To compute
--><p>Để chứng minh rằng điều này là đúng, ta cần một tính toán chi tiết. Nếu
bạn tin rằng quy tắc trực quan ở trên là đúng, bạn có thể bỏ qua phần
trình bày này. Để tính toán</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-46">
<span class="eqno">(18.4.53)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-46" title="Permalink to this equation">¶</a></span>\[\frac{d}{d\mathbf{V}} \|\mathbf{X} - \mathbf{U}\mathbf{V}\|_2^2,\]</div>
<!--
we must find for every $a$, and $b$
--><p>với mỗi <span class="math notranslate nohighlight">\(a\)</span> và <span class="math notranslate nohighlight">\(b\)</span>, ta phải tính.</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-47">
<span class="eqno">(18.4.54)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-47" title="Permalink to this equation">¶</a></span>\[\frac{d}{dv_{ab}} \|\mathbf{X} - \mathbf{U}\mathbf{V}\|_2^{2}= \frac{d}{dv_{ab}} \sum_{i, j}\left(x_{ij} - \sum_k u_{ik}v_{kj}\right)^2.\]</div>
<!--
Recalling that all entries of $\mathbf{X}$ and $\mathbf{U}$ are constants as far as $\frac{d}{dv_{ab}}$ is concerned,
we may push the derivative inside the sum, and apply the chain rule to the square to get
--><p>Hãy nhớ lại rằng tất cả các phần tử của <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> và
<span class="math notranslate nohighlight">\(\mathbf{U}\)</span> là hằng số khi tính <span class="math notranslate nohighlight">\(\frac{d}{dv_{ab}}\)</span>, chúng
ta có thể đẩy đạo hàm bên trong tổng, và áp dụng quy tắc dây chuyền sau
đó bình phương lên để có</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-48">
<span class="eqno">(18.4.55)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-48" title="Permalink to this equation">¶</a></span>\[\frac{d}{dv_{ab}} \|\mathbf{X} - \mathbf{U}\mathbf{V}\|_2^{2}= \sum_{i, j}2\left(x_{ij} - \sum_k u_{ik}v_{kj}\right)\left(-\sum_k u_{ik}\frac{dv_{kj}}{dv_{ab}} \right).\]</div>
<!--
As in the previous derivation, we may note that $\frac{dv_{kj}}{dv_{ab}}$ is only non-zero if the $k=a$ and $j=b$.
If either of those conditions do not hold, the term in the sum is zero, and we may freely discard it. We see that
--><p>Tương tự phần diễn giải trước, ta có thể để ý rằng
<span class="math notranslate nohighlight">\(\frac{dv_{kj}}{dv_{ab}}\)</span> chỉ khác không nếu <span class="math notranslate nohighlight">\(k=a\)</span> và
<span class="math notranslate nohighlight">\(j=b\)</span>. Nếu một trong hai điều kiện đó không thỏa, số hạng trong
tổng bằng không, ta có thể tự do loại bỏ nó. Ta thấy rằng</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-49">
<span class="eqno">(18.4.56)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-49" title="Permalink to this equation">¶</a></span>\[\frac{d}{dv_{ab}} \|\mathbf{X} - \mathbf{U}\mathbf{V}\|_2^{2}= -2\sum_{i}\left(x_{ib} - \sum_k u_{ik}v_{kb}\right)u_{ia}.\]</div>
<!--
An important subtlety here is that the requirement that $k=a$ does not occur inside the inner sum since
that $k$ is a dummy variable which we are summing over inside the inner term.
For a notationally cleaner example, consider why
--><p>Một điểm tinh tế quan trọng ở đây là yêu cầu về <span class="math notranslate nohighlight">\(k=a\)</span> không xảy ra
bên trong tổng phía trong bởi <span class="math notranslate nohighlight">\(k\)</span> chỉ là một biến tùy ý để tính
tổng các số hạng trong tổng phía trong. Một ví dụ dễ hiểu hơn:</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-50">
<span class="eqno">(18.4.57)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-50" title="Permalink to this equation">¶</a></span>\[\frac{d}{dx_1} \left(\sum_i x_i \right)^{2}= 2\left(\sum_i x_i \right).\]</div>
<!--
From this point, we may start identifying components of the sum. First,
--><p>Từ đây, ta có thể bắt đầu xác định các thành phần của tổng. Đầu tiên,</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-51">
<span class="eqno">(18.4.58)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-51" title="Permalink to this equation">¶</a></span>\[\sum_k u_{ik}v_{kb} = [\mathbf{U}\mathbf{V}]_{ib}.\]</div>
<!--
So the entire expression in the inside of the sum is
--><p>Cho nên toàn bộ biểu thức bên trong tổng là</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-52">
<span class="eqno">(18.4.59)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-52" title="Permalink to this equation">¶</a></span>\[x_{ib} - \sum_k u_{ik}v_{kb} = [\mathbf{X}-\mathbf{U}\mathbf{V}]_{ib}.\]</div>
<!--
This means we may now write our derivative as
--><p>Điều này nghĩa là giờ đây đạo hàm của ta có thể viết dưới dạng</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-53">
<span class="eqno">(18.4.60)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-53" title="Permalink to this equation">¶</a></span>\[\frac{d}{dv_{ab}} \|\mathbf{X} - \mathbf{U}\mathbf{V}\|_2^{2}= -2\sum_{i}[\mathbf{X}-\mathbf{U}\mathbf{V}]_{ib}u_{ia}.\]</div>
<!--
We want this to look like the $a, b$ element of a matrix so we can use the technique as in the previous example to arrive at a matrix expression,
which means that we need to exchange the order of the indices on $u_{ia}$.
If we notice that $u_{ia} = [\mathbf{U}^\top]_{ai}$, we can then write
--><p>Chúng ta có thể muốn nó trông giống như phần tử <span class="math notranslate nohighlight">\(a, b\)</span> của một ma
trận để có thể sử dụng các kỹ thuật trong các ví dụ trước đó nhằm đạt
được một biểu thức ma trận, nghĩa là ta cần phải hoán đổi thứ tự của các
chỉ số trên <span class="math notranslate nohighlight">\(u_{ia}\)</span>. Nếu để ý
<span class="math notranslate nohighlight">\(u_{ia} = [\mathbf{U}^\top]_{ai}\)</span>, ta có thể viết</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-54">
<span class="eqno">(18.4.61)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-54" title="Permalink to this equation">¶</a></span>\[\frac{d}{dv_{ab}} \|\mathbf{X} - \mathbf{U}\mathbf{V}\|_2^{2}= -2\sum_{i} [\mathbf{U}^\top]_{ai}[\mathbf{X}-\mathbf{U}\mathbf{V}]_{ib}.\]</div>
<!--
This is a matrix product, and thus we can conclude that
--><p>Đây là tích một ma trận, vì thế ta có thể kết luận</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-55">
<span class="eqno">(18.4.62)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-55" title="Permalink to this equation">¶</a></span>\[\frac{d}{dv_{ab}} \|\mathbf{X} - \mathbf{U}\mathbf{V}\|_2^{2}= -2[\mathbf{U}^\top(\mathbf{X}-\mathbf{U}\mathbf{V})]_{ab}.\]</div>
<!--
and thus we may write the solution to :eqref:`eq_mat_goal_2`
--><p>và vì vậy ta có lời giải cho <a class="reference internal" href="#equation-eq-mat-goal-2">(18.4.49)</a></p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-56">
<span class="eqno">(18.4.63)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-vn-56" title="Permalink to this equation">¶</a></span>\[\frac{d}{d\mathbf{V}} \|\mathbf{X} - \mathbf{U}\mathbf{V}\|_2^{2}= -2\mathbf{U}^\top(\mathbf{X} - \mathbf{U}\mathbf{V}).\]</div>
<!--
This matches the solution we guessed above!
--><p>Lời giải này trùng với biểu thức mà ta đoán ở phía trên!</p>
<!--
It is reasonable to ask at this point, "Why can I not just write down matrix versions of all the calculus rules I have learned?
It is clear this is still mechanical. Why do we not just get it over with!"
And indeed there are such rules and :cite:`Petersen.Pedersen.ea.2008` provides an excellent summary.
However, due to the plethora of ways matrix operations can be combined compared to single values, there are many more matrix derivative rules than single variable ones.
It is often the case that it is best to work with the indices, or leave it up to automatic differentiation when appropriate.
--><p>Lúc này cũng dễ hiểu nếu ta tự hỏi “Tại sao không viết tất cả các quy
tắc giải tích đã từng học thành dạng ma trận? Điều này rõ ràng là công
việc máy móc. Tại sao ta không đơn giản là làm hết một lần cho xong?” Và
thực sự có những quy tắc như thế, <a class="bibtex reference internal" href="../chapter_references/zreferences.html#petersen-pedersen-ea-2008" id="id1">[Petersen et al., 2008]</a>
cho ta một bản tóm tắt tuyệt vời. Tuy nhiên, vì số cách kết hợp các phép
toán ma trận nhiều hơn hẳn so với các giá trị một biến, nên có nhiều quy
tắc đạo hàm ma trận hơn các quy tắc dành cho hàm cho một biến. Thông
thường, tốt nhất là làm việc với các chỉ số, hoặc dùng vi phân tự động
khi thích hợp.</p>
</div>
<div class="section" id="tom-tat">
<h2><span class="section-number">18.4.8. </span>Tóm tắt<a class="headerlink" href="#tom-tat" title="Permalink to this headline">¶</a></h2>
<!--
* In higher dimensions, we can define gradients which serve the same purpose as derivatives in one dimension.
These allow us to see how a multi-variable function changes when we make an arbitrary small change to the inputs.
* The backpropagation algorithm can be seen to be a method of organizing the multi-variable chain rule to allow for the efficient computation of many partial derivatives.
* Matrix calculus allows us to write the derivatives of matrix expressions in concise ways.
--><ul class="simple">
<li>Với không gian nhiều chiều, chúng ta có thể định nghĩa gradient cùng
mục đích như các đạo hàm một chiều. Điều này cho phép ta thấy cách
một hàm đa biến thay đổi như thế nào khi có bất kỳ thay đổi nhỏ xảy
ra ở đầu vào.</li>
<li>Thuật toán lan truyền ngược có thể được xem như một phương pháp trong
việc tổ chức quy tắc dây chuyền đa biến cho phép tính toán hiệu quả
các đạo hàm riêng.</li>
<li>Giải tích ma trận cho phép chúng ta viết các đạo hàm của biểu thức ma
trận một cách gọn gàng hơn.</li>
</ul>
</div>
<div class="section" id="bai-tap">
<h2><span class="section-number">18.4.9. </span>Bài tập<a class="headerlink" href="#bai-tap" title="Permalink to this headline">¶</a></h2>
<!--
1. Given a column vector $\boldsymbol{\beta}$, compute the derivatives of both $f(\mathbf{x}) = \boldsymbol{\beta}^\top\mathbf{x}$
and $g(\mathbf{x}) = \mathbf{x}^\top\boldsymbol{\beta}$. Why do you get the same answer?
2. Let $\mathbf{v}$ be an $n$ dimension vector. What is $\frac{\partial}{\partial\mathbf{v}}\|\mathbf{v}\|_2$?
3. Let $L(x, y) = \log(e^x + e^y)$.  Compute the gradient.  What is the sum of the components of the gradient?
4. Let $f(x, y) = x^2y + xy^2$. Show that the only critical point is $(0,0)$. By considering $f(x, x)$, determine if $(0,0)$ is a maximum, minimum, or neither.
5. Suppose that we are minimizing a function $f(\mathbf{x}) = g(\mathbf{x}) + h(\mathbf{x})$.
How can we geometrically interpret the condition of $\nabla f = 0$ in terms of $g$ and $h$?
--><ol class="arabic simple">
<li>Cho một vector cột <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span>, tính các đạo hàm của
cả hai ma trận
<span class="math notranslate nohighlight">\(f(\mathbf{x}) = \boldsymbol{\beta}^\top\mathbf{x}\)</span> và ma trận
<span class="math notranslate nohighlight">\(g(\mathbf{x}) = \mathbf{x}^\top\boldsymbol{\beta}\)</span>. Hãy cho
biết tại sao bạn lại ra cùng đáp án?</li>
<li>Cho <span class="math notranslate nohighlight">\(\mathbf{v}\)</span> là một vector <span class="math notranslate nohighlight">\(n\)</span> chiều. Vậy
<span class="math notranslate nohighlight">\(\frac{\partial}{\partial\mathbf{v}}\|\mathbf{v}\|_2\)</span>? là gì?</li>
<li>Cho <span class="math notranslate nohighlight">\(L(x, y) = \log(e^x + e^y)\)</span>. Tính toán gradient. Tổng của
các thành phần của gradient là gì?</li>
<li>Cho <span class="math notranslate nohighlight">\(f(x, y) = x^2y + xy^2\)</span>. Chứng minh rằng điểm tới hạn duy
nhất là <span class="math notranslate nohighlight">\((0,0)\)</span>. Bằng việc xem xét <span class="math notranslate nohighlight">\(f(x, x)\)</span>, hãy xác
định xem <span class="math notranslate nohighlight">\((0,0)\)</span> là cực đại, cực tiểu, hay không phải cả hai.</li>
<li>Giả sử ta đang tối thiểu hàm
<span class="math notranslate nohighlight">\(f(\mathbf{x}) = g(\mathbf{x}) + h(\mathbf{x})\)</span>. Làm cách nào
ta có thể diễn giải bằng hình học điều kiện <span class="math notranslate nohighlight">\(\nabla f = 0\)</span>
thông qua <span class="math notranslate nohighlight">\(g\)</span> và <span class="math notranslate nohighlight">\(h\)</span>?</li>
</ol>
</div>
<div class="section" id="thao-luan">
<h2><span class="section-number">18.4.10. </span>Thảo luận<a class="headerlink" href="#thao-luan" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li>Tiếng Anh: <a class="reference external" href="https://discuss.d2l.ai/t/413">MXNet</a>,
<a class="reference external" href="https://discuss.d2l.ai/t/1090">Pytorch</a>,
<a class="reference external" href="https://discuss.d2l.ai/t/1091">Tensorflow</a></li>
<li>Tiếng Việt: <a class="reference external" href="https://forum.machinelearningcoban.com/c/d2l">Diễn đàn Machine Learning Cơ
Bản</a></li>
</ul>
</div>
<div class="section" id="nhung-nguoi-thuc-hien">
<h2><span class="section-number">18.4.11. </span>Những người thực hiện<a class="headerlink" href="#nhung-nguoi-thuc-hien" title="Permalink to this headline">¶</a></h2>
<p>Bản dịch trong trang này được thực hiện bởi:</p>
<ul class="simple">
<li>Đoàn Võ Duy Thanh</li>
<li>Lê Khắc Hồng Phúc</li>
<li>Phạm Hồng Vinh</li>
<li>Nguyễn Lê Quang Nhật</li>
<li>Nguyễn Văn Quang</li>
<li>Nguyễn Thanh Hòa</li>
<li>Nguyễn Văn Cường</li>
<li>Trần Yến Thy</li>
<li>Nguyễn Mai Hoàng Long</li>
</ul>
</div>
</div>


        </div>
        <div class="side-doc-outline">
            <div class="side-doc-outline--content"> 
<div class="localtoc">
    <p class="caption">
      <span class="caption-text">Table Of Contents</span>
    </p>
    <ul>
<li><a class="reference internal" href="#">18.4. Giải tích Nhiều biến</a><ul>
<li><a class="reference internal" href="#dao-ham-trong-khong-gian-nhieu-chieu">18.4.1. Đạo hàm trong Không gian Nhiều chiều</a></li>
<li><a class="reference internal" href="#y-nghia-hinh-hoc-cua-gradient-va-thuat-toan-ha-gradient">18.4.2. Ý nghĩa Hình học của Gradient và Thuật toán Hạ Gradient</a></li>
<li><a class="reference internal" href="#mot-vai-chu-y-ve-toi-uu-hoa">18.4.3. Một vài chú ý về Tối ưu hóa</a></li>
<li><a class="reference internal" href="#quy-tac-day-chuyen-cho-ham-da-bien">18.4.4. Quy tắc Dây chuyền cho Hàm đa biến</a></li>
<li><a class="reference internal" href="#thuat-toan-lan-truyen-nguoc-backpropagation">18.4.5. Thuật toán Lan truyền ngược (<em>Backpropagation</em>)</a></li>
<li><a class="reference internal" href="#hessian">18.4.6. Hessian</a></li>
<li><a class="reference internal" href="#giai-tich-ma-tran">18.4.7. Giải tích Ma trận</a></li>
<li><a class="reference internal" href="#tom-tat">18.4.8. Tóm tắt</a></li>
<li><a class="reference internal" href="#bai-tap">18.4.9. Bài tập</a></li>
<li><a class="reference internal" href="#thao-luan">18.4.10. Thảo luận</a></li>
<li><a class="reference internal" href="#nhung-nguoi-thuc-hien">18.4.11. Những người thực hiện</a></li>
</ul>
</li>
</ul>

</div>
            </div>
        </div>

      <div class="clearer"></div>
    </div><div class="pagenation">
     <a id="button-prev" href="single-variable-calculus_vn.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="P">
         <i class="pagenation-arrow-L fas fa-arrow-left fa-lg"></i>
         <div class="pagenation-text">
            <span class="pagenation-direction">Previous</span>
            <div>18.3. Giải tích một biến</div>
         </div>
     </a>
     <a id="button-next" href="integral-calculus_vn.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="N">
         <i class="pagenation-arrow-R fas fa-arrow-right fa-lg"></i>
        <div class="pagenation-text">
            <span class="pagenation-direction">Next</span>
            <div>18.5. Giải tích Tích phân</div>
        </div>
     </a>
  </div>
        
        </main>
    </div>
  </body>
</html>