<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    
    <title>11.7. Hạ Gradient &#8212; Đắm mình vào Học Sâu 0.14.4 documentation</title>

    <link rel="stylesheet" href="../_static/basic.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/d2l.css" />
    <link rel="stylesheet" href="../_static/material-design-lite-1.3.0/material.blue-deep_orange.min.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx_materialdesign_theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fontawesome/all.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fonts.css" type="text/css" />
    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/language_data.js"></script>
    <script src="../_static/d2l.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="shortcut icon" href="../_static/favicon.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="11.8. Hạ Gradient Ngẫu nhiên" href="sgd_vn.html" />
    <link rel="prev" title="11.6. Tính lồi" href="convexity_vn.html" /> 
  </head>
<body>
    <div class="mdl-layout mdl-js-layout mdl-layout--fixed-header mdl-layout--fixed-drawer"><header class="mdl-layout__header mdl-layout__header--waterfall ">
    <div class="mdl-layout__header-row">
        
        <nav class="mdl-navigation breadcrumb">
            <a class="mdl-navigation__link" href="index_vn.html"><span class="section-number">11. </span>Thuật toán Tối ưu</a><i class="material-icons">navigate_next</i>
            <a class="mdl-navigation__link is-active"><span class="section-number">11.7. </span>Hạ Gradient</a>
        </nav>
        <div class="mdl-layout-spacer"></div>
        <nav class="mdl-navigation">
        
<form class="form-inline pull-sm-right" action="../search.html" method="get">
      <div class="mdl-textfield mdl-js-textfield mdl-textfield--expandable mdl-textfield--floating-label mdl-textfield--align-right">
        <label id="quick-search-icon" class="mdl-button mdl-js-button mdl-button--icon"  for="waterfall-exp">
          <i class="material-icons">search</i>
        </label>
        <div class="mdl-textfield__expandable-holder">
          <input class="mdl-textfield__input" type="text" name="q"  id="waterfall-exp" placeholder="Search" />
          <input type="hidden" name="check_keywords" value="yes" />
          <input type="hidden" name="area" value="default" />
        </div>
      </div>
      <div class="mdl-tooltip" data-mdl-for="quick-search-icon">
      Quick search
      </div>
</form>
        
<a id="button-show-source"
    class="mdl-button mdl-js-button mdl-button--icon"
    href="../_sources/chapter_optimization/gd_vn.rst.txt" rel="nofollow">
  <i class="material-icons">code</i>
</a>
<div class="mdl-tooltip" data-mdl-for="button-show-source">
Show Source
</div>
        </nav>
    </div>
    <div class="mdl-layout__header-row header-links">
      <div class="mdl-layout-spacer"></div>
      <nav class="mdl-navigation">
          
              <a  class="mdl-navigation__link" href="https://github.com/aivivn/d2l-vn">
                  <i class="fab fa-github"></i>
                  GitHub
              </a>
          
              <a  class="mdl-navigation__link" href="https://forum.machinelearningcoban.com/">
                  <i class="fab fa-discourse"></i>
                  Forum
              </a>
          
              <a  class="mdl-navigation__link" href="https://www.d2l.ai/">
                  <i class="fas fa-external-link-alt"></i>
                  English
              </a>
      </nav>
    </div>
</header><header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <img class="logo" src="../_static/logo-with-text-vi.png" alt="Đắm mình vào Học Sâu"/>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul>
<li class="toctree-l1"><a class="reference internal" href="../intro_vn.html">Giới thiệu từ nhóm dịch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface/index_vn.html">Lời nói đầu</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_install/index_vn.html">Cài đặt</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_notation/index_vn.html">Ký hiệu</a></li>
</ul>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../chapter_introduction/index_vn.html">1. Giới thiệu</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preliminaries/index_vn.html">2. Sơ bộ</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/ndarray_vn.html">2.1. Thao tác với Dữ liệu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/pandas_vn.html">2.2. Tiền xử lý dữ liệu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra_vn.html">2.3. Đại số tuyến tính</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/calculus_vn.html">2.4. Giải tích</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/autograd_vn.html">2.5. Tính vi phân Tự động</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/probability_vn.html">2.6. Xác suất</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/lookup-api_vn.html">2.7. Tài liệu</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_linear-networks/index_vn.html">3. Mạng nơ-ron Tuyến tính</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/linear-regression_vn.html">3.1. Hồi quy Tuyến tính</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/linear-regression-scratch_vn.html">3.2. Lập trình Hồi quy Tuyến tính từ đầu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/linear-regression-gluon_vn.html">3.3. Cách lập trình súc tích Hồi quy Tuyến tính</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/softmax-regression_vn.html">3.4. Hồi quy Softmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/fashion-mnist_vn.html">3.5. Bộ dữ liệu Phân loại Ảnh (Fashion-MNIST)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/softmax-regression-scratch_vn.html">3.6. Lập trình Hồi quy Sofmax từ đầu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/softmax-regression-gluon_vn.html">3.7. Cách lập trình súc tích Hồi quy Softmax</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_multilayer-perceptrons/index_vn.html">4. Perceptron Đa tầng</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/mlp_vn.html">4.1. Perceptron đa tầng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/mlp-scratch_vn.html">4.2. Lập trình Perceptron Đa tầng từ đầu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/mlp-gluon_vn.html">4.3. Cách lập trình súc tích Perceptron Đa tầng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/underfit-overfit_vn.html">4.4. Lựa Chọn Mô Hình, Dưới Khớp và Quá Khớp</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/weight-decay_vn.html">4.5. Suy giảm trọng số</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/dropout_vn.html">4.6. Dropout</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/backprop_vn.html">4.7. Lan truyền xuôi, Lan truyền ngược và Đồ thị tính toán</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/numerical-stability-and-init_vn.html">4.8. Ổn định Số học và Khởi tạo</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/environment_vn.html">4.9. Cân nhắc tới Môi trường</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/kaggle-house-price_vn.html">4.10. Dự đoán Giá Nhà trên Kaggle</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_deep-learning-computation/index_vn.html">5. Tính toán Học sâu</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_deep-learning-computation/model-construction_vn.html">5.1. Tầng và Khối</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_deep-learning-computation/parameters_vn.html">5.2. Quản lý Tham số</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_deep-learning-computation/deferred-init_vn.html">5.3. Khởi tạo trễ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_deep-learning-computation/custom-layer_vn.html">5.4. Các tầng Tuỳ chỉnh</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_deep-learning-computation/read-write_vn.html">5.5. Đọc/Ghi tệp</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_deep-learning-computation/use-gpu_vn.html">5.6. GPU</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_convolutional-neural-networks/index_vn.html">6. Mạng Nơ-ron Tích chập</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/why-conv_vn.html">6.1. Từ Tầng Kết nối Dày đặc đến phép Tích chập</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/conv-layer_vn.html">6.2. Phép Tích chập cho Ảnh</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/padding-and-strides_vn.html">6.3. Đệm và Sải Bước</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/channels_vn.html">6.4. Đa kênh Đầu vào và Đầu ra</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/pooling_vn.html">6.5. Gộp (<em>Pooling</em>)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/lenet_vn.html">6.6. Mạng Nơ-ron Tích chập (LeNet)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_convolutional-modern/index_vn.html">7. Mạng Nơ-ron Tích chập Hiện đại</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/alexnet_vn.html">7.1. Mạng Nơ-ron Tích chập Sâu (AlexNet)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/vgg_vn.html">7.2. Mạng sử dụng Khối (VGG)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/nin_vn.html">7.3. Mạng trong Mạng (<em>Network in Network - NiN</em>)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/googlenet_vn.html">7.4. Mạng nối song song (GoogLeNet)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/batch-norm_vn.html">7.5. Chuẩn hoá theo batch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/resnet_vn.html">7.6. Mạng phần dư (ResNet)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/densenet_vn.html">7.7. Mạng Tích chập Kết nối Dày đặc (DenseNet)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_recurrent-neural-networks/index_vn.html">8. Mạng Nơ-ron Hồi tiếp</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/sequence_vn.html">8.1. Mô hình chuỗi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/text-preprocessing_vn.html">8.2. Tiền Xử lý Dữ liệu Văn bản</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/language-models-and-dataset_vn.html">8.3. Mô hình Ngôn ngữ và Tập dữ liệu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/rnn_vn.html">8.4. Mạng nơ-ron Hồi tiếp</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/rnn-scratch_vn.html">8.5. Lập trình Mạng nơ-ron Hồi tiếp từ đầu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/rnn-gluon_vn.html">8.6. Lập trình súc tích Mạng nơ-ron Hồi tiếp</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/bptt_vn.html">8.7. Lan truyền Ngược qua Thời gian</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_recurrent-modern/index_vn.html">9. Mạng Nơ-ron Hồi tiếp Hiện đại</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/gru_vn.html">9.1. Nút Hồi tiếp có Cổng (GRU)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/lstm_vn.html">9.2. Bộ nhớ Ngắn hạn Dài (LSTM)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/deep-rnn_vn.html">9.3. Mạng Nơ-ron Hồi tiếp Sâu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/bi-rnn_vn.html">9.4. Mạng Nơ-ron Hồi tiếp Hai chiều</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/machine-translation-and-dataset_vn.html">9.5. Dịch Máy và Tập dữ liệu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/encoder-decoder_vn.html">9.6. Kiến trúc Mã hoá - Giải mã</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/seq2seq_vn.html">9.7. Chuỗi sang Chuỗi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/beam-search_vn.html">9.8. Tìm kiếm Chùm</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_attention-mechanisms/index_vn.html">10. Cơ chế Tập trung</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms/attention_vn.html">10.1. Cơ chế Tập trung</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms/seq2seq-attention_vn.html">10.2. Chuỗi sang Chuỗi áp dụng Cơ chế Tập trung</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms/transformer_vn.html">10.3. Kiến trúc Transformer</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="index_vn.html">11. Thuật toán Tối ưu</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="optimization-intro_vn.html">11.1. Tối ưu và Học sâu</a></li>
<li class="toctree-l2"><a class="reference internal" href="optimization-intro_vn.html#cac-thach-thuc-cua-toi-uu-trong-hoc-sau">11.2. Các Thách thức của Tối ưu trong Học sâu</a></li>
<li class="toctree-l2"><a class="reference internal" href="optimization-intro_vn.html#cac-vung-cuc-tieu">11.3. Các vùng Cực tiểu</a></li>
<li class="toctree-l2"><a class="reference internal" href="optimization-intro_vn.html#cac-diem-yen-ngua">11.4. Các điểm Yên ngựa</a></li>
<li class="toctree-l2"><a class="reference internal" href="optimization-intro_vn.html#tieu-bien-gradient">11.5. Tiêu biến Gradient</a></li>
<li class="toctree-l2"><a class="reference internal" href="convexity_vn.html">11.6. Tính lồi</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">11.7. Hạ Gradient</a></li>
<li class="toctree-l2"><a class="reference internal" href="sgd_vn.html">11.8. Hạ Gradient Ngẫu nhiên</a></li>
<li class="toctree-l2"><a class="reference internal" href="minibatch-sgd_vn.html">11.9. Hạ Gradient Ngẫu nhiên theo Minibatch</a></li>
<li class="toctree-l2"><a class="reference internal" href="momentum_vn.html">11.10. Động lượng</a></li>
<li class="toctree-l2"><a class="reference internal" href="adagrad_vn.html">11.11. Adagrad</a></li>
<li class="toctree-l2"><a class="reference internal" href="rmsprop_vn.html">11.12. RMSProp</a></li>
<li class="toctree-l2"><a class="reference internal" href="adadelta_vn.html">11.13. Adadelta</a></li>
<li class="toctree-l2"><a class="reference internal" href="adam_vn.html">11.14. Adam</a></li>
<li class="toctree-l2"><a class="reference internal" href="lr-scheduler_vn.html">11.15. Định thời Tốc độ Học</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_computational-performance/index_vn.html">12. Hiệu năng Tính toán</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/hybridize_vn.html">12.1. Trình biên dịch và Trình thông dịch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/async-computation_vn.html">12.2. Tính toán Bất đồng bộ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/auto-parallelism_vn.html">12.3. Song song hóa Tự động</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/hardware_vn.html">12.4. Phần cứng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/multiple-gpus_vn.html">12.5. Huấn luyện đa GPU</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/multiple-gpus-concise_vn.html">12.6. Cách lập trình Súc tích đa GPU</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/parameterserver_vn.html">12.7. Máy chủ Tham số</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_computer-vision/index_vn.html">13. Thị giác Máy tính</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/image-augmentation_vn.html">13.1. Tăng cường Ảnh</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/fine-tuning_vn.html">13.2. Tinh Chỉnh</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/bounding-box_vn.html">13.3. Phát hiện Vật thể và Khoanh vùng Đối tượng (Khung chứa)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/anchor_vn.html">13.4. Khung neo</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/multiscale-object-detection_vn.html">13.5. Phát hiện Vật thể Đa tỷ lệ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/object-detection-dataset_vn.html">13.6. Tập dữ liệu Phát hiện Đối tượng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/ssd_vn.html">13.7. Phát hiện Nhiều khung Một lượt (SSD)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/rcnn_vn.html">13.8. CNN theo Vùng (R-CNN)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/semantic-segmentation-and-dataset_vn.html">13.9. Phân vùng theo Ngữ nghĩa và Tập dữ liệu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/transposed-conv_vn.html">13.10. Tích chập Chuyển vị</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/fcn_vn.html">13.11. Mạng Tích chập Đầy đủ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/neural-style_vn.html">13.12. Truyền tải Phong cách Nơ-ron</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/kaggle-cifar10_vn.html">13.13. Phân loại ảnh (CIFAR-10) trên Kaggle</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/kaggle-dog_vn.html">13.14. Nhận diện Giống Chó (ImageNet Dogs) trên Kaggle</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/index_vn.html">14. Xử lý Ngôn ngữ Tự nhiên: Tiền Huấn luyện</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/word2vec_vn.html">14.1. Embedding Từ (word2vec)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/approx-training_vn.html">14.2. Huấn luyện Gần đúng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/word-embedding-dataset_vn.html">14.3. Tập dữ liệu để Tiền Huấn luyện Embedding Từ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/word2vec-pretraining_vn.html">14.4. Tiền huấn luyện word2vec</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/glove_vn.html">14.5. Embedding từ với Vector Toàn cục (GloVe)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/subword-embedding_vn.html">14.6. Embedding từ con</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/similarity-analogy_vn.html">14.7. Tìm kiếm từ Đồng nghĩa và Loại suy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/bert_vn.html">14.8. Biểu diễn Mã hóa hai chiều từ Transformer (BERT)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/bert-dataset_vn.html">14.9. Tập dữ liệu để Tiền huấn luyện BERT</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/bert-pretraining_vn.html">14.10. Tiền Huấn luyện BERT</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_natural-language-processing-applications/index_vn.html">15. Xử lý Ngôn ngữ Tự nhiên: Ứng dụng</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/sentiment-analysis-and-dataset_vn.html">15.1. Tác vụ Phân tích Cảm xúc và Bộ Dữ liệu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/sentiment-analysis-rnn_vn.html">15.2. Phân tích Cảm xúc: Sử dụng Mạng Nơ-ron Hồi tiếp</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/sentiment-analysis-cnn_vn.html">15.3. Phân tích Cảm xúc: Sử dụng Mạng Nơ-ron Tích Chập</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/natural-language-inference-and-dataset_vn.html">15.4. Suy luận ngôn ngữ tự nhiên và Tập dữ liệu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/natural-language-inference-attention_vn.html">15.5. Suy luận Ngôn ngữ Tự nhiên: Sử dụng Cơ chế Tập trung</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/finetuning-bert_vn.html">15.6. Tinh chỉnh BERT cho các Ứng dụng Cấp Chuỗi và Cấp Token</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/natural-language-inference-bert_vn.html">15.7. Suy luận Ngôn ngữ Tự nhiên: Tinh chỉnh BERT</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_recommender-systems/index_vn.html">16. Hệ thống Đề xuất</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/recsys-intro_vn.html">16.1. Tổng quan về Hệ thống Đề xuất</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/movielens_vn.html">16.2. Tập dữ liệu MovieLens</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/mf_vn.html">16.3. Phân rã Ma trận</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/autorec_vn.html">16.4. AutoRec: Dự đoán Đánh giá với Bộ tự Mã hóa</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/ranking_vn.html">16.5. Cá nhân hóa Xếp hạng trong Hệ thống Đề xuất</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/neumf_vn.html">16.6. Lọc Cộng tác Nơ-ron cho Cá nhân hóa Xếp hạng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/seqrec_vn.html">16.7. Hệ thống Đề xuất có Nhận thức về Chuỗi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/ctr_vn.html">16.8. Hệ thống Đề xuất Giàu Đặc trưng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/fm_vn.html">16.9. Máy Phân rã ma trận</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/deepfm_vn.html">16.10. Máy Phân rã Ma trận Sâu</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_generative-adversarial-networks/index_vn.html">17. Mạng Đối sinh</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_generative-adversarial-networks/gan_vn.html">17.1. Mạng Đối sinh</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_generative-adversarial-networks/dcgan_vn.html">17.2. Mạng Đối sinh Tích chập Sâu</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/index_vn.html">18. Phụ lục: Toán học cho Học Sâu</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/geometry-linear-algebraic-ops_vn.html">18.1. Các phép toán Hình học và Đại số Tuyến tính</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/eigendecomposition_vn.html">18.2. Phân rã trị riêng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/single-variable-calculus_vn.html">18.3. Giải tích một biến</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/multivariable-calculus_vn.html">18.4. Giải tích Nhiều biến</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/integral-calculus_vn.html">18.5. Giải tích Tích phân</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/random-variables_vn.html">18.6. Biến Ngẫu nhiên</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/maximum-likelihood_vn.html">18.7. Hợp lý Cực đại</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/distributions_vn.html">18.8. Các Phân phối Xác suất</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/naive-bayes_vn.html">18.9. Bộ phân loại Naive Bayes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/statistics_vn.html">18.10. Thống kê</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/information-theory_vn.html">18.11. Lý thuyết Thông tin</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/index_vn.html">19. Phụ lục: Công cụ cho Học Sâu</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/jupyter_vn.html">19.1. Sử dụng Jupyter</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/sagemaker_vn.html">19.2. Sử dụng Amazon SageMaker</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/aws_vn.html">19.3. Sử dụng Máy ảo AWS EC2</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/colab_vn.html">19.4. Sử dụng Google Colab</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/selecting-servers-gpus_vn.html">19.5. Lựa chọn Máy chủ &amp; GPU</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/contributing_vn.html">19.6. Đóng góp cho Quyển sách</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/d2l_vn.html">19.7. Tài liệu API của <code class="docutils literal notranslate"><span class="pre">d2l</span></code></a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../chapter_references/zreferences.html">Tài liệu tham khảo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../glossary.html">Bảng thuật ngữ</a></li>
</ul>

            </nav>
        
        </div>
    
</header>
        <main class="mdl-layout__content" tabIndex="0">

	<script type="text/javascript" src="../_static/sphinx_materialdesign_theme.js "></script>
    <header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <img class="logo" src="../_static/logo-with-text-vi.png" alt="Đắm mình vào Học Sâu"/>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul>
<li class="toctree-l1"><a class="reference internal" href="../intro_vn.html">Giới thiệu từ nhóm dịch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface/index_vn.html">Lời nói đầu</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_install/index_vn.html">Cài đặt</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_notation/index_vn.html">Ký hiệu</a></li>
</ul>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../chapter_introduction/index_vn.html">1. Giới thiệu</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preliminaries/index_vn.html">2. Sơ bộ</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/ndarray_vn.html">2.1. Thao tác với Dữ liệu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/pandas_vn.html">2.2. Tiền xử lý dữ liệu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra_vn.html">2.3. Đại số tuyến tính</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/calculus_vn.html">2.4. Giải tích</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/autograd_vn.html">2.5. Tính vi phân Tự động</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/probability_vn.html">2.6. Xác suất</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/lookup-api_vn.html">2.7. Tài liệu</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_linear-networks/index_vn.html">3. Mạng nơ-ron Tuyến tính</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/linear-regression_vn.html">3.1. Hồi quy Tuyến tính</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/linear-regression-scratch_vn.html">3.2. Lập trình Hồi quy Tuyến tính từ đầu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/linear-regression-gluon_vn.html">3.3. Cách lập trình súc tích Hồi quy Tuyến tính</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/softmax-regression_vn.html">3.4. Hồi quy Softmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/fashion-mnist_vn.html">3.5. Bộ dữ liệu Phân loại Ảnh (Fashion-MNIST)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/softmax-regression-scratch_vn.html">3.6. Lập trình Hồi quy Sofmax từ đầu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/softmax-regression-gluon_vn.html">3.7. Cách lập trình súc tích Hồi quy Softmax</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_multilayer-perceptrons/index_vn.html">4. Perceptron Đa tầng</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/mlp_vn.html">4.1. Perceptron đa tầng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/mlp-scratch_vn.html">4.2. Lập trình Perceptron Đa tầng từ đầu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/mlp-gluon_vn.html">4.3. Cách lập trình súc tích Perceptron Đa tầng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/underfit-overfit_vn.html">4.4. Lựa Chọn Mô Hình, Dưới Khớp và Quá Khớp</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/weight-decay_vn.html">4.5. Suy giảm trọng số</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/dropout_vn.html">4.6. Dropout</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/backprop_vn.html">4.7. Lan truyền xuôi, Lan truyền ngược và Đồ thị tính toán</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/numerical-stability-and-init_vn.html">4.8. Ổn định Số học và Khởi tạo</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/environment_vn.html">4.9. Cân nhắc tới Môi trường</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/kaggle-house-price_vn.html">4.10. Dự đoán Giá Nhà trên Kaggle</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_deep-learning-computation/index_vn.html">5. Tính toán Học sâu</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_deep-learning-computation/model-construction_vn.html">5.1. Tầng và Khối</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_deep-learning-computation/parameters_vn.html">5.2. Quản lý Tham số</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_deep-learning-computation/deferred-init_vn.html">5.3. Khởi tạo trễ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_deep-learning-computation/custom-layer_vn.html">5.4. Các tầng Tuỳ chỉnh</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_deep-learning-computation/read-write_vn.html">5.5. Đọc/Ghi tệp</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_deep-learning-computation/use-gpu_vn.html">5.6. GPU</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_convolutional-neural-networks/index_vn.html">6. Mạng Nơ-ron Tích chập</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/why-conv_vn.html">6.1. Từ Tầng Kết nối Dày đặc đến phép Tích chập</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/conv-layer_vn.html">6.2. Phép Tích chập cho Ảnh</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/padding-and-strides_vn.html">6.3. Đệm và Sải Bước</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/channels_vn.html">6.4. Đa kênh Đầu vào và Đầu ra</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/pooling_vn.html">6.5. Gộp (<em>Pooling</em>)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/lenet_vn.html">6.6. Mạng Nơ-ron Tích chập (LeNet)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_convolutional-modern/index_vn.html">7. Mạng Nơ-ron Tích chập Hiện đại</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/alexnet_vn.html">7.1. Mạng Nơ-ron Tích chập Sâu (AlexNet)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/vgg_vn.html">7.2. Mạng sử dụng Khối (VGG)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/nin_vn.html">7.3. Mạng trong Mạng (<em>Network in Network - NiN</em>)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/googlenet_vn.html">7.4. Mạng nối song song (GoogLeNet)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/batch-norm_vn.html">7.5. Chuẩn hoá theo batch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/resnet_vn.html">7.6. Mạng phần dư (ResNet)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/densenet_vn.html">7.7. Mạng Tích chập Kết nối Dày đặc (DenseNet)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_recurrent-neural-networks/index_vn.html">8. Mạng Nơ-ron Hồi tiếp</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/sequence_vn.html">8.1. Mô hình chuỗi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/text-preprocessing_vn.html">8.2. Tiền Xử lý Dữ liệu Văn bản</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/language-models-and-dataset_vn.html">8.3. Mô hình Ngôn ngữ và Tập dữ liệu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/rnn_vn.html">8.4. Mạng nơ-ron Hồi tiếp</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/rnn-scratch_vn.html">8.5. Lập trình Mạng nơ-ron Hồi tiếp từ đầu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/rnn-gluon_vn.html">8.6. Lập trình súc tích Mạng nơ-ron Hồi tiếp</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/bptt_vn.html">8.7. Lan truyền Ngược qua Thời gian</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_recurrent-modern/index_vn.html">9. Mạng Nơ-ron Hồi tiếp Hiện đại</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/gru_vn.html">9.1. Nút Hồi tiếp có Cổng (GRU)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/lstm_vn.html">9.2. Bộ nhớ Ngắn hạn Dài (LSTM)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/deep-rnn_vn.html">9.3. Mạng Nơ-ron Hồi tiếp Sâu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/bi-rnn_vn.html">9.4. Mạng Nơ-ron Hồi tiếp Hai chiều</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/machine-translation-and-dataset_vn.html">9.5. Dịch Máy và Tập dữ liệu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/encoder-decoder_vn.html">9.6. Kiến trúc Mã hoá - Giải mã</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/seq2seq_vn.html">9.7. Chuỗi sang Chuỗi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/beam-search_vn.html">9.8. Tìm kiếm Chùm</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_attention-mechanisms/index_vn.html">10. Cơ chế Tập trung</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms/attention_vn.html">10.1. Cơ chế Tập trung</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms/seq2seq-attention_vn.html">10.2. Chuỗi sang Chuỗi áp dụng Cơ chế Tập trung</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms/transformer_vn.html">10.3. Kiến trúc Transformer</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="index_vn.html">11. Thuật toán Tối ưu</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="optimization-intro_vn.html">11.1. Tối ưu và Học sâu</a></li>
<li class="toctree-l2"><a class="reference internal" href="optimization-intro_vn.html#cac-thach-thuc-cua-toi-uu-trong-hoc-sau">11.2. Các Thách thức của Tối ưu trong Học sâu</a></li>
<li class="toctree-l2"><a class="reference internal" href="optimization-intro_vn.html#cac-vung-cuc-tieu">11.3. Các vùng Cực tiểu</a></li>
<li class="toctree-l2"><a class="reference internal" href="optimization-intro_vn.html#cac-diem-yen-ngua">11.4. Các điểm Yên ngựa</a></li>
<li class="toctree-l2"><a class="reference internal" href="optimization-intro_vn.html#tieu-bien-gradient">11.5. Tiêu biến Gradient</a></li>
<li class="toctree-l2"><a class="reference internal" href="convexity_vn.html">11.6. Tính lồi</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">11.7. Hạ Gradient</a></li>
<li class="toctree-l2"><a class="reference internal" href="sgd_vn.html">11.8. Hạ Gradient Ngẫu nhiên</a></li>
<li class="toctree-l2"><a class="reference internal" href="minibatch-sgd_vn.html">11.9. Hạ Gradient Ngẫu nhiên theo Minibatch</a></li>
<li class="toctree-l2"><a class="reference internal" href="momentum_vn.html">11.10. Động lượng</a></li>
<li class="toctree-l2"><a class="reference internal" href="adagrad_vn.html">11.11. Adagrad</a></li>
<li class="toctree-l2"><a class="reference internal" href="rmsprop_vn.html">11.12. RMSProp</a></li>
<li class="toctree-l2"><a class="reference internal" href="adadelta_vn.html">11.13. Adadelta</a></li>
<li class="toctree-l2"><a class="reference internal" href="adam_vn.html">11.14. Adam</a></li>
<li class="toctree-l2"><a class="reference internal" href="lr-scheduler_vn.html">11.15. Định thời Tốc độ Học</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_computational-performance/index_vn.html">12. Hiệu năng Tính toán</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/hybridize_vn.html">12.1. Trình biên dịch và Trình thông dịch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/async-computation_vn.html">12.2. Tính toán Bất đồng bộ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/auto-parallelism_vn.html">12.3. Song song hóa Tự động</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/hardware_vn.html">12.4. Phần cứng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/multiple-gpus_vn.html">12.5. Huấn luyện đa GPU</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/multiple-gpus-concise_vn.html">12.6. Cách lập trình Súc tích đa GPU</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/parameterserver_vn.html">12.7. Máy chủ Tham số</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_computer-vision/index_vn.html">13. Thị giác Máy tính</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/image-augmentation_vn.html">13.1. Tăng cường Ảnh</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/fine-tuning_vn.html">13.2. Tinh Chỉnh</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/bounding-box_vn.html">13.3. Phát hiện Vật thể và Khoanh vùng Đối tượng (Khung chứa)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/anchor_vn.html">13.4. Khung neo</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/multiscale-object-detection_vn.html">13.5. Phát hiện Vật thể Đa tỷ lệ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/object-detection-dataset_vn.html">13.6. Tập dữ liệu Phát hiện Đối tượng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/ssd_vn.html">13.7. Phát hiện Nhiều khung Một lượt (SSD)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/rcnn_vn.html">13.8. CNN theo Vùng (R-CNN)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/semantic-segmentation-and-dataset_vn.html">13.9. Phân vùng theo Ngữ nghĩa và Tập dữ liệu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/transposed-conv_vn.html">13.10. Tích chập Chuyển vị</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/fcn_vn.html">13.11. Mạng Tích chập Đầy đủ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/neural-style_vn.html">13.12. Truyền tải Phong cách Nơ-ron</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/kaggle-cifar10_vn.html">13.13. Phân loại ảnh (CIFAR-10) trên Kaggle</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/kaggle-dog_vn.html">13.14. Nhận diện Giống Chó (ImageNet Dogs) trên Kaggle</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/index_vn.html">14. Xử lý Ngôn ngữ Tự nhiên: Tiền Huấn luyện</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/word2vec_vn.html">14.1. Embedding Từ (word2vec)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/approx-training_vn.html">14.2. Huấn luyện Gần đúng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/word-embedding-dataset_vn.html">14.3. Tập dữ liệu để Tiền Huấn luyện Embedding Từ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/word2vec-pretraining_vn.html">14.4. Tiền huấn luyện word2vec</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/glove_vn.html">14.5. Embedding từ với Vector Toàn cục (GloVe)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/subword-embedding_vn.html">14.6. Embedding từ con</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/similarity-analogy_vn.html">14.7. Tìm kiếm từ Đồng nghĩa và Loại suy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/bert_vn.html">14.8. Biểu diễn Mã hóa hai chiều từ Transformer (BERT)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/bert-dataset_vn.html">14.9. Tập dữ liệu để Tiền huấn luyện BERT</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/bert-pretraining_vn.html">14.10. Tiền Huấn luyện BERT</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_natural-language-processing-applications/index_vn.html">15. Xử lý Ngôn ngữ Tự nhiên: Ứng dụng</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/sentiment-analysis-and-dataset_vn.html">15.1. Tác vụ Phân tích Cảm xúc và Bộ Dữ liệu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/sentiment-analysis-rnn_vn.html">15.2. Phân tích Cảm xúc: Sử dụng Mạng Nơ-ron Hồi tiếp</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/sentiment-analysis-cnn_vn.html">15.3. Phân tích Cảm xúc: Sử dụng Mạng Nơ-ron Tích Chập</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/natural-language-inference-and-dataset_vn.html">15.4. Suy luận ngôn ngữ tự nhiên và Tập dữ liệu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/natural-language-inference-attention_vn.html">15.5. Suy luận Ngôn ngữ Tự nhiên: Sử dụng Cơ chế Tập trung</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/finetuning-bert_vn.html">15.6. Tinh chỉnh BERT cho các Ứng dụng Cấp Chuỗi và Cấp Token</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/natural-language-inference-bert_vn.html">15.7. Suy luận Ngôn ngữ Tự nhiên: Tinh chỉnh BERT</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_recommender-systems/index_vn.html">16. Hệ thống Đề xuất</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/recsys-intro_vn.html">16.1. Tổng quan về Hệ thống Đề xuất</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/movielens_vn.html">16.2. Tập dữ liệu MovieLens</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/mf_vn.html">16.3. Phân rã Ma trận</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/autorec_vn.html">16.4. AutoRec: Dự đoán Đánh giá với Bộ tự Mã hóa</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/ranking_vn.html">16.5. Cá nhân hóa Xếp hạng trong Hệ thống Đề xuất</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/neumf_vn.html">16.6. Lọc Cộng tác Nơ-ron cho Cá nhân hóa Xếp hạng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/seqrec_vn.html">16.7. Hệ thống Đề xuất có Nhận thức về Chuỗi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/ctr_vn.html">16.8. Hệ thống Đề xuất Giàu Đặc trưng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/fm_vn.html">16.9. Máy Phân rã ma trận</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/deepfm_vn.html">16.10. Máy Phân rã Ma trận Sâu</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_generative-adversarial-networks/index_vn.html">17. Mạng Đối sinh</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_generative-adversarial-networks/gan_vn.html">17.1. Mạng Đối sinh</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_generative-adversarial-networks/dcgan_vn.html">17.2. Mạng Đối sinh Tích chập Sâu</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/index_vn.html">18. Phụ lục: Toán học cho Học Sâu</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/geometry-linear-algebraic-ops_vn.html">18.1. Các phép toán Hình học và Đại số Tuyến tính</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/eigendecomposition_vn.html">18.2. Phân rã trị riêng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/single-variable-calculus_vn.html">18.3. Giải tích một biến</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/multivariable-calculus_vn.html">18.4. Giải tích Nhiều biến</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/integral-calculus_vn.html">18.5. Giải tích Tích phân</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/random-variables_vn.html">18.6. Biến Ngẫu nhiên</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/maximum-likelihood_vn.html">18.7. Hợp lý Cực đại</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/distributions_vn.html">18.8. Các Phân phối Xác suất</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/naive-bayes_vn.html">18.9. Bộ phân loại Naive Bayes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/statistics_vn.html">18.10. Thống kê</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/information-theory_vn.html">18.11. Lý thuyết Thông tin</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/index_vn.html">19. Phụ lục: Công cụ cho Học Sâu</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/jupyter_vn.html">19.1. Sử dụng Jupyter</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/sagemaker_vn.html">19.2. Sử dụng Amazon SageMaker</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/aws_vn.html">19.3. Sử dụng Máy ảo AWS EC2</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/colab_vn.html">19.4. Sử dụng Google Colab</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/selecting-servers-gpus_vn.html">19.5. Lựa chọn Máy chủ &amp; GPU</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/contributing_vn.html">19.6. Đóng góp cho Quyển sách</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/d2l_vn.html">19.7. Tài liệu API của <code class="docutils literal notranslate"><span class="pre">d2l</span></code></a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../chapter_references/zreferences.html">Tài liệu tham khảo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../glossary.html">Bảng thuật ngữ</a></li>
</ul>

            </nav>
        
        </div>
    
</header>

    <div class="document">
        <div class="page-content" role="main">
        
  <!--
# Gradient Descent
--><div class="section" id="ha-gradient">
<span id="sec-gd"></span><h1><span class="section-number">11.7. </span>Hạ Gradient<a class="headerlink" href="#ha-gradient" title="Permalink to this headline">¶</a></h1>
<!--
In this section we are going to introduce the basic concepts underlying gradient descent.
This is brief by necessity.
See e.g., :cite:`Boyd.Vandenberghe.2004` for an in-depth introduction to convex optimization.
Although the latter is rarely used directly in deep learning, an understanding of gradient descent is key to understanding stochastic gradient descent algorithms.
For instance, the optimization problem might diverge due to an overly large learning rate.
This phenomenon can already be seen in gradient descent.
Likewise, preconditioning is a common technique in gradient descent and carries over to more advanced algorithms.
Let us start with a simple special case.
--><p>Trong phần này chúng tôi sẽ giới thiệu các khái niệm cơ bản trong thuật
toán hạ gradient. Nội dung cần thiết sẽ được trình bày ngắn gọn. Độc giả
có thể tham khảo <a class="bibtex reference internal" href="../chapter_references/zreferences.html#boyd-vandenberghe-2004" id="id1">[Boyd &amp; Vandenberghe, 2004]</a> để có góc nhìn sâu về
bài toán tối ưu lồi. Mặc dù tối ưu lồi hiếm khi được áp dụng trực tiếp
trong học sâu, kiến thức về thuật toán hạ gradient là chìa khóa để hiểu
rõ hơn về thuật toán hạ gradient ngẫu nhiên. Ví dụ, bài toán tối ưu có
thể phân kỳ do tốc độ học quá lớn. Hiện tượng này có thể quan sát được
trong thuật toán hạ gradient. Tương tự, tiền điều kiện
(<em>preconditioning</em>) là một kỹ thuật phổ biến trong thuật toán hạ
gradient và nó cũng được áp dụng trong các thuật toán tân tiến hơn. Hãy
bắt đầu với một trường hợp đặc biệt và đơn giản.</p>
<!--
## Gradient Descent in One Dimension
--><div class="section" id="ha-gradient-trong-mot-chieu">
<h2><span class="section-number">11.7.1. </span>Hạ Gradient trong Một Chiều<a class="headerlink" href="#ha-gradient-trong-mot-chieu" title="Permalink to this headline">¶</a></h2>
<!--
Gradient descent in one dimension is an excellent example to explain why the gradient descent algorithm may reduce the value of the objective function.
Consider some continuously differentiable real-valued function $f: \mathbb{R} \rightarrow \mathbb{R}$.
Using a Taylor expansion (:numref:`sec_single_variable_calculus`) we obtain that
--><p>Hạ gradient trong một chiều là ví dụ tuyệt vời để giải thích tại sao
thuật toán hạ gradient có thể giảm giá trị hàm mục tiêu. Hãy xem xét một
hàm số thực khả vi liên tục
<span class="math notranslate nohighlight">\(f: \mathbb{R} \rightarrow \mathbb{R}\)</span>. Áp dụng khai triển Taylor
(<a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/single-variable-calculus_vn.html#sec-single-variable-calculus"><span class="std std-numref">Section 18.3</span></a>), ta có</p>
<div class="math notranslate nohighlight" id="equation-gd-taylor">
<span class="eqno">(11.7.1)<a class="headerlink" href="#equation-gd-taylor" title="Permalink to this equation">¶</a></span>\[f(x + \epsilon) = f(x) + \epsilon f'(x) + \mathcal{O}(\epsilon^2).\]</div>
<!--
That is, in first approximation $f(x+\epsilon)$ is given by the function value $f(x)$ and the first derivative $f'(x)$ at $x$.
It is not unreasonable to assume that for small $\epsilon$ moving in the direction of the negative gradient will decrease $f$.
To keep things simple we pick a fixed step size $\eta > 0$ and choose $\epsilon = -\eta f'(x)$.
Plugging this into the Taylor expansion above we get
--><p>Trong đó xấp xỉ bậc nhất <span class="math notranslate nohighlight">\(f(x+\epsilon)\)</span> được tính bằng giá trị
hàm <span class="math notranslate nohighlight">\(f(x)\)</span> và đạo hàm bậc nhất <span class="math notranslate nohighlight">\(f'(x)\)</span> tại <span class="math notranslate nohighlight">\(x\)</span>. Có lý
khi giả sử rằng di chuyển theo hướng ngược chiều gradient với
<span class="math notranslate nohighlight">\(\epsilon\)</span> nhỏ sẽ làm suy giảm giá trị <span class="math notranslate nohighlight">\(f\)</span>. Để đơn giản hóa
vấn đề, ta cố định sải bước cập nhật (tốc độ học) <span class="math notranslate nohighlight">\(\eta &gt; 0\)</span> và
chọn <span class="math notranslate nohighlight">\(\epsilon = -\eta f'(x)\)</span>. Thay biểu thức này vào khai triển
Taylor ở trên, ta thu được</p>
<div class="math notranslate nohighlight" id="equation-chapter-optimization-gd-vn-0">
<span class="eqno">(11.7.2)<a class="headerlink" href="#equation-chapter-optimization-gd-vn-0" title="Permalink to this equation">¶</a></span>\[f(x - \eta f'(x)) = f(x) - \eta f'^2(x) + \mathcal{O}(\eta^2 f'^2(x)).\]</div>
<!--
If the derivative $f'(x) \neq 0$ does not vanish we make progress since $\eta f'^2(x)>0$.
Moreover, we can always choose $\eta$ small enough for the higher order terms to become irrelevant.
Hence we arrive at
--><div class="line-block">
<div class="line">Nếu đạo hàm <span class="math notranslate nohighlight">\(f'(x) \neq 0\)</span> không tiêu biến, quá trình tối ưu sẽ
có tiến triển do <span class="math notranslate nohighlight">\(\eta f'^2(x)&gt;0\)</span>.</div>
<div class="line">Hơn nữa, chúng ta luôn có thể chọn <span class="math notranslate nohighlight">\(\eta\)</span> đủ nhỏ để loại bỏ các
hạng tử bậc cao hơn trong phép cập nhật. Do đó, ta có</div>
</div>
<div class="math notranslate nohighlight" id="equation-chapter-optimization-gd-vn-1">
<span class="eqno">(11.7.3)<a class="headerlink" href="#equation-chapter-optimization-gd-vn-1" title="Permalink to this equation">¶</a></span>\[f(x - \eta f'(x)) \lessapprox f(x).\]</div>
<!--
This means that, if we use
--><p>Điều này có nghĩa là, nếu chúng ta áp dụng</p>
<div class="math notranslate nohighlight" id="equation-chapter-optimization-gd-vn-2">
<span class="eqno">(11.7.4)<a class="headerlink" href="#equation-chapter-optimization-gd-vn-2" title="Permalink to this equation">¶</a></span>\[x \leftarrow x - \eta f'(x)\]</div>
<!--
to iterate $x$, the value of function $f(x)$ might decline.
Therefore, in gradient descent we first choose an initial value $x$ and a constant $\eta > 0$ and then use them to continuously iterate $x$ until the stop condition is reached,
for example, when the magnitude of the gradient $|f'(x)|$ is small enough or the number of iterations has reached a certain value.
--><p>để cập nhật <span class="math notranslate nohighlight">\(x\)</span>, giá trị của hàm <span class="math notranslate nohighlight">\(f(x)\)</span> có thể giảm. Do đó,
trong thuật toán hạ gradient, đầu tiên chúng ta chọn giá trị khởi tạo
cho <span class="math notranslate nohighlight">\(x\)</span> và hằng số <span class="math notranslate nohighlight">\(\eta &gt; 0\)</span>, từ đó cập nhật giá trị
<span class="math notranslate nohighlight">\(x\)</span> liên tục cho tới khi thỏa mãn điều kiện dừng, ví dụ như khi độ
lớn của gradient <span class="math notranslate nohighlight">\(|f'(x)|\)</span> đủ nhỏ hoặc số lần cập nhật đạt một
ngưỡng nhất định.</p>
<!--
For simplicity we choose the objective function $f(x)=x^2$ to illustrate how to implement gradient descent.
Although we know that $x=0$ is the solution to minimize $f(x)$, we still use this simple function to observe how $x$ changes.
As always, we begin by importing all required modules.
--><p>Để đơn giản hóa vấn đề, chúng ta chọn hàm mục tiêu <span class="math notranslate nohighlight">\(f(x)=x^2\)</span> để
minh họa cách lập trình thuật toán hạ gradient. Ta sử dụng ví dụ đơn
giản này để quan sát cách mà <span class="math notranslate nohighlight">\(x\)</span> thay đổi, dù đã biết rằng
<span class="math notranslate nohighlight">\(x=0\)</span> là nghiệm để cực tiểu hóa <span class="math notranslate nohighlight">\(f(x)\)</span>. Như mọi khi, chúng
ta bắt đầu bằng cách nhập tất cả các mô-đun cần thiết.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="kn">from</span> <span class="nn">d2l</span> <span class="kn">import</span> <span class="n">mxnet</span> <span class="k">as</span> <span class="n">d2l</span>
<span class="kn">from</span> <span class="nn">mxnet</span> <span class="kn">import</span> <span class="n">np</span><span class="p">,</span> <span class="n">npx</span>
<span class="n">npx</span><span class="o">.</span><span class="n">set_np</span><span class="p">()</span>
</pre></div>
</div>
<p>&lt;!–</p>
<p>–&gt;</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">f</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span>  <span class="c1"># Objective function</span>
<span class="n">gradf</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">x</span>  <span class="c1"># Its derivative</span>
</pre></div>
</div>
<!--
Next, we use $x=10$ as the initial value and assume $\eta=0.2$.
Using gradient descent to iterate $x$ for 10 times we can see that, eventually, the value of $x$ approaches the optimal solution.
--><p>Tiếp theo, chúng ta sử dụng <span class="math notranslate nohighlight">\(x=10\)</span> làm giá trị khởi tạo và chọn
<span class="math notranslate nohighlight">\(\eta=0.2\)</span>. Áp dụng thuật toán hạ gradient để cập nhật <span class="math notranslate nohighlight">\(x\)</span>
trong 10 vòng lặp, chúng ta có thể thấy cuối cùng giá trị của <span class="math notranslate nohighlight">\(x\)</span>
cũng tiệm cận nghiệm tối ưu.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">gd</span><span class="p">(</span><span class="n">eta</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="mf">10.0</span>
    <span class="n">results</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">-=</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">gradf</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;epoch 10, x:&#39;</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">results</span>
<span class="n">res</span> <span class="o">=</span> <span class="n">gd</span><span class="p">(</span><span class="mf">0.2</span><span class="p">)</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">epoch</span> <span class="mi">10</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="mf">0.06046617599999997</span>
</pre></div>
</div>
<!--
The progress of optimizing over $x$ can be plotted as follows.
--><p>Đồ thị quá trình tối ưu hóa theo <span class="math notranslate nohighlight">\(x\)</span> được vẽ như sau.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">show_trace</span><span class="p">(</span><span class="n">res</span><span class="p">):</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">res</span><span class="p">)),</span> <span class="nb">abs</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">res</span><span class="p">)))</span>
    <span class="n">f_line</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="n">n</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>
    <span class="n">d2l</span><span class="o">.</span><span class="n">set_figsize</span><span class="p">()</span>
    <span class="n">d2l</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">f_line</span><span class="p">,</span> <span class="n">res</span><span class="p">],</span> <span class="p">[[</span><span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">f_line</span><span class="p">],</span> <span class="p">[</span><span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">res</span><span class="p">]],</span>
             <span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;f(x)&#39;</span><span class="p">,</span> <span class="n">fmts</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="s1">&#39;-o&#39;</span><span class="p">])</span>

<span class="n">show_trace</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>
</pre></div>
</div>
<div class="figure align-default">
<img alt="../_images/output_gd_vn_7f2a08_8_0.svg" src="../_images/output_gd_vn_7f2a08_8_0.svg" /></div>
<!--
### Learning Rate
--><div class="section" id="toc-do-hoc">
<span id="section-gd-learningrate"></span><h3><span class="section-number">11.7.1.1. </span>Tốc độ học<a class="headerlink" href="#toc-do-hoc" title="Permalink to this headline">¶</a></h3>
<!--
The learning rate $\eta$ can be set by the algorithm designer.
If we use a learning rate that is too small, it will cause $x$ to update very slowly, requiring more iterations to get a better solution.
To show what happens in such a case, consider the progress in the same optimization problem for $\eta = 0.05$.
As we can see, even after 10 steps we are still very far from the optimal solution.
--><p>Tốc độ học <span class="math notranslate nohighlight">\(\eta\)</span> có thể được thiết lập khi thiết kế thuật toán.
Nếu ta sử dụng tốc độ học quá nhỏ thì <span class="math notranslate nohighlight">\(x\)</span> sẽ được cập nhật rất
chậm, đòi hỏi số bước cập nhật nhiều hơn để thu được nghiệm tốt hơn. Để
minh họa, hãy xem xét quá trình học trong cùng bài toán tối ưu ở phía
trên với <span class="math notranslate nohighlight">\(\eta = 0.05\)</span>. Như chúng ta có thể thấy, ngay cả sau 10
bước cập nhật, chúng ta vẫn còn ở rất xa nghiệm tối ưu.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">show_trace</span><span class="p">(</span><span class="n">gd</span><span class="p">(</span><span class="mf">0.05</span><span class="p">))</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">epoch</span> <span class="mi">10</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="mf">3.4867844009999995</span>
</pre></div>
</div>
<div class="figure align-default">
<img alt="../_images/output_gd_vn_7f2a08_10_1.svg" src="../_images/output_gd_vn_7f2a08_10_1.svg" /></div>
<!--
Conversely, if we use an excessively high learning rate, $\left|\eta f'(x)\right|$ might be too large for the first-order Taylor expansion formula.
That is, the term $\mathcal{O}(\eta^2 f'^2(x))$ in :eqref:`gd-taylor` might become significant.
In this case, we cannot guarantee that the iteration of $x$ will be able to lower the value of $f(x)$.
For example, when we set the learning rate to $\eta=1.1$, $x$ overshoots the optimal solution $x=0$ and gradually diverges.
--><p>Ngược lại, nếu ta sử dụng tốc độ học quá cao, giá trị
<span class="math notranslate nohighlight">\(\left|\eta f'(x)\right|\)</span> có thể rất lớn trong khai triển Taylor
bậc nhất. Cụ thể, hạng tử <span class="math notranslate nohighlight">\(\mathcal{O}(\eta^2 f'^2(x))\)</span> trong
:eqref: <code class="docutils literal notranslate"><span class="pre">gd-taylor</span></code> sẽ có thể có giá trị lớn. Trong trường hợp này, ta
không thể đảm bảo rằng việc cập nhật <span class="math notranslate nohighlight">\(x\)</span> sẽ có thể làm suy giảm
giá trị của <span class="math notranslate nohighlight">\(f(x)\)</span>. Ví dụ, khi chúng ta thiết lập tốc độ học
<span class="math notranslate nohighlight">\(\eta=1.1\)</span>, <span class="math notranslate nohighlight">\(x\)</span> sẽ lệch rất xa so với nghiệm tối ưu
<span class="math notranslate nohighlight">\(x=0\)</span> và dần dần phân kỳ.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">show_trace</span><span class="p">(</span><span class="n">gd</span><span class="p">(</span><span class="mf">1.1</span><span class="p">))</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">epoch</span> <span class="mi">10</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="mf">61.917364224000096</span>
</pre></div>
</div>
<div class="figure align-default">
<img alt="../_images/output_gd_vn_7f2a08_12_1.svg" src="../_images/output_gd_vn_7f2a08_12_1.svg" /></div>
<!--
### Local Minima
--></div>
<div class="section" id="cuc-tieu">
<h3><span class="section-number">11.7.1.2. </span>Cực Tiểu<a class="headerlink" href="#cuc-tieu" title="Permalink to this headline">¶</a></h3>
<!--
To illustrate what happens for nonconvex functions consider the case of $f(x) = x \cdot \cos c x$.
This function has infinitely many local minima.
Depending on our choice of learning rate and depending on how well conditioned the problem is, we may end up with one of many solutions.
The example below illustrates how an (unrealistically) high learning rate will lead to a poor local minimum.
--><p>Để minh họa quá trình học các hàm không lồi, ta xem xét trường hợp
<span class="math notranslate nohighlight">\(f(x) = x \cdot \cos c x\)</span>. Hàm này có vô số cực tiểu. Tùy thuộc
vào tốc độ học được chọn và điều kiện của bài toán, chúng ta có thể thu
được một trong số rất nhiều nghiệm. Ví dụ dưới đây minh họa việc thiết
lập tốc độ học quá cao (không thực tế) sẽ dẫn đến điểm cực tiểu không
tốt.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">c</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="mf">0.15</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span>
<span class="n">f</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">c</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span>
<span class="n">gradf</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">c</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="n">c</span> <span class="o">*</span> <span class="n">x</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">c</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span>
<span class="n">show_trace</span><span class="p">(</span><span class="n">gd</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">epoch</span> <span class="mi">10</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="o">-</span><span class="mf">1.5281651</span>
</pre></div>
</div>
<div class="figure align-default">
<img alt="../_images/output_gd_vn_7f2a08_14_1.svg" src="../_images/output_gd_vn_7f2a08_14_1.svg" /></div>
<!--
## Multivariate Gradient Descent
--></div>
</div>
<div class="section" id="ha-gradient-da-bien">
<h2><span class="section-number">11.7.2. </span>Hạ Gradient Đa biến<a class="headerlink" href="#ha-gradient-da-bien" title="Permalink to this headline">¶</a></h2>
<!--
Now that we have a better intuition of the univariate case, let us consider the situation where $\mathbf{x} \in \mathbb{R}^d$.
That is, the objective function $f: \mathbb{R}^d \to \mathbb{R}$ maps vectors into scalars. Correspondingly its gradient is multivariate, too.
It is a vector consisting of $d$ partial derivatives:
--><p>Bây giờ chúng ta đã có trực quan tốt hơn về trường hợp đơn biến, ta hãy
xem xét trường hợp trong đó <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathbb{R}^d\)</span>. Cụ thể,
hàm mục tiêu <span class="math notranslate nohighlight">\(f: \mathbb{R}^d \to \mathbb{R}\)</span> ánh xạ các vector
tới các giá trị vô hướng. Gradient tương ứng cũng là đa biến, là một
vector gồm <span class="math notranslate nohighlight">\(d\)</span> đạo hàm riêng:</p>
<div class="math notranslate nohighlight" id="equation-chapter-optimization-gd-vn-3">
<span class="eqno">(11.7.5)<a class="headerlink" href="#equation-chapter-optimization-gd-vn-3" title="Permalink to this equation">¶</a></span>\[\nabla f(\mathbf{x}) = \bigg[\frac{\partial f(\mathbf{x})}{\partial x_1}, \frac{\partial f(\mathbf{x})}{\partial x_2}, \ldots, \frac{\partial f(\mathbf{x})}{\partial x_d}\bigg]^\top.\]</div>
<!--
Each partial derivative element $\partial f(\mathbf{x})/\partial x_i$ in the gradient indicates the rate of change of $f$ at $\mathbf{x}$ with respect to the input $x_i$.
As before in the univariate case we can use the corresponding Taylor approximation for multivariate functions to get some idea of what we should do.
In particular, we have that
--><p>Mỗi đạo hàm riêng <span class="math notranslate nohighlight">\(\partial f(\mathbf{x})/\partial x_i\)</span> trong
gradient biểu diễn tốc độ thay đổi theo <span class="math notranslate nohighlight">\(x_i\)</span> của <span class="math notranslate nohighlight">\(f\)</span> tại
<span class="math notranslate nohighlight">\(\mathbf{x}\)</span>. Như trong trường hợp đơn biến giới thiệu ở phần
trước, ta sử dụng khai triển Taylor tương ứng cho các hàm đa biến. Cụ
thể, ta có</p>
<div class="math notranslate nohighlight" id="equation-gd-multi-taylor">
<span class="eqno">(11.7.6)<a class="headerlink" href="#equation-gd-multi-taylor" title="Permalink to this equation">¶</a></span>\[f(\mathbf{x} + \mathbf{\epsilon}) = f(\mathbf{x}) + \mathbf{\epsilon}^\top \nabla f(\mathbf{x}) + \mathcal{O}(\|\mathbf{\epsilon}\|^2).\]</div>
<!--
In other words, up to second order terms in $\mathbf{\epsilon}$ the direction of steepest descent is given by the negative gradient $-\nabla f(\mathbf{x})$.
Choosing a suitable learning rate $\eta > 0$ yields the prototypical gradient descent algorithm:
--><p>Nói cách khác, chiều giảm mạnh nhất được cho bởi gradient âm
<span class="math notranslate nohighlight">\(-\nabla f(\mathbf{x})\)</span>, các hạng tử từ bậc hai trở lên trong
<span class="math notranslate nohighlight">\(\mathbf{\epsilon}\)</span> có thể bỏ qua. Chọn một tốc độ học phù hợp
<span class="math notranslate nohighlight">\(\eta &gt; 0\)</span>, ta được thuật toán hạ gradient nguyên bản dưới đây:</p>
<div class="math notranslate nohighlight" id="equation-chapter-optimization-gd-vn-4">
<span class="eqno">(11.7.7)<a class="headerlink" href="#equation-chapter-optimization-gd-vn-4" title="Permalink to this equation">¶</a></span>\[\mathbf{x} \leftarrow \mathbf{x} - \eta \nabla f(\mathbf{x}).\]</div>
<!--
To see how the algorithm behaves in practice let us construct an objective function $f(\mathbf{x})=x_1^2+2x_2^2$ with a two-dimensional vector $\mathbf{x} = [x_1, x_2]^\top$ as input and a scalar as output.
The gradient is given by $\nabla f(\mathbf{x}) = [2x_1, 4x_2]^\top$.
We will observe the trajectory of $\mathbf{x}$ by gradient descent from the initial position $[-5, -2]$.
We need two more helper functions.
The first uses an update function and applies it $20$ times to the initial value.
The second helper visualizes the trajectory of $\mathbf{x}$.
--><p>Để xem thuật toán hoạt động như thế nào trong thực tế, ta hãy xây dựng
một hàm mục tiêu <span class="math notranslate nohighlight">\(f(\mathbf{x})=x_1^2+2x_2^2\)</span> với đầu vào là
vector hai chiều <span class="math notranslate nohighlight">\(\mathbf{x} = [x_1, x_2]^\top\)</span> và đầu ra là một
số vô hướng. Gradient được cho bởi
<span class="math notranslate nohighlight">\(\nabla f(\mathbf{x}) = [2x_1, 4x_2]^\top\)</span>. Ta sẽ quan sát đường
đi của <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> được sinh bởi thuật toán hạ gradient bắt đầu
từ vị trí <span class="math notranslate nohighlight">\([-5, -2]\)</span>. Chúng ta cần thêm hai hàm hỗ trợ. Hàm đầu
tiên là hàm cập nhật và được sử dụng <span class="math notranslate nohighlight">\(20\)</span> lần cho giá trị khởi tạo
ban đầu. Hàm thứ hai là hàm vẽ biểu đồ đường đi của <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">train_2d</span><span class="p">(</span><span class="n">trainer</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="mi">20</span><span class="p">):</span>  <span class="c1">#@save</span>
    <span class="sd">&quot;&quot;&quot;Optimize a 2-dim objective function with a customized trainer.&quot;&quot;&quot;</span>
    <span class="c1"># s1 and s2 are internal state variables and will</span>
    <span class="c1"># be used later in the chapter</span>
    <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">s1</span><span class="p">,</span> <span class="n">s2</span> <span class="o">=</span> <span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
    <span class="n">results</span> <span class="o">=</span> <span class="p">[(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">steps</span><span class="p">):</span>
        <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">s1</span><span class="p">,</span> <span class="n">s2</span> <span class="o">=</span> <span class="n">trainer</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">s1</span><span class="p">,</span> <span class="n">s2</span><span class="p">)</span>
        <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">results</span>

<span class="k">def</span> <span class="nf">show_trace_2d</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">results</span><span class="p">):</span>  <span class="c1">#@save</span>
    <span class="sd">&quot;&quot;&quot;Show the trace of 2D variables during optimization.&quot;&quot;&quot;</span>
    <span class="n">d2l</span><span class="o">.</span><span class="n">set_figsize</span><span class="p">()</span>
    <span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="o">*</span><span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">results</span><span class="p">),</span> <span class="s1">&#39;-o&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;#ff7f0e&#39;</span><span class="p">)</span>
    <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mf">5.5</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">),</span>
                          <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">))</span>
    <span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">f</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">),</span> <span class="n">colors</span><span class="o">=</span><span class="s1">&#39;#1f77b4&#39;</span><span class="p">)</span>
    <span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x1&#39;</span><span class="p">)</span>
    <span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;x2&#39;</span><span class="p">)</span>
</pre></div>
</div>
<!--
Next, we observe the trajectory of the optimization variable $\mathbf{x}$ for learning rate $\eta = 0.1$.
We can see that after 20 steps the value of $\mathbf{x}$ approaches its minimum at $[0, 0]$.
Progress is fairly well-behaved albeit rather slow.
--><p>Tiếp theo, chúng ta sẽ quan sát quỹ đạo của biến tối ưu hóa
<span class="math notranslate nohighlight">\(\mathbf{x}\)</span> với tốc độ học <span class="math notranslate nohighlight">\(\eta = 0.1\)</span>. Chúng ta có thể
thấy rằng sau 20 bước, giá trị <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> đã đạt cực tiểu tại
<span class="math notranslate nohighlight">\([0, 0]\)</span>. Quá trình khá tốt mặc dù hơi chậm.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">f</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">:</span> <span class="n">x1</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">x2</span> <span class="o">**</span> <span class="mi">2</span>  <span class="c1"># Objective</span>
<span class="n">gradf</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">:</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">x1</span><span class="p">,</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">x2</span><span class="p">)</span>  <span class="c1"># Gradient</span>

<span class="k">def</span> <span class="nf">gd</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">s1</span><span class="p">,</span> <span class="n">s2</span><span class="p">):</span>
    <span class="p">(</span><span class="n">g1</span><span class="p">,</span> <span class="n">g2</span><span class="p">)</span> <span class="o">=</span> <span class="n">gradf</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span>  <span class="c1"># Compute gradient</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">x1</span> <span class="o">-</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">g1</span><span class="p">,</span> <span class="n">x2</span> <span class="o">-</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">g2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>  <span class="c1"># Update variables</span>

<span class="n">eta</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">show_trace_2d</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">train_2d</span><span class="p">(</span><span class="n">gd</span><span class="p">))</span>
</pre></div>
</div>
<div class="figure align-default">
<img alt="../_images/output_gd_vn_7f2a08_18_0.svg" src="../_images/output_gd_vn_7f2a08_18_0.svg" /></div>
<!--
## Adaptive Methods
--></div>
<div class="section" id="nhung-phuong-phap-thich-nghi">
<h2><span class="section-number">11.7.3. </span>Những Phương pháp Thích nghi<a class="headerlink" href="#nhung-phuong-phap-thich-nghi" title="Permalink to this headline">¶</a></h2>
<!--
As we could see in :numref:`section_gd-learningrate`, getting the learning rate $\eta$ "just right" is tricky.
If we pick it too small, we make no progress.
If we pick it too large, the solution oscillates and in the worst case it might even diverge.
What if we could determine $\eta$ automatically or get rid of having to select a step size at all?
Second order methods that look not only at the value and gradient of the objective but also at its *curvature* can help in this case.
While these methods cannot be applied to deep learning directly due to the computational cost,
they provide useful intuition into how to design advanced optimization algorithms that mimic many of the desirable properties of the algorithms outlined below.
--><p>Như chúng ta có thể thấy ở <a class="reference internal" href="#section-gd-learningrate"><span class="std std-numref">Section 11.7.1.1</span></a>, chọn tốc
độ học <span class="math notranslate nohighlight">\(\eta\)</span> “vừa đủ” rất khó. Nếu chọn giá trị quá nhỏ, ta sẽ
không có tiến triển. Nếu chọn giá trị quá lớn, nghiệm sẽ dao động và
trong trường hợp tệ nhất, thậm chí sẽ phân kỳ. Sẽ ra sao nếu chúng ta có
thể chọn <span class="math notranslate nohighlight">\(\eta\)</span> một cách tự động, hoặc giả như loại bỏ được việc
chọn kích thước bước? Các phương pháp bậc hai không chỉ dựa vào giá trị
và gradient của hàm mục tiêu mà còn dựa vào “độ cong” của hàm, từ đó có
thể điều chỉnh tốc độ học. Dù những phương pháp này không thể áp dụng
vào học sâu một cách trực tiếp do chi phí tính toán lớn, chúng đem đến
những gợi ý hữu ích để thiết kế các thuật toán tối ưu cao cấp hơn, mang
nhiều tính chất mong muốn dựa trên các thuật toán dưới đây.</p>
<!--
### Newton's Method
--><div class="section" id="phuong-phap-newton">
<h3><span class="section-number">11.7.3.1. </span>Phương pháp Newton<a class="headerlink" href="#phuong-phap-newton" title="Permalink to this headline">¶</a></h3>
<!--
Reviewing the Taylor expansion of $f$ there is no need to stop after the first term.
In fact, we can write it as
--><p>Trong khai triển Taylor của <span class="math notranslate nohighlight">\(f\)</span>, ta không cần phải dừng ngay sau
số hạng đầu tiên. Trên thực tế, ta có thể viết lại như sau</p>
<div class="math notranslate nohighlight" id="equation-gd-hot-taylor">
<span class="eqno">(11.7.8)<a class="headerlink" href="#equation-gd-hot-taylor" title="Permalink to this equation">¶</a></span>\[f(\mathbf{x} + \mathbf{\epsilon}) = f(\mathbf{x}) + \mathbf{\epsilon}^\top \nabla f(\mathbf{x}) + \frac{1}{2} \mathbf{\epsilon}^\top \nabla \nabla^\top f(\mathbf{x}) \mathbf{\epsilon} + \mathcal{O}(\|\mathbf{\epsilon}\|^3).\]</div>
<!--
To avoid cumbersome notation we define $H_f := \nabla \nabla^\top f(\mathbf{x})$ to be the *Hessian* of $f$.
This is a $d \times d$ matrix. For small $d$ and simple problems $H_f$ is easy to compute.
For deep networks, on the other hand, $H_f$ may be prohibitively large, due to the cost of storing $\mathcal{O}(d^2)$ entries.
Furthermore it may be too expensive to compute via backprop as we would need to apply backprop to the backpropagation call graph.
For now let us ignore such considerations and look at what algorithm we'd get.
--><p>Để tránh việc kí hiệu quá nhiều, ta định nghĩa
<span class="math notranslate nohighlight">\(H_f := \nabla \nabla^\top f(\mathbf{x})\)</span> là <em>ma trận Hessian</em> của
<span class="math notranslate nohighlight">\(f\)</span>. Đây là ma trận kích thước <span class="math notranslate nohighlight">\(d \times d\)</span>. Với <span class="math notranslate nohighlight">\(d\)</span>
nhỏ và trong các bài toán đơn giản, ta sẽ dễ tính được <span class="math notranslate nohighlight">\(H_f\)</span>.
Nhưng với các mạng sâu, kích thước của <span class="math notranslate nohighlight">\(H_f\)</span> có thể cực lớn, do
chi phí lưu trữ bậc hai <span class="math notranslate nohighlight">\(\mathcal{O}(d^2)\)</span>. Hơn nữa việc tính toán
lan truyền ngược có thể đòi hỏi rất nhiều chi phí tính toán. Tạm thời
hãy bỏ qua những lưu ý đó và nhìn vào thuật toán mà ta có được.</p>
<!--
After all, the minimum of $f$ satisfies $\nabla f(\mathbf{x}) = 0$.
Taking derivatives of :eqref:`gd-hot-taylor` with regard to $\mathbf{\epsilon}$ and ignoring higher order terms we arrive at
--><p>Suy cho cùng, cực tiểu của <span class="math notranslate nohighlight">\(f\)</span> sẽ thỏa
<span class="math notranslate nohighlight">\(\nabla f(\mathbf{x}) = 0\)</span>. Lấy các đạo hàm của
<a class="reference internal" href="#equation-gd-hot-taylor">(11.7.8)</a> theo <span class="math notranslate nohighlight">\(\mathbf{\epsilon}\)</span> và bỏ qua các số
hạng bậc cao ta thu được</p>
<div class="math notranslate nohighlight" id="equation-chapter-optimization-gd-vn-5">
<span class="eqno">(11.7.9)<a class="headerlink" href="#equation-chapter-optimization-gd-vn-5" title="Permalink to this equation">¶</a></span>\[\nabla f(\mathbf{x}) + H_f \mathbf{\epsilon} = 0 \text{ và~do~đó }
\mathbf{\epsilon} = -H_f^{-1} \nabla f(\mathbf{x}).\]</div>
<!--
That is, we need to invert the Hessian $H_f$ as part of the optimization problem.
--><p>Nghĩa là, ta cần phải nghịch đảo ma trận Hessian <span class="math notranslate nohighlight">\(H_f\)</span> như một
phần của bài toán tối ưu hóa.</p>
<!--
For $f(x) = \frac{1}{2} x^2$ we have $\nabla f(x) = x$ and $H_f = 1$.
Hence for any $x$ we obtain $\epsilon = -x$.
In other words, a single step is sufficient to converge perfectly without the need for any adjustment!
Alas, we got a bit lucky here since the Taylor expansion was exact.
Let us see what happens in other problems.
--><p>Với <span class="math notranslate nohighlight">\(f(x) = \frac{1}{2} x^2\)</span> ta có <span class="math notranslate nohighlight">\(\nabla f(x) = x\)</span> và
<span class="math notranslate nohighlight">\(H_f = 1\)</span>. Do đó với <span class="math notranslate nohighlight">\(x\)</span> bất kỳ, ta đều thu được
<span class="math notranslate nohighlight">\(\epsilon = -x\)</span>. Nói cách khác, một bước đơn lẻ là đã đủ để hội tụ
một cách hoàn hảo mà không cần bất kỳ tinh chỉnh nào! Chúng ta khá may
mắn ở đây vì khai triển Taylor không cần xấp xỉ. Hãy xem thử điều gì sẽ
xảy ra với các bài toán khác.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">c</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">f</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">cosh</span><span class="p">(</span><span class="n">c</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span>  <span class="c1"># Objective</span>
<span class="n">gradf</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">c</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sinh</span><span class="p">(</span><span class="n">c</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span>  <span class="c1"># Derivative</span>
<span class="n">hessf</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">c</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">cosh</span><span class="p">(</span><span class="n">c</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span>  <span class="c1"># Hessian</span>

<span class="k">def</span> <span class="nf">newton</span><span class="p">(</span><span class="n">eta</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="mf">10.0</span>
    <span class="n">results</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">-=</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">gradf</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">/</span> <span class="n">hessf</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;epoch 10, x:&#39;</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">results</span>
<span class="n">show_trace</span><span class="p">(</span><span class="n">newton</span><span class="p">())</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">epoch</span> <span class="mi">10</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="mf">0.0</span>
</pre></div>
</div>
<div class="figure align-default">
<img alt="../_images/output_gd_vn_7f2a08_20_1.svg" src="../_images/output_gd_vn_7f2a08_20_1.svg" /></div>
<!--
Now let us see what happens when we have a *nonconvex* function, such as $f(x) = x \cos(c x)$.
After all, note that in Newton's method we end up dividing by the Hessian.
This means that if the second derivative is *negative* we would walk into the direction of *increasing* $f$.
That is a fatal flaw of the algorithm.
Let us see what happens in practice.
--><p>Giờ hãy xem điều gì xảy ra với một hàm <em>không lồi</em>, ví dụ như
<span class="math notranslate nohighlight">\(f(x) = x \cos(c x)\)</span>. Sau tất cả, hãy lưu ý rằng trong phương pháp
Newton, chúng ta cuối cùng sẽ phải chia cho ma trận Hessian. Điều này
nghĩa là nếu đạo hàm bậc hai là <em>âm</em> thì chúng ta phải đi theo hướng
<em>tăng</em> <span class="math notranslate nohighlight">\(f\)</span>. Đó là khiếm khuyết chết người của thuật toán này. Hãy
xem điều gì sẽ xảy ra trong thực tế.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">c</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="mf">0.15</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span>
<span class="n">f</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">c</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span>
<span class="n">gradf</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">c</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="n">c</span> <span class="o">*</span> <span class="n">x</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">c</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span>
<span class="n">hessf</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">c</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">c</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="n">x</span> <span class="o">*</span> <span class="n">c</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">c</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span>

<span class="n">show_trace</span><span class="p">(</span><span class="n">newton</span><span class="p">())</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">epoch</span> <span class="mi">10</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="mf">26.834133</span>
</pre></div>
</div>
<div class="figure align-default">
<img alt="../_images/output_gd_vn_7f2a08_22_1.svg" src="../_images/output_gd_vn_7f2a08_22_1.svg" /></div>
<!--
This went spectacularly wrong.
How can we fix it? One way would be to "fix" the Hessian by taking its absolute value instead.
Another strategy is to bring back the learning rate.
This seems to defeat the purpose, but not quite.
Having second order information allows us to be cautious whenever the curvature is large and to take longer steps whenever the objective is flat.
Let us see how this works with a slightly smaller learning rate, say $\eta = 0.5$. As we can see, we have quite an efficient algorithm.
--><p>Kết quả trả về là cực kỳ sai. Có một cách khắc phục là “sửa” ma trận
Hessian bằng cách lấy giá trị tuyệt đối của nó. Một chiến lược khác là
đưa tốc độ học trở lại. Điều này có vẻ sẽ phá hỏng mục tiêu ban đầu
nhưng không hẳn. Có được thông tin bậc hai sẽ cho phép chúng ta thận
trọng bất cứ khi nào độ cong trở nên lớn và cho phép thực hiện các bước
dài hơn mỗi khi hàm mục tiêu phẳng. Hãy xem nó hoạt động như thế nào với
một tốc độ học khá nhỏ, <span class="math notranslate nohighlight">\(\eta = 0.5\)</span> chẳng hạn. Như ta có thể
thấy, chúng ta có một thuật toán khá hiệu quả.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">show_trace</span><span class="p">(</span><span class="n">newton</span><span class="p">(</span><span class="mf">0.5</span><span class="p">))</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">epoch</span> <span class="mi">10</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="mf">7.26986</span>
</pre></div>
</div>
<div class="figure align-default">
<img alt="../_images/output_gd_vn_7f2a08_24_1.svg" src="../_images/output_gd_vn_7f2a08_24_1.svg" /></div>
<!--
### Convergence Analysis
--></div>
<div class="section" id="phan-tich-hoi-tu">
<h3><span class="section-number">11.7.3.2. </span>Phân tích Hội tụ<a class="headerlink" href="#phan-tich-hoi-tu" title="Permalink to this headline">¶</a></h3>
<!--
We only analyze the convergence rate for convex and three times differentiable $f$, where at its minimum $x^*$ the second derivative is nonzero, i.e., where $f''(x^*) > 0$.
The multivariate proof is a straightforward extension of the argument below and omitted since it doesn't help us much in terms of intuition.
--><p>Chúng ta sẽ chỉ phân tích tốc độ hội tụ đối với hàm <span class="math notranslate nohighlight">\(f\)</span> lồi và khả
vi ba lần, đây là hàm số có đạo hàm bậc hai tại cực tiểu <span class="math notranslate nohighlight">\(x^*\)</span>
khác không (<span class="math notranslate nohighlight">\(f''(x^*) &gt; 0\)</span>).</p>
<!--
Denote by $x_k$ the value of $x$ at the $k$-th iteration and let $e_k := x_k - x^*$ be the distance from optimality.
By Taylor series expansion we have that the condition $f'(x^*) = 0$ can be written as
--><p>Đặt <span class="math notranslate nohighlight">\(x_k\)</span> là giá trị của <span class="math notranslate nohighlight">\(x\)</span> tại vòng lặp thứ <span class="math notranslate nohighlight">\(k\)</span> và
<span class="math notranslate nohighlight">\(e_k := x_k - x^*\)</span> là khoảng cách đến điểm tối ưu. Theo khai triển
Taylor, điều kiện <span class="math notranslate nohighlight">\(f'(x^*) = 0\)</span> được viết lại thành</p>
<div class="math notranslate nohighlight" id="equation-chapter-optimization-gd-vn-6">
<span class="eqno">(11.7.10)<a class="headerlink" href="#equation-chapter-optimization-gd-vn-6" title="Permalink to this equation">¶</a></span>\[0 = f'(x_k - e_k) = f'(x_k) - e_k f''(x_k) + \frac{1}{2} e_k^2 f'''(\xi_k).\]</div>
<!--
This holds for some $\xi_k \in [x_k - e_k, x_k]$. Recall that we have the update $x_{k+1} = x_k - f'(x_k) / f''(x_k)$.
Dividing the above expansion by $f''(x_k)$ yields
--><p>Điều này đúng với một vài <span class="math notranslate nohighlight">\(\xi_k \in [x_k - e_k, x_k]\)</span>. Hãy nhớ
rằng chúng ta có công thức cập nhật
<span class="math notranslate nohighlight">\(x_{k+1} = x_k - f'(x_k) / f''(x_k)\)</span>. Chia khai triển Taylor ở
trên cho <span class="math notranslate nohighlight">\(f''(x_k)\)</span>, ta thu được</p>
<div class="math notranslate nohighlight" id="equation-chapter-optimization-gd-vn-7">
<span class="eqno">(11.7.11)<a class="headerlink" href="#equation-chapter-optimization-gd-vn-7" title="Permalink to this equation">¶</a></span>\[e_k - f'(x_k) / f''(x_k) = \frac{1}{2} e_k^2 f'''(\xi_k) / f''(x_k).\]</div>
<!--
Plugging in the update equations leads to the following bound $e_{k+1} \leq e_k^2 f'''(\xi_k) / f'(x_k)$.
Consequently, whenever we are in a region of bounded $f'''(\xi_k) / f''(x_k) \leq c$, we have a quadratically decreasing error $e_{k+1} \leq c e_k^2$.
--><p>Thay vào phương trình cập nhật sẽ dẫn đến ràng buộc
<span class="math notranslate nohighlight">\(e_{k+1} \leq e_k^2 f'''(\xi_k) / f'(x_k)\)</span>. Do đó, khi nằm trong
miền ràng buộc <span class="math notranslate nohighlight">\(f'''(\xi_k) / f''(x_k) \leq c\)</span>, ta sẽ có sai số
giảm theo bình phương <span class="math notranslate nohighlight">\(e_{k+1} \leq c e_k^2\)</span>.</p>
<!--
As an aside, optimization researchers call this *linear* convergence, whereas a condition such as $e_{k+1} \leq \alpha e_k$ would be called a *constant* rate of convergence.
Note that this analysis comes with a number of caveats: We do not really have much of a guarantee when we will reach the region of rapid convergence.
Instead, we only know that once we reach it, convergence will be very quick.
Second, this requires that $f$ is well-behaved up to higher order derivatives.
It comes down to ensuring that $f$ does not have any "surprising" properties in terms of how it might change its values.
--><p>Bên cạnh đó, các nhà nghiên cứu tối ưu hóa gọi đây là hội tụ <em>tuyến
tính</em>, còn điều kiện <span class="math notranslate nohighlight">\(e_{k+1} \leq \alpha e_k\)</span> được gọi là tốc độ
hội tụ <em>không đổi</em>. Lưu ý rằng phân tích này đi kèm với một số lưu ý:
Chúng ta không thực sự biết rằng khi nào mình sẽ tiến tới được vùng hội
tụ nhanh. Thay vào đó, ta chỉ biết rằng một khi đến được đó, việc hội tụ
sẽ xảy ra rất nhanh chóng. Thêm nữa, điều này yêu cầu <span class="math notranslate nohighlight">\(f\)</span> được xử
lý tốt ở các đạo hàm bậc cao. Nó đảm bảo không có bất cứ một tính chất
“bất ngờ” nào của <span class="math notranslate nohighlight">\(f\)</span> có thể dẫn đến sự thay đổi giá trị của nó.</p>
<!--
### Preconditioning
--></div>
<div class="section" id="tien-dieu-kien">
<h3><span class="section-number">11.7.3.3. </span>Tiền Điều kiện<a class="headerlink" href="#tien-dieu-kien" title="Permalink to this headline">¶</a></h3>
<!--
Quite unsurprisingly computing and storing the full Hessian is very expensive.
It is thus desirable to find alternatives.
One way to improve matters is by avoiding to compute the Hessian in its entirety but only compute the *diagonal* entries.
While this is not quite as good as the full Newton method, it is still much better than not using it.
Moreover, estimates for the main diagonal elements are what drives some of the innovation in stochastic gradient descent optimization algorithms.
This leads to update algorithms of the form
--><p>Không có gì ngạc nhiên khi việc tính toán và lưu trữ toàn bộ ma trận
Hessian là rất tốn kém. Do đó ta cần tìm kiếm một phương pháp thay thế.
Một cách để cải thiện vấn đề này là tránh tính toán toàn bộ ma trận
Hessian, chỉ tính toán các giá trị thuộc <em>đường chéo</em>. Mặc dù cách trên
không tốt bằng phương pháp Newton hoàn chỉnh nhưng vẫn tốt hơn nhiều so
với không sử dụng nó. Hơn nữa, ước lượng các giá trị đường chéo chính là
thứ thúc đẩy sự đổi mới trong các thuật toán tối ưu hóa hạ gradient ngẫu
nhiên. Thuật toán cập nhật sẽ có dạng</p>
<div class="math notranslate nohighlight" id="equation-chapter-optimization-gd-vn-8">
<span class="eqno">(11.7.12)<a class="headerlink" href="#equation-chapter-optimization-gd-vn-8" title="Permalink to this equation">¶</a></span>\[\mathbf{x} \leftarrow \mathbf{x} - \eta \mathrm{diag}(H_f)^{-1} \nabla \mathbf{x}.\]</div>
<!--
To see why this might be a good idea consider a situation where one variable denotes height in millimeters and the other one denotes height in kilometers.
Assuming that for both the natural scale is in meters we have a terrible mismatch in parameterizations.
Using preconditioning removes this.
Effectively preconditioning with gradient descent amounts to selecting a different learning rate for each coordinate.
--><p>Để thấy tại sao điều này có thể là một ý tưởng tốt, ta ví dụ có hai biến
số biểu thị chiều cao, một biến với đơn vị mm, biến còn lại với đơn vị
km. Với cả hai đơn vị đo, khi quy đổi ra mét, chúng ta đều có sự sai
lệch lớn trong việc tham số hóa. Sử dụng tiền điều kiện sẽ loại bỏ vấn
đề này. Tiền điều kiện một cách hiệu quả cùng hạ gradient giúp chọn ra
các tốc độ học khác nhau cho từng trục tọa độ.</p>
<!--
### Gradient Descent with Line Search
--></div>
<div class="section" id="ha-gradient-cung-tim-kiem-duong-thang">
<h3><span class="section-number">11.7.3.4. </span>Hạ gradient cùng Tìm kiếm Đường thẳng<a class="headerlink" href="#ha-gradient-cung-tim-kiem-duong-thang" title="Permalink to this headline">¶</a></h3>
<!--
One of the key problems in gradient descent was that we might overshoot the goal or make insufficient progress.
A simple fix for the problem is to use line search in conjunction with gradient descent.
That is, we use the direction given by $\nabla f(\mathbf{x})$ and then perform binary search as to which step length $\eta$ minimizes $f(\mathbf{x} - \eta \nabla f(\mathbf{x}))$.
--><div class="line-block">
<div class="line">Một trong những vấn đề chính của hạ gradient là chúng ta có thể vượt
quá khỏi mục tiêu hoặc không đạt đủ sự tiến bộ. Có một cách khắc phục
đơn giản cho vấn đề này là sử dụng tìm kiếm đường thẳng (<em>line
search</em>) kết hợp với hạ gradient.</div>
<div class="line">Chúng ta sử dụng hướng được cho bởi <span class="math notranslate nohighlight">\(\nabla f(\mathbf{x})\)</span> và
sau đó dùng tìm kiếm nhị phân để tìm ra độ dài bước <span class="math notranslate nohighlight">\(\eta\)</span> có
thể cực tiểu hóa <span class="math notranslate nohighlight">\(f(\mathbf{x} - \eta \nabla f(\mathbf{x}))\)</span>.</div>
</div>
<!--
This algorithm converges rapidly (for an analysis and proof see e.g., :cite:`Boyd.Vandenberghe.2004`).
However, for the purpose of deep learning this is not quite so feasible, since each step of the line search would require us to evaluate the objective function on the entire dataset.
This is way too costly to accomplish.
--><p>Thuật toán này sẽ hội tụ nhanh chóng (xem phân tích và chứng minh ở
<a class="bibtex reference internal" href="../chapter_references/zreferences.html#boyd-vandenberghe-2004" id="id2">[Boyd &amp; Vandenberghe, 2004]</a>). Tuy nhiên, đối với mục đích của học
sâu thì nó không thực sự khả thi, lý do là mỗi bước của tìm kiếm đường
thẳng sẽ yêu cầu chúng ta ước lượng hàm mục tiêu trên toàn bộ tập dữ
liệu. Điều này quá tốn kém để có thể thực hiện.</p>
<!--
## Summary
--></div>
</div>
<div class="section" id="tong-ket">
<h2><span class="section-number">11.7.4. </span>Tổng kết<a class="headerlink" href="#tong-ket" title="Permalink to this headline">¶</a></h2>
<!--
* Learning rates matter. Too large and we diverge, too small and we do not make progress.
* Gradient descent can get stuck in local minima.
* In high dimensions adjusting learning the learning rate is complicated.
* Preconditioning can help with scale adjustment.
* Newton's method is a lot faster *once* it has started working properly in convex problems.
* Beware of using Newton's method without any adjustments for nonconvex problems.
--><ul class="simple">
<li>Tốc độ học rất quan trọng. Quá lớn sẽ khiến việc tối ưu hóa phân kỳ,
quá nhỏ sẽ không thu được sự tiến bộ nào.</li>
<li>Hạ gradient có thể bị kẹt tại cực tiểu cục bộ.</li>
<li>Trong bài toán nhiều chiều, tinh chỉnh việc học tốc độ học sẽ phức
tạp.</li>
<li>Tiền điều kiện có thể giúp trong việc tinh chỉnh thang đo.</li>
<li>Phương pháp Newton nhanh hơn rất nhiều <em>một khi</em> hoạt động trên bài
toán lồi phù hợp.</li>
<li>Hãy cẩn trọng trong việc dùng phương pháp Newton cho các bài toán
không lồi mà không tinh chỉnh.</li>
</ul>
<!--
## Exercises
--></div>
<div class="section" id="bai-tap">
<h2><span class="section-number">11.7.5. </span>Bài tập<a class="headerlink" href="#bai-tap" title="Permalink to this headline">¶</a></h2>
<!--
1. Experiment with different learning rates and objective functions for gradient descent.
2. Implement line search to minimize a convex function in the interval $[a, b]$.
    * Do you need derivatives for binary search, i.e., to decide whether to pick $[a, (a+b)/2]$ or $[(a+b)/2, b]$.
    * How rapid is the rate of convergence for the algorithm?
    * Implement the algorithm and apply it to minimizing $\log (\exp(x) + \exp(-2*x -3))$.
3. Design an objective function defined on $\mathbb{R}^2$ where gradient descent is exceedingly slow. Hint - scale different coordinates differently.
4. Implement the lightweight version of Newton's method using preconditioning:
    * Use diagonal Hessian as preconditioner.
    * Use the absolute values of that rather than the actual (possibly signed) values.
    * Apply this to the problem above.
5. Apply the algorithm above to a number of objective functions (convex or not). What happens if you rotate coordinates by $45$ degrees?
--><ol class="arabic simple">
<li>Hãy thử các tốc độ học, hàm mục tiêu khác nhau cho hạ gradient.</li>
<li>Khởi tạo tìm kiếm đường thẳng để cực tiểu hóa hàm lồi trong khoảng
<span class="math notranslate nohighlight">\([a, b]\)</span>.<ul>
<li>Bạn có cần đạo hàm để tìm kiếm nhị phân không, ví dụ, để quyết
định xem sẽ chọn <span class="math notranslate nohighlight">\([a, (a+b)/2]\)</span> hay <span class="math notranslate nohighlight">\([(a+b)/2, b]\)</span>?</li>
<li>Tốc độ hội tụ của thuật toán nhanh chậm thế nào?</li>
<li>Hãy khởi tạo thuật toán và áp dụng nó để cực tiểu hóa
<span class="math notranslate nohighlight">\(\log (\exp(x) + \exp(-2*x -3))\)</span>.</li>
</ul>
</li>
<li>Thiết kế một hàm mục tiêu thuộc <span class="math notranslate nohighlight">\(\mathbb{R}^2\)</span> mà việc hạ
gradient rất chậm. Gợi ý: sử dụng trục tọa độ có thang đo khác nhau.</li>
<li>Khởi tạo một phiên bản nhỏ gọn của phương pháp Newton sử dụng tiền
điều kiện:<ul>
<li>Dùng ma trận đường chéo Hessian làm tiền điều kiện.</li>
<li>Sử dụng các giá trị tuyệt đối của nó thay vì các giá trị có dấu.</li>
<li>Áp dụng điều này cho bài toán phía trên.</li>
</ul>
</li>
<li>Áp dụng thuật toán phía trên cho các hàm mục tiêu (lồi lẫn không
lồi). Điều gì sẽ xảy ra nếu xoay các trục tọa độ một góc <span class="math notranslate nohighlight">\(45\)</span>
độ?</li>
</ol>
</div>
<div class="section" id="thao-luan">
<h2><span class="section-number">11.7.6. </span>Thảo luận<a class="headerlink" href="#thao-luan" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><a class="reference external" href="https://discuss.d2l.ai/t/351">Tiếng Anh - MXNet</a></li>
<li><a class="reference external" href="https://discuss.d2l.ai/t/491">Tiếng Anh - Pytorch</a></li>
<li><a class="reference external" href="https://forum.machinelearningcoban.com/c/d2l">Tiếng Việt</a></li>
</ul>
</div>
<div class="section" id="nhung-nguoi-thuc-hien">
<h2><span class="section-number">11.7.7. </span>Những người thực hiện<a class="headerlink" href="#nhung-nguoi-thuc-hien" title="Permalink to this headline">¶</a></h2>
<p>Bản dịch trong trang này được thực hiện bởi:</p>
<ul class="simple">
<li>Đoàn Võ Duy Thanh</li>
<li>Nguyễn Văn Quang</li>
<li>Nguyễn Lê Quang Nhật</li>
<li>Nguyễn Văn Quang</li>
<li>Nguyễn Văn Cường</li>
<li>Phạm Hồng Vinh</li>
<li>Phạm Minh Đức</li>
<li>Nguyễn Thanh Hòa</li>
<li>Võ Tấn Phát</li>
</ul>
</div>
</div>


        </div>
        <div class="side-doc-outline">
            <div class="side-doc-outline--content"> 
<div class="localtoc">
    <p class="caption">
      <span class="caption-text">Table Of Contents</span>
    </p>
    <ul>
<li><a class="reference internal" href="#">11.7. Hạ Gradient</a><ul>
<li><a class="reference internal" href="#ha-gradient-trong-mot-chieu">11.7.1. Hạ Gradient trong Một Chiều</a><ul>
<li><a class="reference internal" href="#toc-do-hoc">11.7.1.1. Tốc độ học</a></li>
<li><a class="reference internal" href="#cuc-tieu">11.7.1.2. Cực Tiểu</a></li>
</ul>
</li>
<li><a class="reference internal" href="#ha-gradient-da-bien">11.7.2. Hạ Gradient Đa biến</a></li>
<li><a class="reference internal" href="#nhung-phuong-phap-thich-nghi">11.7.3. Những Phương pháp Thích nghi</a><ul>
<li><a class="reference internal" href="#phuong-phap-newton">11.7.3.1. Phương pháp Newton</a></li>
<li><a class="reference internal" href="#phan-tich-hoi-tu">11.7.3.2. Phân tích Hội tụ</a></li>
<li><a class="reference internal" href="#tien-dieu-kien">11.7.3.3. Tiền Điều kiện</a></li>
<li><a class="reference internal" href="#ha-gradient-cung-tim-kiem-duong-thang">11.7.3.4. Hạ gradient cùng Tìm kiếm Đường thẳng</a></li>
</ul>
</li>
<li><a class="reference internal" href="#tong-ket">11.7.4. Tổng kết</a></li>
<li><a class="reference internal" href="#bai-tap">11.7.5. Bài tập</a></li>
<li><a class="reference internal" href="#thao-luan">11.7.6. Thảo luận</a></li>
<li><a class="reference internal" href="#nhung-nguoi-thuc-hien">11.7.7. Những người thực hiện</a></li>
</ul>
</li>
</ul>

</div>
            </div>
        </div>

      <div class="clearer"></div>
    </div><div class="pagenation">
     <a id="button-prev" href="convexity_vn.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="P">
         <i class="pagenation-arrow-L fas fa-arrow-left fa-lg"></i>
         <div class="pagenation-text">
            <span class="pagenation-direction">Previous</span>
            <div>11.6. Tính lồi</div>
         </div>
     </a>
     <a id="button-next" href="sgd_vn.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="N">
         <i class="pagenation-arrow-R fas fa-arrow-right fa-lg"></i>
        <div class="pagenation-text">
            <span class="pagenation-direction">Next</span>
            <div>11.8. Hạ Gradient Ngẫu nhiên</div>
        </div>
     </a>
  </div>
        
        </main>
    </div>
  </body>
</html>